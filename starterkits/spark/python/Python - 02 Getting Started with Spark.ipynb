{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python - Getting Started with Spark\n",
    "\n",
    "As part of this module we will take a simple use case and try to scratch the surface of the Spark. We will be using simple use case to demontrate end to end Data Engineering Pipeline.\n",
    "\n",
    "* Understand Data Model\n",
    "* Define Problem Statement\n",
    "* Creating Spark Context\n",
    "* Setting Run Time Job Properties\n",
    "* Reading data from CSV Files\n",
    "* Apply Filtering\n",
    "* Row Level Transformations\n",
    "* Perform Joins\n",
    "* Aggregate Data\n",
    "* Perform Sorting\n",
    "* Write output to Files\n",
    "* Complete Script\n",
    "* Validating Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand Data Model\n",
    "\n",
    "Let us understand the data model and also characteristics of the data.\n",
    "\n",
    "* Base directory for **retail_db** data sets is **/public/retail_db**.\n",
    "* It have six folders, each folder represents a separate table.\n",
    "  * Product Catalog Tables\n",
    "    * products\n",
    "    * categories\n",
    "    * departments\n",
    "  * Customers Table\n",
    "    * customers\n",
    "  * Transactional Tables\n",
    "    * orders\n",
    "    * order_items\n",
    "* **orders** and **order_items** are related. **orders** is parent table and **order_items** is child table for orders.\n",
    "* All folders have one ore more files under them.\n",
    "* Each line represents a record and have values related to multiple columns. Each record is delimited or separated by **comma (,)**.\n",
    "* First field in each orders record is order_id and it is a primary key (unique and not null)\n",
    "* Second field in each order_items record is order_item_order_id which is a foreign key attribute to orders order_id.\n",
    "* There are other relationships as well, however they are not relevant to get started. We will primarily focus on orders and order_items data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem Statement\n",
    "\n",
    "Get monthly revenue considering complete or closed orders\n",
    "\n",
    "* We will use orders and order_items data.\n",
    "* **orders** is available at **/public/retail_db/orders**\n",
    "* **order_items** is available at **/public/retail_db/order_items**\n",
    "* We need to consider orders with COMPLETE or CLOSED status.\n",
    "* Revenue can be computed using **order_item_subtotal** which is 5th attribute in order_items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Context\n",
    "\n",
    "Let us understand how to create Spark Context using `SparkSession` from `pyspark.sql`.\n",
    "\n",
    "* We need to have spark context to leverage both APIs as well as distributed computing framework.\n",
    "* `SparkSession` is a wrapper class which will use existing Spark Context or create new one.\n",
    "* We can customize the behavior of Spark Context created by passing properties using `config` or by using APIs such as `appName`, `master` etc.\n",
    "* APIs are provided only for most commonly used properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:44952\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Getting Started - Monthly Revenue</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f848714a278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Run Time Job Properties\n",
    "\n",
    "Let us understand how to customize run time behavior of submitted jobs.\n",
    "\n",
    "* Once Spark Context is created, we can customize run time behavior by using `spark.conf.set`. \n",
    "* In our case let us set a property called as `spark.sql.shuffle.partitions` to 2.\n",
    "* If we do not set this property, by default it will use 200 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.shuffle.partitions', '2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When using Jupyter Notebook, if you want to improvise the readability of the data of the show command then you can set `spark.sql.repl.eagerEval.enabled` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from CSV Files\n",
    "\n",
    "Let us quickly see how we can read data from CSV Files.\n",
    "\n",
    "* Spark provide several APIs to read the files of different file formats.\n",
    "* All the out of the box APIs are available under `spark.read`.\n",
    "* In our case we have to read text files where each record is delimited or separated by comma (',').\n",
    "* To create Data Frames for `orders` and `order_items` we can pass the path to `spark.read.csv`.\n",
    "* There are other options as well which can be passed using keyword arguments. You can run help on `spark.read.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnanValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpositiveInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegativeInf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxColumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxCharsPerColumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmaxMalformedLogPerPartition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcolumnNameOfCorruptRecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msamplingRatio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menforceSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0memptyValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
       "\n",
       "This function will go through the input once to determine the input schema if\n",
       "``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
       "``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
       "\n",
       ":param path: string, or list of strings, for input path(s),\n",
       "             or RDD of Strings storing CSV rows.\n",
       ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
       "               or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
       ":param sep: sets a single character as a separator for each field and value.\n",
       "            If None is set, it uses the default value, ``,``.\n",
       ":param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
       "                 it uses the default value, ``UTF-8``.\n",
       ":param quote: sets a single character used for escaping quoted values where the\n",
       "              separator can be part of the value. If None is set, it uses the default\n",
       "              value, ``\"``. If you would like to turn off quotations, you need to set an\n",
       "              empty string.\n",
       ":param escape: sets a single character used for escaping quotes inside an already\n",
       "               quoted value. If None is set, it uses the default value, ``\\``.\n",
       ":param comment: sets a single character used for skipping lines beginning with this\n",
       "                character. By default (None), it is disabled.\n",
       ":param header: uses the first line as names of columns. If None is set, it uses the\n",
       "               default value, ``false``.\n",
       ":param inferSchema: infers the input schema automatically from data. It requires one extra\n",
       "               pass over the data. If None is set, it uses the default value, ``false``.\n",
       ":param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
       "                      forcibly applied to datasource files, and headers in CSV files will be\n",
       "                      ignored. If the option is set to ``false``, the schema will be\n",
       "                      validated against all headers in CSV files or the first header in RDD\n",
       "                      if the ``header`` option is set to ``true``. Field names in the schema\n",
       "                      and column names in CSV headers are checked by their positions\n",
       "                      taking into account ``spark.sql.caseSensitive``. If None is set,\n",
       "                      ``true`` is used by default. Though the default value is ``true``,\n",
       "                      it is recommended to disable the ``enforceSchema`` option\n",
       "                      to avoid incorrect results.\n",
       ":param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
       "                                values being read should be skipped. If None is set, it\n",
       "                                uses the default value, ``false``.\n",
       ":param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
       "                                 values being read should be skipped. If None is set, it\n",
       "                                 uses the default value, ``false``.\n",
       ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
       "                  the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
       "                  applies to all supported types including the string type.\n",
       ":param nanValue: sets the string representation of a non-number value. If None is set, it\n",
       "                 uses the default value, ``NaN``.\n",
       ":param positiveInf: sets the string representation of a positive infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param negativeInf: sets the string representation of a negative infinity value. If None\n",
       "                    is set, it uses the default value, ``Inf``.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
       "                   set, it uses the default value, ``20480``.\n",
       ":param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
       "                          value being read. If None is set, it uses the default value,\n",
       "                          ``-1`` meaning unlimited length.\n",
       ":param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
       "                                    If specified, it is ignored.\n",
       ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
       "             set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
       "             parse only required columns in CSV under column pruning. Therefore, corrupt\n",
       "             records can be different based on required set of fields. This behavior can\n",
       "             be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
       "             (enabled by default).\n",
       "\n",
       "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
       "          into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
       "          fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
       "          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
       "          schema does not have the field, it drops corrupt records during parsing. \\\n",
       "          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
       "          When it meets a record having fewer tokens than the length of the schema, \\\n",
       "          sets ``null`` to extra fields. When the record has more tokens than the \\\n",
       "          length of the schema, it drops extra tokens.\n",
       "        * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
       "        * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
       "\n",
       ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
       "                                  created by ``PERMISSIVE`` mode. This overrides\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
       "                                  it uses the value specified in\n",
       "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
       ":param multiLine: parse records, which may span multiple lines. If None is\n",
       "                  set, it uses the default value, ``false``.\n",
       ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
       "                                  the quote character. If None is set, the default value is\n",
       "                                  escape character when escape and quote characters are\n",
       "                                  different, ``\\0`` otherwise.\n",
       ":param samplingRatio: defines fraction of rows used for schema inferring.\n",
       "                      If None is set, it uses the default value, ``1.0``.\n",
       ":param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
       "                   the default value, empty string.\n",
       "\n",
       ">>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
       ">>> df.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       ">>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
       ">>> df2 = spark.read.csv(rdd)\n",
       ">>> df2.dtypes\n",
       "[('_c0', 'string'), ('_c1', 'string')]\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.read.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also set `spark.sql.repl.eagerEval.enabled` to `True` and then just run Data Frame name as part of Jupyter Notebook Cell to preview the data in the Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>order_id</th><th>order_date</th><th>order_customer_id</th><th>order_status</th></tr>\n",
       "<tr><td>1</td><td>2013-07-25 00:00:...</td><td>11599</td><td>CLOSED</td></tr>\n",
       "<tr><td>2</td><td>2013-07-25 00:00:...</td><td>256</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>3</td><td>2013-07-25 00:00:...</td><td>12111</td><td>COMPLETE</td></tr>\n",
       "<tr><td>4</td><td>2013-07-25 00:00:...</td><td>8827</td><td>CLOSED</td></tr>\n",
       "<tr><td>5</td><td>2013-07-25 00:00:...</td><td>11318</td><td>COMPLETE</td></tr>\n",
       "<tr><td>6</td><td>2013-07-25 00:00:...</td><td>7130</td><td>COMPLETE</td></tr>\n",
       "<tr><td>7</td><td>2013-07-25 00:00:...</td><td>4530</td><td>COMPLETE</td></tr>\n",
       "<tr><td>8</td><td>2013-07-25 00:00:...</td><td>2911</td><td>PROCESSING</td></tr>\n",
       "<tr><td>9</td><td>2013-07-25 00:00:...</td><td>5657</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>10</td><td>2013-07-25 00:00:...</td><td>5648</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>11</td><td>2013-07-25 00:00:...</td><td>918</td><td>PAYMENT_REVIEW</td></tr>\n",
       "<tr><td>12</td><td>2013-07-25 00:00:...</td><td>1837</td><td>CLOSED</td></tr>\n",
       "<tr><td>13</td><td>2013-07-25 00:00:...</td><td>9149</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>14</td><td>2013-07-25 00:00:...</td><td>9842</td><td>PROCESSING</td></tr>\n",
       "<tr><td>15</td><td>2013-07-25 00:00:...</td><td>2568</td><td>COMPLETE</td></tr>\n",
       "<tr><td>16</td><td>2013-07-25 00:00:...</td><td>7276</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>17</td><td>2013-07-25 00:00:...</td><td>2667</td><td>COMPLETE</td></tr>\n",
       "<tr><td>18</td><td>2013-07-25 00:00:...</td><td>1205</td><td>CLOSED</td></tr>\n",
       "<tr><td>19</td><td>2013-07-25 00:00:...</td><td>9488</td><td>PENDING_PAYMENT</td></tr>\n",
       "<tr><td>20</td><td>2013-07-25 00:00:...</td><td>9198</td><td>PROCESSING</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "DataFrame[order_id: int, order_date: string, order_customer_id: int, order_status: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- order_item_order_id: integer (nullable = true)\n",
      " |-- order_item_product_id: integer (nullable = true)\n",
      " |-- order_item_quantity: integer (nullable = true)\n",
      " |-- order_item_subtotal: float (nullable = true)\n",
      " |-- order_item_product_price: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "|            1|                  1|                  957|                  1|             299.98|                  299.98|\n",
      "|            2|                  2|                 1073|                  1|             199.99|                  199.99|\n",
      "|            3|                  2|                  502|                  5|              250.0|                    50.0|\n",
      "|            4|                  2|                  403|                  1|             129.99|                  129.99|\n",
      "|            5|                  4|                  897|                  2|              49.98|                   24.99|\n",
      "|            6|                  4|                  365|                  5|             299.95|                   59.99|\n",
      "|            7|                  4|                  502|                  3|              150.0|                    50.0|\n",
      "|            8|                  4|                 1014|                  4|             199.92|                   49.98|\n",
      "|            9|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           10|                  5|                  365|                  5|             299.95|                   59.99|\n",
      "|           11|                  5|                 1014|                  2|              99.96|                   49.98|\n",
      "|           12|                  5|                  957|                  1|             299.98|                  299.98|\n",
      "|           13|                  5|                  403|                  1|             129.99|                  129.99|\n",
      "|           14|                  7|                 1073|                  1|             199.99|                  199.99|\n",
      "|           15|                  7|                  957|                  1|             299.98|                  299.98|\n",
      "|           16|                  7|                  926|                  5|              79.95|                   15.99|\n",
      "|           17|                  8|                  365|                  3|             179.97|                   59.99|\n",
      "|           18|                  8|                  365|                  5|             299.95|                   59.99|\n",
      "|           19|                  8|                 1014|                  4|             199.92|                   49.98|\n",
      "|           20|                  8|                  502|                  1|               50.0|                    50.0|\n",
      "+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_items.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Filtering\n",
    "\n",
    "Let us see how we can filter out records in Data Frame.\n",
    "* We can either use `filter` or `where` to filter the data. Both of them serve the same purpose.\n",
    "* We can pass the condictions either in SQL Style or API Style.\n",
    "* In this case, we have used SQL Style to check `order_status` for `COMPLETE` or `CLOSED` orders.\n",
    "* We can perform all standard filtering conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered = orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example for API Style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    filter(orders.order_status.isin('COMPLETE', 'CLOSED')). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row Level Transformations\n",
    "\n",
    "Let us see how we can project and also derive new fields out of existing fields leveraging functions.\n",
    "\n",
    "* One of the ways to project data is by using `select` on top of Data Frame.\n",
    "* Spark provides almost 300 pre defined functions as part of `pyspark.sql.functions`.\n",
    "* In our case we need to import and use `date_format` function to extract month from existing date.\n",
    "* Later we will also import and use functions such as `sum` and `round` while aggregating the data.\n",
    "* We can also provide meaningful names to derived fields using `alias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "orders_transformed = orders_filtered. \\\n",
    "    select('order_id', date_format('order_date', 'yyyyMM').alias('order_month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Joins\n",
    "\n",
    "Let us join both the data sets which have the fields we are interested in. \n",
    "\n",
    "* We can join data sets using `join`.\n",
    "* We also might have to pass join condition in case the column names are different between the data sets.\n",
    "* In our case we have to join `orders` and `order_items` using `orders.order_id` and `order_items.order_item_order_id`.\n",
    "\n",
    "We can join original Data Frames as well and generate order_month while grouping the data as demonstrated in the **Complete Script** Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month = orders_transformed. \\\n",
    "    join(order_items, \n",
    "         orders.order_id == order_items.order_item_order_id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Data\n",
    "\n",
    "As we have joined `orders` and `order_items`, let us perform the aggregation.\n",
    "* In this case want to compute revenue for each month.\n",
    "* `order_month` is derived field which contain both year and month.\n",
    "* We can use `order_month` as part of `groupBy` so that data can be grouped. It will generate a special Data Frame of type `GroupedData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_details_by_month. \\\n",
    "    groupBy('order_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now invoke aggregate functions such as `sum` and pass the desired field using which we want to aggregate (in this case we can pass `order_item_subtotal` to `sum`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum, round\n",
    "\n",
    "monthly_revenue = order_details_by_month. \\\n",
    "    groupBy('order_month'). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Sorting\n",
    "\n",
    "As we got the revenue for each month, let us sort the data so that we can review the output for the validation.\n",
    "\n",
    "* We can use `orderBy` or `sort` to sort the data.\n",
    "* By default data will be sorted in ascending order.\n",
    "* In this case we are sorting the data by `order_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted = monthly_revenue. \\\n",
    "    orderBy('order_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_revenue_sorted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write output to Files\n",
    "\n",
    "As data is read, processed and sorted - now it is time to write data to files in underlying file system.\n",
    "\n",
    "* In our environment **/public** is read only folder. You will not be able to add files under subdirectories of **/public**.\n",
    "* Assuming you have write access to **/user/[OS_USER_NAME]**, I have used **/user/{username}/retail_db/monthly_revenue** as target folder.\n",
    "* `{username}` is replaced by the OS user used for login using `getpass.getuser()`.\n",
    "* `coalesce(1)` is used to write the output to one file.\n",
    "* If the folder and files already exists, `mode('overwrite')` will replace existing folder with new files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "monthly_revenue_sorted. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Script\n",
    "\n",
    "Here is the complete script or program which takes care of the following:\n",
    "\n",
    "* Create Spark Context and set the properties.\n",
    "* Read the data related to different tables.\n",
    "* Process the data using relevant Spark Data Frame APIs.\n",
    "* Write the data back to file system\n",
    "\n",
    "Entire Data processing and writing the data back to file system is developed using Piped approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, sum, round\n",
    "\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    appName('Getting Started - Monthly Revenue'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()\n",
    "\n",
    "spark.conf.set('spark.sql.shuffle.partitions', '2')\n",
    "\n",
    "# Reading orders\n",
    "orders_path = '/public/retail_db/orders'\n",
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv(orders_path, \n",
    "        schema=\"order_id INT, order_date STRING, \" +\n",
    "               \"order_customer_id INT, order_status STRING\"\n",
    "       )\n",
    "\n",
    "# Reading order_items\n",
    "order_items_path = '/public/retail_db/order_items'\n",
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    csv(order_items_path, \n",
    "        schema=\"order_item_id INT, order_item_order_id INT, \" +\n",
    "               \"order_item_product_id INT, order_item_quantity INT, \" +\n",
    "               \"order_item_subtotal FLOAT, order_item_product_price FLOAT\"\n",
    "       )\n",
    "\n",
    "orders. \\\n",
    "    filter('order_status in (\"COMPLETE\", \"CLOSED\")'). \\\n",
    "    join(order_items, orders.order_id == order_items.order_item_order_id). \\\n",
    "    groupBy(date_format('order_date', 'yyyyMM').alias('order_month')). \\\n",
    "    agg(round(sum('order_item_subtotal'), 2).alias('revenue')). \\\n",
    "    orderBy('order_month'). \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    csv(f'/user/{username}/retail_db/monthly_revenue',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Output\n",
    "\n",
    "Let us go ahead and validate the output.\n",
    "\n",
    "* In our case we are using HDFS and hence we should be able to use HDFS commands to validate.\n",
    "* Let us first list the files which will give some idea about when they are created.\n",
    "* For some file formats, we will also see extension as well as compression algorithm used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of small text files we can use `cat` to see the contents. It might not work if the files are compressed.\n",
    "* Also, it is not a good practice to use `cat` for larger text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -cat /user/`whoami`/retail_db/monthly_revenue/part*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
