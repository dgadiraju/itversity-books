{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details about HDFS Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this section we will get into all important HDFS commands.\n",
    "\n",
    "* Getting list of commands and help\n",
    "* Creating directories and changing ownership\n",
    "* Managing files and file permissions\n",
    "* Controlling access using ACLs\n",
    "* Overriding Properties\n",
    "* HDFS usage and Metadata Commands\n",
    "\n",
    "We are deliberately not sharing command snippets here for most of the topics. We would recommend to watch video and practice commands by looking into help of the command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of commands and help\n",
    "\n",
    "Let us explore details about how to list the commands and get the help or usage for given command.\n",
    "\n",
    "* Even though we can run commands from almost all the nodes in the clusters, we should only use Gateway to run HDFS Commands.\n",
    "* First we need to make sure designated Gateway server is Gateway for HDFS service so that we can run commands from Gateway node. In our case we have designated **bigdataserver-1** as Gateway.\n",
    "* Let us make sure that **bigdataserver-1** is added as HDFS Gateway so that we can run our commands successfully.\n",
    "* Also we can run commands by connecting to multiple clusters. However, we cannot configure one server as Gateway for multiple clusters and hence we have to specify the URI for Namenode using -fs. We can get Namenode URI from core-site.xml or Cloudera Manager.\n",
    "* Typically Namenode process will be running on port number 8020.\n",
    "* <mark>hadoop fs</mark> – list all the commands available\n",
    "* <mark>hadoop fs -usage</mark> – will give us basic usage for given command\n",
    "* <mark>hadoop fs -help</mark> – will give us additional information for all the commands\n",
    "* We can run help on individual commands as well.\n",
    "* Let us also review very important command <mark>hadoop fs -ls</mark> to list files and directories under given path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Directories and Changing Ownership\n",
    "\n",
    "Now let us have a look at how to create directories and manage ownership.\n",
    "\n",
    "* By default hdfs is superuser of HDFS\n",
    "* <mark>hadoop fs -mkdir</mark> – to create directories\n",
    "* <mark>hadoop fs -chown</mark> – to change ownership of files\n",
    "* chown can also be used to change the group. We can change the group using **-chgrp** command as well. Make sure to run **-help** on **chgrp** and check the details.\n",
    "* Creating user space\n",
    "    * Create directory with user id **cloudera** under **/user**\n",
    "    * Change ownership to the same name as the directory created earlier (**/user/cloudera**)\n",
    "    * You can validate permissions by using hadoop fs -ls command on /user\n",
    "* Let us create OS users on bigdataserver-1 and then user spaces for **cloudera, itversity** and **demo**.\n",
    "* We will be using these to demonstrate ACLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing Files and File Permissions\n",
    "\n",
    "Now let us get into commands related to managing files in HDFS. It includes deleting files, copying files as well as HDFS File Permissions.\n",
    "\n",
    "***Deleting Files from HDFS***\n",
    "Let us see how we can delete files from HDFS.\n",
    "\n",
    "* As we have already copied data into HDFS, let us start with deleting files using **hadoop fs -rm** command.\n",
    "    * When we use rm command, files will be copied to .Trash directory by default. It acts as recycle bin to overcome issue of deleting files accidentally.\n",
    "    * We can use -skipTrash to skip recycle bin and delete data permanently. However, it cannot be undone.\n",
    "    * .Trash can be cleaned up manually by users belonging to superuser group such as HDFS or automatically based on trash related properties defined in core-site.xml.\n",
    "\n",
    "***Copying Files between local file system and HDFS***\n",
    "\n",
    "We can copy files from local file system and vice versa. We can append data into existing files in HDFS.\n",
    "\n",
    "* <mark>hadoop fs -copyFromLocal</mark> or <mark>hadoop fs -put</mark> – to copy files from local filesystem and HDFS. Process of copying data is already covered. File will be divided into blocks and will be stored on Datanodes in distributed fashion based on block size and replication factor.\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/10/04HDFSAnatomyOfFileWrite.png)\n",
    "\n",
    "\n",
    "* <mark>hadoop fs -copyToLocal</mark> or <mark>hadoop fs -get</mark> – to copy files from HDFS to local filesystem. It will read all the blocks using index in sequence and construct the file in local file system.\n",
    "* We can also use <mark>hadoop fs -appendToFile</mark> to append data to existing file.\n",
    "* However, we will not be able to update or fix data in files when they are in HDFS. If we have to fix any data, we have to move file to local file system, fix data and then again copy to HDFS.\n",
    "* We can move files from local file system to HDFS using <mark>hadoop fs -moveFromLocal</mark>. Even though there is a command moveToLocal, functionality is not implemented yet.\n",
    "\n",
    "***Copying or Moving Files within HDFS***\n",
    "\n",
    "We can also copy files with in HDFS using commands like cp and mv.\n",
    "\n",
    "* <mark>hadoop fs -cp</mark> to copy files from one HDFS location to another HDFS location\n",
    "* <mark>hadoop fs -mv</mark> to move files from one HDFS location to another HDFS location\n",
    "* mv is faster than cp as mv deals with only metadata where as cp have to copy all the blocks.\n",
    "* If you have to rename or move the files make sure to run <mark>hadoop fs -mv</mark>\n",
    "\n",
    "***Previewing Data***\n",
    "\n",
    "Let us see how we can preview the data in HDFS.\n",
    "\n",
    "* If we are dealing with files contain text data (files of text file format), we can preview contents of the files using different commands as **-tail, -cat** etc.\n",
    "* **-tail** can be used to preview last 1 KB of the file\n",
    "* **-cat** can be used to print the whole contents of the file on the screen. Be careful while using -cat as **it will take a while for even medium sized files.**\n",
    "* If you want to get first few lines from file you can redirect output of **hadoop fs -cat** to Linux **more** command\n",
    "\n",
    "***HDFS File Permissions***\n",
    "\n",
    "Let us go through file permissions in HDFS.\n",
    "\n",
    "\n",
    "* As we create the files, we can check the permissions on them using **-ls** command.\n",
    "* Typically the owner of the user space will have **rwx,** while members of the group specified as well as others have **rx**\n",
    "* We can change the permissions using **hadoop fs -chmod**\n",
    "* We can specify permissions mode (e.g.: +x to grant execute access to owner, grop as well as others) as well as octal mode (e.g.: 755 to grant rwx for owner, rx for group and others)\n",
    "\n",
    "\n",
    "If you are not familiar with linux command chmod, we would highly recommend you to spend some time to get detailed understanding of it as it is very important with respect to file permissions.\n",
    "Let us copy data into all 3 user spaces for the users cloudera, itversity and demo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Access using ACLs\n",
    "\n",
    "ACLs stands for Access Control Lists and it gives finer level access control over files. Without ACLs permissions are controlled at owner, group and others levels only.\n",
    "\n",
    "* To use ACLs in HDFS, we need to set dfs.namenode.acls.enabled to true as part of hdfs-site.xml.\n",
    "\n",
    "* We can use <mark>hadoop fs -setfacl</mark> to set ACL at file or directory level.\n",
    "\n",
    "* <mark>hadoop fs -getfacl</mark> can be used to get details about ACL on a file or a directory.\n",
    "\n",
    "* First, let us see examples at the file level, then directory level and then deleting ACLs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "hdfs dfs -touchz /user/cloudera/aclsfile.txt\n",
    "#Create some test data in local file and append to aclsfile.txt as cloudera\n",
    "#somedata contains output of hadoop fs -help\n",
    "hadoop fs -help > somedata\n",
    "hadoop fs -appendToFile somedata /user/cloudera/aclsfiledemo.txt\n",
    "hadoop fs -setfacl -m user:itversity:rw- /user/cloudera/aclsfiledemo.txt\n",
    "hadoop fs -getfacl /user/cloudera/aclsfiledemo.txt\n",
    "\n",
    "#Now as itversity, you should be able to append data to aclsfiledemo.txt\n",
    "hadoop fs -help > somedata\n",
    "hadoop fs -appendToFile somedata /user/cloudera/aclsfiledemo.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "\n",
    "#Now let us see on a directory\n",
    "hadoop fs -mkdir /user/cloudera/aclsdemo\n",
    "hadoop fs -chmod 757 /user/cloudera/aclsdemo\n",
    "#Any user on the system will be able to write to /user/cloudera/aclsdemo\n",
    "\n",
    "#Using acls we can restrict write access to itversity but not demo\n",
    "#First change the permssions back to 755\n",
    "hadoop fs -chmod 755 /user/cloudera/aclsdemo\n",
    "hadoop fs -setfacl -m user:itversity:rwx /user/cloudera/aclsdemo\n",
    "#Now user itversity should be able to copy files into the directory\n",
    "#but not other users like demo\n",
    "\n",
    "#We can give access to demo as well and check the acl details\n",
    "hadoop fs -setfacl -m user:demo:rwx /user/cloudera/aclsdemo\n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo\n",
    "\n",
    "#Default ACL\n",
    "hadoop fs -setfacl -m default:other::--- /user/cloudera/aclsdemo\n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo\n",
    "\n",
    "#We can use --set to override ACLs\n",
    "hadoop fs -setfacl --set \\\n",
    "  user::rw-,user:itversity:rw-,user:demo:rw-,group::r--,other::r-- \\\n",
    "  /user/cloudera/aclsdemo.txt\n",
    "  \n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo.txt\n",
    "\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Access from a particular user or group can be removed using -x\n",
    "hadoop fs -setfacl -x user:demo /user/cloudera/aclsdemo\n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo\n",
    "\n",
    "#Using -b we can remove all ACLs that are added\n",
    "hadoop fs -setfacl -b /user/cloudera/aclsdemo\n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo\n",
    "\n",
    "#Remove Default ACL\n",
    "hadoop fs -setfacl -k /user/cloudera/aclsdemo\n",
    "hadoop fs -getfacl /user/cloudera/aclsdemo\n",
    "\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overriding Properties\n",
    "\n",
    "Let us see how we can override properties while running commands such as **hadoop fs**. Let us first review properties files such as core-site.xml and hdfs-site.xml.\n",
    "\n",
    "* We can override any non-final property using -**Dproperty_name=property_value** as part of hadoop fs command.\n",
    "* We can also use options such as -fs to override Namenode URI\n",
    "* We can also change replication factor using subcommand -setrep\n",
    "* Some of the properties might have been defined as final as part of the properties files such as core-site.xml or hdfs-site.xml. -D will not have any impact in that case.\n",
    "* When it comes to Cloudera Manager, we are not supposed to override properties by updating files – instead, we need to use something called Safety Valve that comes as part of Cloudera Manager.\n",
    "\n",
    "### HDFS usage commands and getting metadata\n",
    "\n",
    "Now let us have a look at HDFS usage commands and also commands used to get the metadata.\n",
    "\n",
    "* <mark>hadoop fs -df</mark> – to get details about the amount of storage used by HDFS. Use -s to get summarized information and -h to get information in readable format.\n",
    "* <mark>hadoop fs -du</mark> – to get the size of data that is copied\n",
    "* <mark>hdfs fsck</mark> – to get metadata for given directory or files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
