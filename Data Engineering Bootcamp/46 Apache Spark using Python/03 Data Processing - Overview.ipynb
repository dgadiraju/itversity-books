{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Processing - Overview\n",
    "\n",
    "## Pre-requisites and Module Introduction\n",
    "\n",
    "Let us understand prerequisites before getting into the module.\n",
    "* Good understanding of Data Processing using Python.\n",
    "* Data Processing Life Cycle\n",
    " * Reading Data from files\n",
    " * Processing Data using APIs\n",
    " * Writing Processed Data back to files\n",
    "* We can also use Databases as sources and sinks. It will be covered at a later point in time.\n",
    "* We can also read data in streaming fashion which is out of the scope of this course.\n",
    "\n",
    "\n",
    "We will get an overview of the Data Processing Life Cycle by the end of the module.\n",
    "* Read airlines data from the file.\n",
    "* Preview the schema and data to understand the characteristics of the data.\n",
    "* Get an overview of Data Frame APIs as well as functions used to process the data.\n",
    "* Check if there are any duplicates in the data.\n",
    "* Get an overview of how to write data in Data Frames to Files using File Formats such as Parquet using Compression.\n",
    "* Reorganize the data by month with different file format and using partitioning strategy.\n",
    "* We will deep dive into Data Frame APIs to process the data in subsequent modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Spark Context\n",
    "\n",
    "Let us start Spark Context using SparkSession.\n",
    "\n",
    "* `SparkSession` is a class that is part of `pyspark.sql` package.\n",
    "* It is a wrapper on top of Spark Context.\n",
    "* When Spark application is submitted using `spark-submit` or `spark-shell` or `pyspark`, a web service called as Spark Context will be started.\n",
    "* Spark Context maintains the context of all the jobs that are submitted until it is killed.\n",
    "* `SparkSession` is nothing but wrapper on top of Spark Context.\n",
    "* We need to first create SparkSession object with any name. But typically we use `spark`. Once it is created, several APIs will be exposed including `read`.\n",
    "* We need to at least set Application Name and also specify the execution mode in which Spark Context should run while creating `SparkSession` object.\n",
    "* We can use `appName` to specify name for the application and `master` to specify the execution mode.\n",
    "* Below is the sample code snippet which will start the Spark Session object for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark.stop()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '12903'). \\\n",
    "    appName('Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark read APIs\n",
    "\n",
    "Let us get the overview of Spark read APIs to read files of different formats.\n",
    "\n",
    "* `spark` has a bunch of APIs to read data from files of different formats.\n",
    "* All APIs are exposed under `spark.read`\n",
    " * `text` - to read single column data from text files as well as reading each of the whole text file as one record.\n",
    " * `csv`- to read text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    " * `json` - to read data from JSON files\n",
    " * `orc` - to read data from ORC files\n",
    " * `parquet` - to read data from Parquet files.\n",
    " * We can also read data from other file formats by plugging in and by using `spark.read.format`\n",
    "* We can also pass options based on the file formats.\n",
    " * `inferSchema` - to infer the data types of the columns based on the data.\n",
    " * `header` - to use header to get the column names in case of text files.\n",
    " * `schema` - to explicitly specify the schema.\n",
    "* We can get the help on APIs like `spark.read.csv` using `help(spark.read.csv)`.\n",
    "* Reading delimited data from text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "|      21|2013-07-25 00:00:...|             2711|        PENDING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        header=True,\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reading JSON data from text files. We can infer schema from the data as each JSON object contain both column name and value.\n",
    "* Example for JSON\n",
    "\n",
    "```\n",
    "{ 'order_id': 1, 'order_date': '2013-07-25 00:00:00.0', 'order_customer_id': 12345, 'order_status': 'COMPLETE' }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark. \\\n",
    "    read. \\\n",
    "    json('/public/retail_db_json/orders'). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand airlines data\n",
    "Let us read one of the files and understand more about the data to determine right API with right options to process data later.\n",
    "* Our airlines data is in text file format.\n",
    "* We can use `spark.read.text` on one of the files to preview the data and understand the following\n",
    " * Whether header is present in files or not.\n",
    " * Field Delimiter that is being used.\n",
    "* Once we determine details about header and field delimiter we can use `spark.read.csv` with appropriate options to read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    text(\"/public/airlines_all/airlines/part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(airlines.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "* Data have header and each field is delimited by a comma.\n",
    "\n",
    "## Inferring Schema\n",
    "\n",
    "Let us understand how we can quickly get schema using one file and apply on other files.\n",
    "* We can pass the file name pattern to `spark.read.csv` and read all the data in files under **hdfs://public/airlines_all/airlines** into Data Frame.\n",
    "* We can use options such as `header` and `inferSchema` to assign names and data types.\n",
    "* However `inferSchema` will end up going through the entire data to assign schema. We can use samplingRatio to process fraction of data and then infer the schema.\n",
    "* In case if the data in all the files have similar structure, we should be able to get the schema using one file and then apply it on others.\n",
    "* In our airlines data schema is consistent across all the files and hence we should be able to get the schema by going through one file and apply on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_part_00000 = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(airlines_part_00000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_part_00000.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_part_00000.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines_schema = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       ). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(airlines_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    schema(airlines_schema). \\\n",
    "    csv(\"/public/airlines_all/airlines/part*\",\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(airlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing airlines data\n",
    "Let us preview the airlines data to understand more about it.\n",
    "* As we have too many files, we will just process one file and preview the data.\n",
    "* File Name: **hdfs://public/airlines_all/airlines/part-00000**\n",
    "* `spark.read.csv` will create a variable of type Data Frame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines_schema = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       ). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    schema(airlines_schema). \\\n",
    "    csv(\"/public/airlines_all/airlines/part*\",\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Data Frame will have structure or schema.\n",
    "\n",
    "* We can print the schema using `airlines.printSchema()`\n",
    "* We can preview the data using `airlines.show()`. By default it shows 20 records and some of the column values might be truncated for readability purpose.\n",
    "* We can review the details of show by using `help(airlines.show)`\n",
    "* We can pass custom number of records and say `truncate=False` to show complete information of all the records requested. It will facilitate us to preview all columns with desired number of records.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can get the number of records or rows in a Data Frame using `airlines.count()`\n",
    "* In Databricks Notebook, we can use `display` to preview the data using Visualization feature\n",
    "* We can perform all kinds of standard transformations on our data. We need to have good knowledge of functions on Data Frames as well as functions on columns to apply all standard transformations.\n",
    "* Let us also validate if there are duplicates in our data, if yes we will remove duplicates while reorganizing the data later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines_schema = spark.read. \\\n",
    "    csv(\"/public/airlines_all/airlines/part-00000\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       ). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    schema(airlines_schema). \\\n",
    "    csv(\"/public/airlines_all/airlines/part-0000*\",\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data Frame APIs\n",
    "\n",
    "Let us get an overview of Data Frame APIs to process data in Data Frames.\n",
    "* Row Level Transformations or Projection of Data can be done using `select`, `selectExpr`, `withColumn`, `drop` on Data Frame.\n",
    "* We typically apply functions from `pyspark.sql.functions` on columns using `select` and `withColumn`\n",
    "* Filtering is typically done either by using `filter` or `where` on Data Frame.\n",
    "* We can pass the condition to `filter` or `where` either by using SQL Style or Programming Language Style.\n",
    "* Global Aggregations can be performed directly on the Data Frame.\n",
    "* By Key or Grouping Aggregations are typically performed using `groupBy` and then aggregate functions using `agg`\n",
    "* We can sort the data in Data Frame using `sort` or `orderBy`\n",
    "* We will talk about Window Functions later. We can use use Window Functions for some advanced Aggregations and Ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Let us understand how to project the data using different options such as `select`, `selectExpr`, `withColumn`, `drop.`\n",
    "\n",
    "* Create Dataframe **employees** using Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "employees = [(1, \"Scott\", \"Tiger\", 1000.0, \"united states\"),\n",
    "             (2, \"Henry\", \"Ford\", 1250.0, \"India\"),\n",
    "             (3, \"Nick\", \"Junior\", 750.0, \"united KINGDOM\"),\n",
    "             (4, \"Bill\", \"Gomes\", 1500.0, \"AUSTRALIA\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF = spark. \\\n",
    "    createDataFrame(employees,\n",
    "                    schema=\"\"\"employee_id INT, first_name STRING, \n",
    "                    last_name STRING, salary FLOAT, nationality STRING\"\"\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project employee first name and last name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "employeesDF. \\\n",
    "    select(\"first_name\", \"last_name\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Project all the fields except for Nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "employeesDF. \\\n",
    "    drop(\"nationality\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will explore most of the APIs to process data in Data Frames as we get into the data processing at a later point in time**\n",
    "\n",
    "## Overview of Functions\n",
    "\n",
    "Let us get an overview of different functions that are available to process data in columns.\n",
    "* While Data Frame APIs work on the Data Frame, at times we might want to apply functions on column values.\n",
    "* Functions to process column values are available under `pyspark.sql.functions`. These are typically used in select or withColumn on top of Data Frame.\n",
    "* There are approximately 300 pre-defined functions available for us.\n",
    "* Some of the important functions can be broadly categorized into String Manipulation, Date Manipulation, Numeric Functions and Aggregate Functions.\n",
    "* String Manipulation Functions\n",
    " * Concatenating Strings - `concat`\n",
    " * Getting Length - `length`\n",
    " * Trimming Strings - `trim`,` rtrim`, `ltrim`\n",
    " * Padding Strings - `lpad`, `rpad`\n",
    " * Extracting Strings - `split`, `substring`\n",
    "* Date Manipulation Functions\n",
    " * Date Arithmetic - `date_add`, `date_sub`, `datediff`, `add_months`\n",
    " * Date Extraction - `dayofmonth`, `month`, `year`\n",
    " * Get beginning period - `trunc`, `date_trunc`\n",
    "* Numeric Functions - `abs`, `greatest`\n",
    "* Aggregate Functions - `sum`, `min`, `max`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "Let us perform a task to understand how functions are typically used.\n",
    "\n",
    "* Project full name by concatenating first name and last name along with other fields excluding first name and last name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, concat\n",
    "\n",
    "employeesDF. \\\n",
    "    withColumn(\"full_name\", concat(\"first_name\", lit(\", \"), \"last_name\")). \\\n",
    "    drop(\"first_name\", \"last_name\"). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF. \\\n",
    "    select(\"employee_id\",\n",
    "           concat(\"first_name\", lit(\", \"), \"last_name\").alias(\"full_name\"),\n",
    "           \"salary\",\n",
    "           \"nationality\"\n",
    "          ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeesDF. \\\n",
    "    selectExpr(\"employee_id\",\n",
    "               \"concat(first_name, ', ', last_name) AS full_name\",\n",
    "               \"salary\",\n",
    "               \"nationality\"\n",
    "              ). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will explore most of the functions as we get into the data processing at a later point in time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Spark Write APIs\n",
    "\n",
    "Let us understand how we can write Data Frames to different file formats.\n",
    "* All the batch write APIs are grouped under write which is exposed to Data Frame objects.\n",
    "* All APIs are exposed under spark.read\n",
    " * `text` - to write single column data to text files.\n",
    " * `csv` - to write to text files with delimiters. Default is a comma, but we can use other delimiters as well.\n",
    " * `json` - to write data to JSON files\n",
    " * `orc` - to write data to ORC files\n",
    " * `parquet` - to write data to Parquet files.\n",
    "* We can also write data to other file formats by plugging in and by using `write.format`, for example **avro**\n",
    "* We can use options based on the type using which we are writing the Data Frame to.\n",
    " * `compression` - Compression codec (`gzip`, `snappy` etc)\n",
    " * `sep` - to specify delimiters while writing into text files using **csv**\n",
    "* We can `overwrite` the directories or `append` to existing directories using `mode`\n",
    "* Create copy of orders data in **parquet** file format with no compression. If the folder already exists overwrite it. Target Location: **/user/[YOUR_USER_NAME]/retail_db/orders**\n",
    "* When you pass options, if there are typos then options will be ignored rather than failing. Be careful and make sure that output is validated.\n",
    "* By default the number of files in the output directory is equal to number of tasks that are used to process the data in the last stage. However, we might want to control number of files so that we don't run into too many small files issue.\n",
    "* We can control number of files by using `coalesce`. It has to be invoked on top of Data Frame before invoking `write`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark. \\\n",
    "    read. \\\n",
    "    csv('/public/retail_db/orders',\n",
    "        header=True,\n",
    "        schema='''\n",
    "            order_id INT, \n",
    "            order_date STRING, \n",
    "            order_customer_id INT, \n",
    "            order_status STRING\n",
    "        '''\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders. \\\n",
    "    write. \\\n",
    "    parquet('/user/training/retail_db/orders', \n",
    "            mode='overwrite', \n",
    "            compression='none'\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using option\n",
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('compression', 'none'). \\\n",
    "    parquet('/user/training/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using format\n",
    "orders. \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    option('compression', 'none'). \\\n",
    "    format('parquet'). \\\n",
    "    save('/user/training/retail_db/orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/training/retail_db/orders\n",
    "\n",
    "# File extension should not contain compression algorithms such as snappy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Read order_items data from **/public/retail_db_json/order_items** and write it to pipe delimited files with gzip compression. Target Location: **/user/[YOUR_USER_NAME]/retail_db/order_items**. Make sure to validate.\n",
    "* Ignore the error if the target location already exists. Also make sure to write into only one file. We can use `coalesce` for it. \n",
    "\n",
    "**`coalesce` will be covered in detail at a later point in time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_items = spark. \\\n",
    "    read. \\\n",
    "    json('/public/retail_db_json/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using format\n",
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    mode('ignore'). \\\n",
    "    option('compression', 'gzip'). \\\n",
    "    option('sep', '|'). \\\n",
    "    format('csv'). \\\n",
    "    save('/user/training/retail_db/order_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach - using keyword arguments\n",
    "order_items. \\\n",
    "    coalesce(1). \\\n",
    "    write. \\\n",
    "    csv('/user/training/retail_db/order_items',\n",
    "        sep='|',\n",
    "        mode='ignore',\n",
    "        compression='gzip'\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/training/retail_db/order_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorganizing airlines data\n",
    "\n",
    "Let us reorganize our airlines data to fewer files where data is compressed and also partitioned by Month.\n",
    "* We have ~1920 files of ~64MB Size.\n",
    "* Data is in the range of 1987 October and 2008 December (255 months)\n",
    "* By default it uses ~1920 threads to process the data and it might end up with too many small files. We can avoid that by using repartition and then partition by the month.\n",
    "* Here are the steps we are going to follow to partition by flight month and save the data to /user/[YOUR_USER_NAME]/airlines.\n",
    " * Read one file first and get the schema.\n",
    " * Read the entire data by applying the schema from the previous step.\n",
    " * Add additional column flightmonth using withColumn by using lpad on month column and concat functions. We need to do this as the month in our data set is of type integer and we want to pad with 0 for months till september to format it into YYYYMM.\n",
    " * Repartition the data into 255 based on the number of months using flightmonth\n",
    " * Partition the data by partitionBy while writing the data to the target location.\n",
    " * We will use parquet file format which will automatically compresses data using Snappy algorithm.\n",
    " \n",
    "**This process will take time, once it is done we will review the target location to which data is copied by partitioning using month**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark.stop()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.dynamicAllocation.enabled', 'false'). \\\n",
    "    config('spark.executor.instances', 40). \\\n",
    "    appName('Data Processing - Overview'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lpad\n",
    "\n",
    "airlines_schema = spark.read. \\\n",
    "    csv('/public/airlines_all/airlines/part-00000',\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "       ). \\\n",
    "    schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = spark.read. \\\n",
    "    schema(airlines_schema). \\\n",
    "    csv('/public/airlines_all/airlines/part*',\n",
    "        header=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(airlines.write.parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"255\")\n",
    "airlines. \\\n",
    "    distinct(). \\\n",
    "    withColumn('flightmonth', concat('year', lpad('month', 2, '0'))). \\\n",
    "    repartition(255, 'flightmonth'). \\\n",
    "    write. \\\n",
    "    mode('overwrite'). \\\n",
    "    partitionBy('flightmonth'). \\\n",
    "    format('parquet'). \\\n",
    "    save('/user/training/airlines-part')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Previewing reorganized data\n",
    "Let us preview the data using reorganized data.\n",
    "* We will use new location going forward - **/public/airlines_all/airlines-part**. Data is already copied into that location.\n",
    "* We have partitioned data by month and stored in that location.\n",
    "* Instead of using complete data set we will read the data from one partition **/public/airlines_all/airlines-part/flightmonth=200801**\n",
    "* First let us create a DataFrame object by using `spark.read.parquet(\"/public/airlines_all/airlines-part/flightmonth=200801\")` - let's say airlines. \n",
    "* We can get the schema of the DataFrame using `airlines.printSchema()`\n",
    "* Use `airlines.show()` or `airlines.show(100, truncate=False)`  to preview the data.\n",
    "* We can also use `display(airlines)` to get airlines data in tabular format as part of Databricks Notebook.\n",
    "* We can also use `airlines.describe().show()` to get some statistics about the Data Frame and `airlines.count()` to get the number of records in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Understand Data\n",
    "Let us analyze and understand more about the data in detail using data of 2008 January.\n",
    "* First let us read the data for the month of 2008 January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines_path = \"/public/airlines_all/airlines-part/flightmonth=200801\"\n",
    "\n",
    "airlines = spark. \\\n",
    "    read. \\\n",
    "    parquet(airlines_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get number of records - `airlines.count()`\n",
    "* Go through the list of columns and understand the purpose of them.\n",
    "  * Year\n",
    "  * Month\n",
    "  * DayOfMonth\n",
    "  * CRSDepTime - Scheduled Departure Time\n",
    "  * DepTime - Actual Departure Time.\n",
    "  * DepDelay - Departure Delay in Minutes\n",
    "  * CRSArrTime - Scheduled Arrival Time\n",
    "  * ArrTime - Actual Arrival Time.\n",
    "  * ArrDelay - Arrival Delay in Minutes.\n",
    "  * UniqueCarrier - Carrier or Airlines\n",
    "  * FlightNum - Flight Number\n",
    "  * Distance - Distance between Origin and Destination\n",
    "  * IsDepDelayed - this is set to yes for those flights where departure is delayed.\n",
    "  * IsArrDelayed -- this is set to yes for those flights where arrival is delayed.\n",
    "* Get number of unique origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines. \\\n",
    "    select(\"Origin\"). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get number of unique destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "airlines. \\\n",
    "    select(\"Dest\"). \\\n",
    "    distinct(). \\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get all unique carriers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines. \\\n",
    "    select('UniqueCarrier'). \\\n",
    "    distinct(). \\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Let us recap about key takeaways from this module.\n",
    "* APIs to read the data from files into Data Frame.\n",
    "* Previewing Schema and the data in Data Frame.\n",
    "* Overview of Data Frame APIs and Functions\n",
    "* Writing data from Data Frame into Files\n",
    "* Reorganizing the airlines data by month\n",
    "* Simple APIs to analyze the data.\n",
    "Now it is time for us to deep dive into APIs to perform all the standard transformations as part of Data Processing.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "\n",
     "\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
