{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different File Formats and Custom Delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will talk about dealing with different file formats and also custom delimiters in text data. We will see how to read and how to write the data. Also we will understand APIs such as persist/cache on Data Frames.\n",
    "\n",
    "* Overview of write APIs – dataframe.write\n",
    "* Overview of read APIs – spark.read\n",
    "* Supported file formats\n",
    "    * csv, text (for text file formats)\n",
    "    * json (using complex schema)\n",
    "    * orc\n",
    "    * parquet\n",
    "    * avrò (3rd party)\n",
    "* Text Data – Custom Delimiters\n",
    "* Data Frames Persistence\n",
    "\n",
    "### Overview of write APIs – dataframe.write\n",
    "\n",
    "Let us see how we can write data to different targets using APIs under write on top of data frame.\n",
    "\n",
    "* Supported file formats – csv, text json, orc, parquet etc.\n",
    "* We can also write data to 3rd party supported file formats such as avro\n",
    "* Data can be written to Hive tables as well\n",
    "* We can also connect to relational databases over JDBC and save our output into remote relational databases.\n",
    "* We can also connect to any 3rd party database using relevant plugin and preserve data over there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schemaString = order_id:int order_date:string order_customer_id:int order_status:string\n",
       "a = Array(order_id:int, order_date:string, order_customer_id:int, order_status:string)\n",
       "fields = Array(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "schema = StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "inputBaseDir = /public/retail_db\n",
       "ordersDF = [order_id: i...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: i..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "ordersDF.printSchema\n",
    "ordersDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.security.AccessControlException\n",
       "Message: Permission denied: user=ramchander_chikkala, access=WRITE, inode=\"/user/sparkscala/orders_json/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4185)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "\n",
       "StackTrace: \tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4185)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
       "  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
       "  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3101)\n",
       "  at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:3069)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1181)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem$25.doCall(DistributedFileSystem.java:1177)\n",
       "  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1195)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:1169)\n",
       "  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1925)\n",
       "  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:343)\n",
       "  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n",
       "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:176)\n",
       "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
       "  ... 44 elided\n",
       "Caused by: org.apache.hadoop.ipc.RemoteException: Permission denied: user=ramchander_chikkala, access=WRITE, inode=\"/user/sparkscala/orders_json/_temporary/0\":hdfs:hdfs:drwxr-xr-x\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:325)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:246)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1934)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkAncestorAccess(FSDirectory.java:1917)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirMkdirOp.mkdirs(FSDirMkdirOp.java:71)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4185)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:1109)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:645)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n",
       "  at org.apache.hadoop.ipc.Client.call(Client.java:1498)\n",
       "  at org.apache.hadoop.ipc.Client.call(Client.java:1398)\n",
       "  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n",
       "  at com.sun.proxy.$Proxy19.mkdirs(Unknown Source)\n",
       "  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:610)\n",
       "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "  at java.lang.reflect.Method.invoke(Method.java:498)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:290)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:202)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:184)\n",
       "  at com.sun.proxy.$Proxy20.mkdirs(Unknown Source)\n",
       "  at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:3099)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDF.write.\n",
    "  format(\"json\").\n",
    "  save(\"/user//sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://nn01.itversity.com:8020/user/ramchander_chikkala/sparkscala/orders_json already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n",
       "  at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:526)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordersDF.write.json(\"/user//sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dataframe-write-examples-02-jdbc.scala```\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "ordersDF.write.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  mode(\"append\").\n",
    "  save\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "ordersDF.\n",
    "  write.\n",
    "  mode(\"append\").\n",
    "  jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dataframe-write-examples-03-hive.scala ```\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "ordersDF.write.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  mode(\"append\").\n",
    "  save\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "ordersDF.\n",
    "  write.\n",
    "  mode(\"append\").\n",
    "  jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "// To create new table and insert into it\n",
    "ordersDF.write.\n",
    "  format(\"hive\").\n",
    "  mode(\"overwrite\").\n",
    "  saveAsTable(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "ordersDF.write.\n",
    "  mode(\"overwrite\").\n",
    "  saveAsTable(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "// To insert data into existing table\n",
    "ordersDF.write.\n",
    "  format(\"hive\").\n",
    "  mode(\"overwrite\").\n",
    "  insertInto(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "ordersDF.write.\n",
    "  mode(\"overwrite\").\n",
    "  insertInto(\"bootcampdemo.orders_hive\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of read APIs – spark.read\n",
    "\n",
    "spark.read have bunch of APIs to read data from different source types.\n",
    "\n",
    "* Supported file formats- csv, text, json, orc, parquet etc\n",
    "* We can also read data from 3rd party supported file formats such as avro\n",
    "* We can read data directly from hive tables\n",
    "* JDBC – to read data from relational databases\n",
    "* There is generic API called format which can be used in conjunction with option to pass relevant arguments and then load data from either files or over JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val orders = spark.read.\n",
    "//   format(\"json\").\n",
    "//   load(\"/public/retail_db_json/orders\")\n",
    "\n",
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "val orders = spark.read.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  load()\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "val orders = spark.read.\n",
    "    jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:31: error: orders is already defined as value orders\n",
       "       val orders = spark.read.table(\"bootcampdemo.orders_hive\")\n",
       "           ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.read.\n",
    "  format(\"hive\").\n",
    "  table(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "val orders = spark.read.table(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported file formats\n",
    "\n",
    "Let us see details about all the supported formats in Spark to create data frames and save them.\n",
    "\n",
    "* Following file formats are supported out of the box with Spark\n",
    "    * text – using text (fixed length) or csv (delimited)\n",
    "    * json\n",
    "    * orc\n",
    "    * parquet\n",
    "* Avro is available with 3rd party plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n",
       "orders_read = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n",
    "\n",
    "\n",
    "val orders_read = spark.read.format(\"text\").\n",
    "  load(\"/user/ramchander_chikkala/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://nn01.itversity.com:8020/user/ramchander_chikkala/sparkscala/orders_text already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\").\n",
    "  write.\n",
    "  format(\"text\").\n",
    "  save(\"/user/ramchander_chikkala/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\").\n",
    "  write.\n",
    "  mode(\"overwrite\").\n",
    "  text(\"/user//sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.text(\"/user/ramchander_chikkala/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|1,2013-07-25 00:0...|\n",
      "|2,2013-07-25 00:0...|\n",
      "|3,2013-07-25 00:0...|\n",
      "|4,2013-07-25 00:0...|\n",
      "|5,2013-07-25 00:0...|\n",
      "|6,2013-07-25 00:0...|\n",
      "|7,2013-07-25 00:0...|\n",
      "|8,2013-07-25 00:0...|\n",
      "|9,2013-07-25 00:0...|\n",
      "|10,2013-07-25 00:...|\n",
      "|11,2013-07-25 00:...|\n",
      "|12,2013-07-25 00:...|\n",
      "|13,2013-07-25 00:...|\n",
      "|14,2013-07-25 00:...|\n",
      "|15,2013-07-25 00:...|\n",
      "|16,2013-07-25 00:...|\n",
      "|17,2013-07-25 00:...|\n",
      "|18,2013-07-25 00:...|\n",
      "|19,2013-07-25 00:...|\n",
      "|20,2013-07-25 00:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://nn01.itversity.com:8020/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n",
    "\n",
    "orders.write.\n",
    "  format(\"csv\").\n",
    "  save(\"/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv\")\n",
    "\n",
    "orders.write.csv(\"/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders_read1 = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read1 = spark.read.\n",
    "  format(\"csv\").\n",
    "  load(\"/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "orders_read1.show()\n",
    "orders_read1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders_read2 = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read2 = spark.read.\n",
    "  csv(\"/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "orders_read2.show()\n",
    "orders_read2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"json\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/ramchander_chikkala/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  json(\"/user/ramchander_chikkala/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"json\").\n",
    "  load(\"/user/ramchander_chikkala/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  json(\"/user/ramchander_chikkala/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"orc\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/ramchander_chikkala/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  orc(\"/user/ramchander_chikkala/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"orc\").\n",
    "  load(\"/user/ramchander_chikkala/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:2: error: unclosed string literal\n",
       "  orc(\"/user/ramchander_chikkala\n",
       "      ^\n",
       "<console>:3: error: unclosed string literal\n",
       "  /sparkscala/orders_orc\")\n",
       "                         ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  orc(\"/user/ramchander_chikkala\n",
    "  /sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"parquet\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/ramchander_chikkala/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  parquet(\"/user/ramchander_chikkala/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"parquet\").\n",
    "  load(\"/user/ramchander_chikkala/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  parquet(\"/user/ramchander_chikkala/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magic ssh does not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ssh\n",
    "spark-shell --master yarn --conf spark.ui.port=12901 --packages com.databricks:spark-avro_2.11:4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-shell --master yarn --conf spark.ui.port=12901 --packages com.databricks:spark-avro_2.11:4.0.0\n",
    "\n",
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:630)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.write.\n",
    "  format(\"com.databricks.spark.avro\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/ramchander_chikkala/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  avro(\"/user/ramchander_chikkala/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"com.databricks.spark.avro\").\n",
    "  load(\"/user/ramchander_chikkala/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = spark.read.\n",
    "  avro(\"/user/ramchander_chikkala/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data – Custom Delimiters\n",
    "\n",
    "Now let us understand how to process text data with the different line as well as field delimiters.\n",
    "\n",
    "* We can read text data into RDD using SparkContext’s textFile. It will treat newline character as a record delimiter.\n",
    "* We have to parse each record in RDD and derive data to process further\n",
    "* With Spark Data Frames we have csv and text APIs to read text data int Data Frame\n",
    "* Both of them use newline character as a record delimiter. When we use csv API to create data frame we can also specify field separator/delimiter using sep as one of the options.\n",
    "* We can also specify sep while writing data into text files with any field separator or delimiter using csv API. Also, we can concatenate data as part of selectExpr with a delimiter of our choice and use text API.\n",
    "* Here is the example to read and write data with ascii null character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputBaseDir = \"/mnt/c/data/retail_db\"\n",
    "val outputBaseDir = \"/mnt/c/data/sparkscala\"\n",
    "\n",
    "val ordersCSV = spark.\n",
    "  read.\n",
    "  csv(inputBaseDir +\"/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(IntegerType)).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  selectExpr(\"concat(order_id, '\\00', order_date, '\\00', order_customer_id, '\\00', order_status)\").\n",
    "  write.\n",
    "  text(outputBaseDir + \"/orders_null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  options(\"sep\", \"\\00\").\n",
    "  csv(outputBaseDir + \"/orders_null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read_csv = spark.\n",
    "  read.\n",
    "  option(\"sep\", \"\\00\").\n",
    "  csv(outputBaseDir + \"/orders_null\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = orders_read_csv.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(IntegerType)).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At times, we might have to deal with text data where line delimiter is different than newline character.\n",
    "* In this case, we need to use HDFS APIs to read data from files with custom line delimiter into RDD and process further (either using transformations/actions or data frame operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"/public/yelp-dataset/yelp_review.csv\"\n",
    "\n",
    "spark.read.text(path).\n",
    "  select(size(split($\"value\", \",\")).alias(\"value\")).\n",
    "  groupBy(\"value\").\n",
    "  count.\n",
    "  show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"textinputformat.record.delimiter\", \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val yelpReview = sc.newAPIHadoopFile(path, \n",
    "  classOf[org.apache.hadoop.mapreduce.lib.input.TextInputFormat], \n",
    "  classOf[org.apache.hadoop.io.LongWritable], \n",
    "  classOf[org.apache.hadoop.io.Text], \n",
    "  conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.map(rec => rec._2.toString).\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.\n",
    "  map(rec => (rec._2.toString.split(\"\\\",\\\"\").size, 1)).\n",
    "  reduceByKey((x, y) => x + y).\n",
    "  collect.\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames Persistence\n",
    "\n",
    "Now let us see how we can persist data frames.\n",
    "\n",
    "* By default, data will be streamed as data frames to executor tasks as data being processed.\n",
    "* Here is what will happen when data is read into executor task while it is being processed\n",
    "    * Deserialize into object\n",
    "    * Stream into memory\n",
    "    * Process data by executor task by applying logic\n",
    "    * Flush deserialized objects from memory as executor tasks are terminated\n",
    "* Some times we might have to read same data multiple times for processing with in the same job. By default every time data need to be deserialized and submitted to executor tasks for processing\n",
    "* To avoid deserializing into java objects when same data have to be read multiple times we can leverage caching.\n",
    "* There are 2 methods persist and cache. By default with data frames caching will be done as MEMORY_AND_DISK from Spark 2.\n",
    "* cache is shorthand method for persist at MEMORY_AND_DISK\n",
    "* This is what happens when we cache Data Frame\n",
    "    * Caching will be done only when data is read at least once for processing\n",
    "    * Each record will be deserialized into object\n",
    "    * These deserialized objects will be cached in memory as long as they fit\n",
    "    * If not, deserialized objects will be spilled out to disk\n",
    "* You can get details about different persistence levels from [here](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
