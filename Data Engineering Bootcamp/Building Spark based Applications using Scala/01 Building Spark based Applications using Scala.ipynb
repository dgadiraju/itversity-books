{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Spark based Applications using Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this lesson we will see how to build scaleable applications using Spark APIs with Scala as programming language.\n",
    "\n",
    "* Setup Development Environment – Scala and Spark\n",
    "* Spark Architecture\n",
    "* Spark Data Structures such as RDD, Data Frames, Data Sets etc\n",
    "* Data Frames and Data Frame Operations\n",
    "* Spark SQL\n",
    "* and more\n",
    "\n",
    "We have not only covered basic transformations such as filtering, joins, aggregations but also analytics/windowing functions are covered in detail.\n",
    "\n",
    "### Setup Development environment on Windows\n",
    "\n",
    "We are considering fresh Windows laptop. We will start with Java/JDK on Windows laptop and we will go through step by step instructions to setup Scala, sbt, WinUtils etc.\n",
    "\n",
    "* For integrated development using IntelliJ\n",
    "* Typically programming will be done with IDEs such as IntelliJ\n",
    "* IDEs are typically integrated with other tools such as git which is code versioning tool. Tools like git facilitate team development.\n",
    "* sbt is build tool for Scala. Once applications are developed using IDE, they are typically built using tools like sbt\n",
    "* WinUtils is required for HDFS APIs to work on Windows laptop\n",
    "\n",
    "**\"Unless java is setup and validated successfully do not go further. If you need our support, please log the issues in our [forums](http://discuss.itversity.com/).**\n",
    "\n",
    "**Setup Java and JDK**\n",
    "\n",
    "* Before getting started check whether Java and JDK are installed or not\n",
    "    * Launch command prompt – Go to search bar on windows laptop, type cmd and hit enter\n",
    "    * Type java -version If it return version, check whether 1.8 or not. It is better to have 1.8 version. If you have other version, consider uninstall and install 1.8 (Search for programs installed and uninstall Java)\n",
    "    * Type javac -version If it return version, check whether 1.8 or not. It is better to have 1.8 version. If you have other version, consider uninstall and install 1.8 (Search for programs installed and uninstall Java)\n",
    "    * If you need other versions, make sure environment variables point to 1.8\n",
    "    * If you do not have Java at all, make sure to follow the instructions and install 1.8 version of JRE and JDK.\n",
    "* **Why do we need to install Java and JDK?** Scala, Spark and many other technologies require Java and JDK to develop and build the applications. Scala is JVM based programming language.\n",
    "* How to install Java and JDK? \n",
    "    * Go to [official page of Oracle](https://www.oracle.com/java/technologies/javase-jdk8-downloads.html) where downloads are available\n",
    "    * Accept the terms and download 64 bit version\n",
    "* How to validate?\n",
    "Use <mark>java -version</mark> and <mark>javac -version</mark> commands in command prompt and see they return 1.8 or not        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Scala with IntelliJ\n",
    "\n",
    "* Now install IntelliJ\n",
    "* There are 2 versions of IntelliJ community edition and enterprise edition\n",
    "* Community edition is free and at times you need to install additional plugins\n",
    "* Enterprise edition is paid and supported and comes with most of the important plugins pre-installed. Also set of plugins are bundled together as part of enterprise edition\n",
    "* Unless you have corporate license for now consider installing community edition.\n",
    "\n",
    "* **Why IntelliJ?**\n",
    "     * IntelliJ is created by JetBrains and it is very popular in building IDEs which boost productivity in team development\n",
    "    * Scala and SBT can be added as plugins using IntelliJ\n",
    "    * Most commonly used tools such as git comes out of the box for versioning the code in the process of application development by teams.\n",
    "* **How to Install?**\n",
    "    * Go to the downloads page and make sure right version is chosen.\n",
    "    * Once downloaded, just double click on installable and follow typical installation process\n",
    "* **How to validate?**\n",
    "    * We will develop a program as part of next section to validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop Hello World Program\n",
    "\n",
    "We will see how to create first program using Scala as sbt project.\n",
    "\n",
    "* Click on New Project\n",
    "* For the first time, it selects java by default. Make sure to choose Scala and then sbt\n",
    "* Give name to the project -> **spark2demo**\n",
    "* Choose right version of Scala -> **2.11.12**\n",
    "* Choose right version of sbt -> **0.13**\n",
    "\n",
    "\" **It will take some time to setup the project**\n",
    "\n",
    "Once done you will see\n",
    "\n",
    "* src directory with the structure src/main/scala\n",
    "* src/main/scala is base directory for scala code\n",
    "* build.sbt under project\n",
    "    * **name** – name of the project\n",
    "    * **version** – project version (0.1)\n",
    "    * **scalaVersion** – scala version (2.11.12)\n",
    "    \n",
    "name := \"spark2demo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"  \n",
    "\n",
    "* Steps to develop HelloWorld program\n",
    "    * Right click on src/main/scala\n",
    "    * Choose Scala Class\n",
    "    * Give name as Hello World and change type to object\n",
    "    * Replace the code with below code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "object HelloWorld {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    println(\"Hello  World\")\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*     \n",
    "    * Right click and run the program\n",
    "    * You should see Hello World in the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\" Make sure IntelliJ setup with Scala is done and validated by running Hello World program. In case of any issues, please log in our [forums](http://discuss.itversity.com/).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup sbt and run application\n",
    "\n",
    "Once the application is developed, we need to build jar file and migrate to higher environments. sbt is the build tool which is typically used for Scala based projects.\n",
    "\n",
    "* Why sbt?\n",
    "    * To build scala based applications to jar file\n",
    "    * Validate jar file to make sure program is running fine\n",
    "* How to setup sbt?\n",
    "    * Setup sbt by downloading relevant downloadable from this link\n",
    "    * For Windows use Microsoft Installer (msi)\n",
    "* How to validate sbt?\n",
    "    * Copy the path by right clicking the project in IntelliJ\n",
    "    * Go to command prompt and cd to the path\n",
    "    * Check the directory structure, you should see\n",
    "        * src directory\n",
    "        * build.sbt\n",
    "    * Run <mark>sbt package</mark>\n",
    "    * It will build jar file and you will see the path\n",
    "    * Run program by using <mark>sbt run</mark> command\n",
    "    * You should see **Hello World** printed on the console\n",
    "    \n",
    "### Add Spark dependencies to the application\n",
    "\n",
    "As we are done with validating IntelliJ, Scala and sbt by developing and running the program, now we are ready to integrate Spark and start developing Scala based applications using Spark APIs.\n",
    "\n",
    "    * Update build.sbt by adding\n",
    "    \n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.0\"\n",
    "\n",
    "* Enable auto-import or click on refresh on type right corner\n",
    "* It will take some time to download dependencies based on your internet speed\n",
    "**\" Be patient until all the spark based dependencies are downloads. You can expand External Dependencies in project view to see list of jars downloaded.**\n",
    "\n",
    "* build.sbt will look like this  \n",
    "\n",
    "name := \"spark2demo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.3.0\"\n",
    "\n",
    "### Set  Win Utils to get HDFS APIs  working\n",
    "\n",
    "* Why to install winutils?\n",
    "    * In the process of building data processing applications using Spark, we need to read data from files\n",
    "    * Spark uses HDFS API to read files from several file systems like HDFS, s3, local etc\n",
    "    * For HDFS APIs to work on Windows, we need to have WinUtils\n",
    "  \n",
    "* How to install winutils?  \n",
    "    * Click [here](https://codeload.github.com/gvreddy1210/64bit/zip/master) to download 64 bit winutils.exe\n",
    "    * Create directory structure like this <mark>C:/hadoop/bin</mark>\n",
    "    * Setup new environment variable HADOOP_HOME\n",
    "        * Search for **Environment Variables** on Windows search bar\n",
    "        * Click on **Add Environment Variables**\n",
    "        * There will be 2 categories of environment variables\n",
    "            * User Variables on top\n",
    "            * **System Variables** on **bottom**\n",
    "            * Make sure to click on Add for **System Variables**\n",
    "            * Name: HADOOP_HOME\n",
    "            * Value: **C:\\hadoop** (don’t include bin)\n",
    "        * Also choose Path and click on Edit\n",
    "            * Click on Add\n",
    "            * Add new entry **%HADOOP_HOME%\\bin**\n",
    "            \n",
    "### Setup Data sets\n",
    "\n",
    "You need to have data sets setup for your practice.  \n",
    "\n",
    "* Go to our [GitHub data repository](https://github.com/dgadirau/data)\n",
    "* You can setup data sets in 2 ways\n",
    "    * If you have git, you can clone to the desired directory on your PC\n",
    "    * Otherwise use download, it will download zip file\n",
    "        * Unzip and copy to C:\\data\n",
    "* You will have multiple datasets ready for your practice\n",
    "\n",
    "### Develop first spark application\n",
    "\n",
    "Now we are ready to develop our first Spark application.\n",
    "\n",
    "* Go to **src/main/scala**\n",
    "* Right click and click on **New -> Package**\n",
    "* Give the package name as **retail_db**\n",
    "* Right click on retail_db and click on **New -> Scala Class**\n",
    "    * Name: GetRevenuePerOrder\n",
    "    * Type: Object\n",
    "* Replace the code with this code snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object GetRevenuePerOrder\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "//package retail_db\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object GetRevenuePerOrder {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf().\n",
    "      setMaster(args(0)).\n",
    "      setAppName(\"Get revenue per order\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "    val orderItems = sc.textFile(args(1))\n",
    "    val revenuePerOrder = orderItems.\n",
    "      map(oi => (oi.split(\",\")(1).toInt, oi.split(\",\")(4).toFloat)).\n",
    "      reduceByKey(_ + _).\n",
    "      map(oi => oi._1 + \",\" + oi._2)\n",
    "\n",
    "    revenuePerOrder.saveAsTextFile(args(2))\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Program takes 3 arguments\n",
    "    * args(0) -> execution mode\n",
    "    * args(1) -> input path\n",
    "    * args(2) -> output path\n",
    "* Running the application\n",
    "    * Go to Run menu -> Edit Configurations\n",
    "    * Add new application\n",
    "    * Give application name GetRevenuePerOrder\n",
    "    * Choose main class: retail_db.GetRevenuePerOrder\n",
    "    * Program arguments: local <input_path> <output_path>\n",
    "    * Use classpath for module: Choose spark2demo\n",
    "    * Click on Apply and then Ok\n",
    "* Now you can run the application by right clicking and choosing Run “GetRevenuePerOrder”\n",
    "* Go to output path and check files are created for output or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build jar file\n",
    "\n",
    "Let us see how we can build the jar file and run it.\n",
    "\n",
    "* Copy the path by right clicking the project in IntelliJ\n",
    "* Go to command prompt and cd to the path\n",
    "* Check the directory structure, you should see\n",
    "    * src directory\n",
    "    * build.sbt\n",
    "* Run sbt package\n",
    "* It will build jar file and you will see the path\n",
    "* It will be typically <project_directory>/target/scala-2.11/spark2demo_2.11-0.1.jar\n",
    "* We can also run using sbt “run-main”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sbt \"run-main retail_db.GetRevenuePerOrder local <input_path> <output_path>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\" Now you are ready with the jar file to get deployed. If you have any issues please raise it in our [forums](http://discuss.itversity.com/c/big-data/apache-spark).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Install Spark on Windows\n",
    "\n",
    "Now let us see the details about setting up Spark on Windows\n",
    "\n",
    "* Why to setup Spark?\n",
    "    * Before deploying on the cluster, it is good practice to test the script using spark-submit.\n",
    "    * To run using spark-submit locally, it is nice to setup Spark on Windows\n",
    "* How to setup Spark?\n",
    "    * Install 7z so that we can unzip and untar spark tar ball, from [here](https://spark.apache.org/downloads.html)\n",
    "    * Download spark 2.3 tar ball by going [here](https://spark.apache.org/downloads.html)\n",
    "    * Choose Spark Release: **2.3.0**\n",
    "    * Choose a package type: **Pre-built for Hadoop 2.7 or later**\n",
    "    * It gives the appropriate link pointing to mirror\n",
    "    * Click on it go to mirror and click on it to download\n",
    "    * Use 7z software to unzip and under to complete setup of spark\n",
    "* We need to configure environment variables to run Spark any where\n",
    "* Keep in mind that Spark is not very well supported on Windows and we will see how to setup on Ubuntu using Windows subsystem for Linux.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure environment variables for Spark\n",
    "\n",
    "Let us see how we can configure environment variables of Spark\n",
    "\n",
    "* Why to setup Environment Variables? To run spark-submit, spark-shell from any where on the PC using the jar file.\n",
    "* How to configure Environment Variables?\n",
    "    * Let us assume that Spark is setup under **C:\\spark-2.3.0-bin-hadoop2.7**\n",
    "    * Setup new environment variable SPARK_HOME\n",
    "        * Search for **Environment Variables** on Windows search bar\n",
    "        * Click on **Add Environment Variables**\n",
    "        * There will be 2 categories of environment variables\n",
    "            * User Variables on top\n",
    "            * **System Variables on bottom**\n",
    "            * Make sure to click on Add for **System Variables**\n",
    "            * Name: SPARK_HOME\n",
    "            * Value: **C:\\spark-2.3.0-bin-hadoop2.7** (don’t include bin)\n",
    "        * Also choose Path and click on Edit\n",
    "            * Click on Add\n",
    "            * Add new entry **%SPARK_HOME%\\bin**\n",
    "            \n",
    "* **How to validate?**   \n",
    "    * Go to any directory and run <mark>spark-shell</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spark job using spark-shell\n",
    "\n",
    "Using spark-shell we can validate ad hoc code to confirm it is working. It will also confirm whether the installation is successful or not.\n",
    "\n",
    "* Run spark-shell\n",
    "* Execute this code and make sure it return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41234,109.94\n",
      "65722,1319.8899\n",
      "28730,349.95\n",
      "68522,329.99\n",
      "23776,329.98\n",
      "32676,719.91003\n",
      "53926,219.97\n",
      "4926,939.85\n",
      "38926,1049.9\n",
      "29270,1379.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderItems = /public/retail_db/order_items MapPartitionsRDD[6] at textFile at <console>:32\n",
       "revenuePerOrder = MapPartitionsRDD[9] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[9] at map at <console>:36"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItems = sc.textFile(\"/public/retail_db/order_items\")\n",
    "val revenuePerOrder = orderItems.\n",
    "  map(oi => (oi.split(\",\")(1).toInt, oi.split(\",\")(4).toFloat)).\n",
    "  reduceByKey(_ + _).\n",
    "  map(oi => oi._1 + \",\" + oi._2)\n",
    "revenuePerOrder.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spark application using Spark submit\n",
    "\n",
    "We can validate the jar file by using spark-submit\n",
    "\n",
    "* spark-submit is the main command to submit the job\n",
    "* --class retail_db.GetRevenuePerOrder, to pass the class name\n",
    "* By default master is local, if you want to override we can use --master\n",
    "* After spark-submit and control arguments we have to give jar file name followed by arguments\n",
    "\n",
    "spark-submit --class retail_db.GetRevenuePerOrder PATH_TO_JAR local INPUT_PATH OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ubuntu using Windows subsystem for Linux\n",
    "\n",
    "Now let us see how we can setup Ubuntu on Windows 10\n",
    "\n",
    "* **Why to setup Ubuntu?**\n",
    "    * Windows is not completely fool proof in running spark jobs.\n",
    "    * Using Ubuntu is better alternative and you will run into fewer issues\n",
    "    * Using Windows subsystem for Linux we can quickly set up Ubuntu virtual machine\n",
    "* **How to setup Ubuntu using Windows subsystem for Linux?**\n",
    "    * Follow this link to setup Ubuntu using Windows subsystem for Linux\n",
    "    * Complete the setup process by giving username for the Ubuntu virtual machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing C Drive using Ubuntu built using Windows subsystem for Linux\n",
    "\n",
    "* It is better to understand how we can access C drive in Ubuntu built using subsystem for Linux\n",
    "* It will facilitate us to access files in C drive\n",
    "* In Linux root file system starts with / and does not have partitions like C drive\n",
    "* The location of C drive is <mark>/mnt/C</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Java and JDK on Ubuntu\n",
    "\n",
    "* Before getting started check whether Java and JDK are installed or not\n",
    "    * Launch command prompt – Go to search bar on windows laptop, type cmd and hit enter\n",
    "    * Type <mark>java -version</mark> If it return version, check whether 1.8 or not. It is better to have 1.8 version. If you have other version, consider uninstall and install 1.8 (Search for programs installed and uninstall Java)\n",
    "    * Type <mark>javac -version</mark> If it return version, check whether 1.8 or not. It is better to have 1.8 version. If you have other version, consider uninstall and install 1.8 (Search for programs installed and uninstall Java)\n",
    "    * If you need other versions, make sure environment variables point to 1.8\n",
    "    * If you do not have Java at all, make sure to follow the instructions and install 1.8 version of JRE and JDK.\n",
    "* **Why do we need to install Java and JDK?** Scala, Spark and many other technologies require Java and JDK to develop and build the applications. Scala is JVM based programming language.\n",
    "* **How to install Java and JDK on Ubuntu?**\n",
    "\n",
    "\n",
    "sudo add-apt-repository ppa:webupd8team/java\n",
    "sudo apt-get update\n",
    "sudo apt-get install oracle-java8-installer\n",
    "\n",
    "\n",
    "* **How to validate?**\n",
    "    * Use <mark>java -version</mark> and <mark>javac -version</mark> commands in command prompt and see they return 1.8 or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Untar Spark\n",
    "\n",
    "Now let us see the details about setting up Spark on Ubuntu or any Linux flavor or Mac.\n",
    "\n",
    "* Why to setup Spark?\n",
    "    * Before deploying on the cluster, it is good practice to test the script using spark-submit.\n",
    "    * To run using spark-submit locally, it is nice to setup Spark on Windows\n",
    "\n",
    "* How to setup Spark?\n",
    "    * Download spark 2.3 tar ball by going here. We can use wget to download the tar ball.\n",
    "        * Choose Spark Release: **2.3.0**\n",
    "        * Choose a package type:** Pre-built for Hadoop 2.7 or later**\n",
    "        * It gives the appropriate link pointing to mirror\n",
    "        * Click on it go to mirror and click on it to download\n",
    "        * Use tar xzf command to untar and unzip tar ball  <mark>– tar xzf spark-2.3.0-bin-hadoop2.7.tgz</mark>\n",
    "* We need to configure environment variables to run Spark any where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment Variables – Mac or Linux\n",
    "\n",
    "Let us see how we can configure environment variables of Spark\n",
    "\n",
    "* **Why to setup Environment Variables?** To run spark-submit, spark-shell from any where on the PC using the jar file.\n",
    "* **How to configure Environment Variables?**\n",
    "    * Let us assume that Spark is setup under\n",
    "        * **/Users/itversity/spark-2.3.0-bin-hadoop2.7 on Mac**\n",
    "        * **/mnt/c/spark-2.3.0-bin-hadoop2.7 on Ubuntu built using Windows subsystem**\n",
    "    * Setup new environment variable SPARK_HOME and update PATH\n",
    "    * Make sure to restart terminal (no need to reboot the machine)\n",
    "    \n",
    " On Mac - .bash_profile\n",
    " \n",
    "export SPARK_HOME=/Users/itversity/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "\n",
    " On Ubuntu built using Windows \n",
    "subsystem for Linux - .profile\n",
    "export SPARK_HOME=/mnt/c/spark-2.3.0-bin-hadoop2.7\n",
    "export PATH=$PATH:$SPARK_HOME/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **How to validate?**\n",
    "    * Go to any directory and run <mark>spark-shell</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41234,109.94\n",
      "65722,1319.8899\n",
      "28730,349.95\n",
      "68522,329.99\n",
      "23776,329.98\n",
      "32676,719.91003\n",
      "53926,219.97\n",
      "4926,939.85\n",
      "38926,1049.9\n",
      "29270,1379.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderItems = /public/retail_db/order_items MapPartitionsRDD[11] at textFile at <console>:32\n",
       "revenuePerOrder = MapPartitionsRDD[14] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[14] at map at <console>:36"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItems = sc.textFile(\"/public/retail_db/order_items\") \n",
    "val revenuePerOrder = orderItems.\n",
    " map(oi => (oi.split(\",\")(1).toInt, oi.split(\",\")(4).toFloat)).\n",
    " reduceByKey(_ + _).\n",
    " map(oi => oi._1 + \",\" + oi._2) \n",
    "revenuePerOrder.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run jar file using Spark Submit\n",
    "\n",
    "We can validate the jar file by using spark-submit\n",
    "\n",
    "* <mark>spark-submit</mark> is the main command to submit the job\n",
    "* <mark>--class retail_db.GetRevenuePerOrder</mark>, to pass the class name\n",
    "* By default master is local, if you want to override we can use <mark>--master</mark>\n",
    "* After spark-submit and control arguments we have to give jar file name followed by arguments\n",
    "\n",
    "spark-submit --class retail_db.GetRevenuePerOrder PATH_TO_JAR local INPUT_PATH OUTPUT_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
