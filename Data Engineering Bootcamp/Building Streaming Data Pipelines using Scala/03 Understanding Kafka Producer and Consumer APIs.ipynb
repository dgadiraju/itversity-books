{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Kafka Producer and Consumer APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic we will see how we can develop programs to produce messages to Kafka Topic and consume messages from Kafka Topic using Scala as Programming language.\n",
    "\n",
    "* Revision about Kafka Topic\n",
    "* Understand producer as well as consumer APIs\n",
    "* Create new project with KafkaWorkshop\n",
    "* Define dependencies in build.sbt\n",
    "* Create application.properties for passing zookeeper and kafka broker information\n",
    "* Use producer API to produce messages to Kafka topic\n",
    "* Use consumer API to consume messages from Kafka topic\n",
    "\n",
    "### Revision about Kafka Topic\n",
    "Kafka topic is nothing but group of files into which messages are added using Producer API and consumed using Consumer API.\n",
    "\n",
    "![](https://kaizen.itversity.com/wp-content/uploads/2018/05/kafka-architecture.png)\n",
    "\n",
    "* Topic can be partitioned, for each of the partition there will be directory in the nodes where Kafka broker is running.\n",
    "* Based on the replication factor each directory related to partition will be mirrored to more than one node on which Kafka broker is running.\n",
    "* We can use 3rd party applications such as Kafka Connect, Flume, Logstash etc to publish messages to topic or use Producer API and produce messages using any programming language.\n",
    "* We can use 3rd party applications such as Kafka Connect, Kafka Streams, Flume, Spark Streaming etc to consume messages or use Consumer API with any programming language.\n",
    "* One or more producers can produce messages to same topic. We can also develop different producers produce messages to different partitions in the same topic.\n",
    "* We can have different consumers or consumer groups consume messages from the same topic in round robin fashion.\n",
    "* We can also have different consumers or consumer group consume messages from different partitions\n",
    "\n",
    "### Producer and Consumer APIs\n",
    "Let us understand the details with respect to Producer and Consumer APIs.\n",
    "\n",
    "* Both Producer and Consumer APIs comes as part of Kafka client jar file\n",
    "* Main package for Kafka client is org.apache.kafka.clients\n",
    "* In that we have sub packages for producer as well as consumer\n",
    "* org.apache.kafka.clients.producer have APIs related to creating Kafka producer object, defining producer configurations etc\n",
    "* org.apache.kafka.clients.consumer have APIs related to creating Kafka consumer object, defining consumer configurations etc\n",
    "* As we have to write objects into files (Kafka topic) we have to serialize and deserialize\n",
    "* APIs related to serialization and deserialization are available under org.apache.kafka.common package\n",
    "\n",
    "### Create Project – KafkaWorkshop\n",
    "Let us create a project from the scratch.\n",
    "\n",
    "* lick on New Project\n",
    "* Choose Scala as programming language and SBT as build tool\n",
    "* Make sure you made these choices\n",
    "    * Project Name – KafkaDemo\n",
    "    * JDK – 1.8\n",
    "    * SBT – 0.13.x\n",
    "    * Scala – 2.11\n",
    "    \n",
    "### Define Dependencies\n",
    "Now let us define dependencies as part of build.sbt to download and use required binaries to build simple applications to produce and consume messages using Scala as programming language.\n",
    "* We need to import few APIs that are part of org.apache.kafka\n",
    "* There are several jar files under kafka\n",
    "* For Producer/Consumer APIs it is required to import kafka-clients\n",
    "* Add the dependency of relevant version in build.sbt – libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"1.0.0\"\n",
    "\n",
    "### Externalize Properties\n",
    "We need to connect to Kafka using Kafka broker/bootstrap server. We typically connect to local broker/bootstrap server in development process and actual production cluster in production. It is better to externalize these properties and use them at run time.\n",
    "* We will use com.typesafe config package to load externalized properties at run time.\n",
    "* Update build.sbt with appropriate dependency – <mark>libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"</mark>\n",
    "* Here is the build.sbt after adding Kafka and typesafe config dependencies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name := \"KafkaWorkshop\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"1.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There are multiple ways to externalize properties\n",
    "    * Bundle properties related to all the environments as part of the code base/compiled jar file\n",
    "    * Place properties file at the same location in all environments\n",
    "    * In the former approach we need to categorize property names based on environment they are pointing to, while in later approach we have same property names but different values pointing to respective environment.\n",
    "    * For demo purpose we will bundle the properties file along with the code.\n",
    "* Create resources folder under src/main\n",
    "* Create file by name application.properties\n",
    "* Add all the properties related to dev and prod. You can add more categories as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev.zookeeper = localhost:2181\n",
    "dev.bootstrap.server = localhost:9092\n",
    "\n",
    "prod.zookeeper = nn01.itversity.com:2181,nn02.itversity.com:2181,nn03.itversity.com:2181\n",
    "prod.bootstrap.server = wn01.itversity.com:6667,wn02.itversity.com:6667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce Messages – Producer API\n",
    "Now it is time to develop our first program leveraging Producer API to write messages to Kafka topic.\n",
    "\n",
    "* Right click on src/main/scala -> new -> Scala Class\n",
    "* Change kind to Object\n",
    "* Give name as KafkaProducerExample and hit enter. A new file will created with object.\n",
    "* Define main function\n",
    "* Import java.util.Properties – <mark>import java.util.Properties</mark> – it is useful to configure necessary properties while creating KafkaProducer object which can be used to produce messages into Kafka topic.\n",
    "* Import ConfigFactory which will be used to load the application.properties file to get Kafka broker/bootstrap server information – <mark>import com.typesafe.config.ConfigFactory</mark>\n",
    "* Import classes related to Producer API to write messages to Kafka topic – <mark>import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}</mark>\n",
    "* We need ProducerConfig to get enums related to setting properties to connect to Kafka topic\n",
    "* Create Properties object and add properties related to broker/bootstrap servers as well as serializer to serialize messages to write into Kafka topic\n",
    "* Once the required properties are added we can create KafkaProducer object –<mark>val producer = new KafkaProducer[String,String] (props)</mark>\n",
    "* We can now create ProducerRecord object by passing topic as well as key and value for the message – <mark>val data = new ProducerRecord[String, String](\"Kafka-Testing\", \"Key\", \"Value\")</mark>\n",
    "* We can send ProducerRecord objects using send API on producer (KafkaProducer object) –<mark> producer.send(data)</mark>\n",
    "* Once you send all the messages, make sure to close producer – <mark>producer.close()</mark>\n",
    "* Here is the complete code example to produce messages."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// KafkaProducerExample.scala\n",
    "\n",
    "import java.util.Properties\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "object KafkaProducerExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ProducerConfig.CLIENT_ID_CONFIG, \"KafkaProducerExample\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "    val data = new ProducerRecord[String, String](\"Kafka-Testing\", \"Key\", \"Value\")\n",
    "    producer.send(data)\n",
    "    producer.close()\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume Messages – Consumer API\n",
    "Now it is time to develop our first program leveraging Consumer API to read messages from Kafka topic.\n",
    "* Right click on src/main/scala -> new -> Scala Class\n",
    "* Change kind to Object\n",
    "* Give name as KafkaConsumerExample and hit enter. A new file will created with object.\n",
    "* Define main function\n",
    "* Import java.util.Properties – <mark>import java.util.Properties</mark> – it is useful to configure necessary properties while creating KafkaConsumer object which can be used to consume messages from Kafka topic.\n",
    "* Also import java.util.Collections to consume messages periodically from last offset as collection from the topic\n",
    "* Import ConfigFactory which will be used to load the application.properties file to get Kafka broker/bootstrap server information – <mark>import com.typesafe.config.ConfigFactory</mark>\n",
    "* Import classes related to Consumer API to write messages to Kafka topic – <mark>import org.apache.kafka.clients.consumer.{KafkaConsumer, ConsumerConfig}</mark>\n",
    "* We need ConsumerConfig to get enums related to setting properties to connect to Kafka topic\n",
    "* Create Properties object and add properties related to broker/bootstrap servers as well as deserializer to deserialize messages to write into Kafka topic\n",
    "* Also configure group id for each consumer group along with offset.\n",
    "* Once the required properties are added we can create KafkaConsumer object – <mark>val consumer = new KafkaConsumer[String, String] (props)</mark>\n",
    "* Subscribe to Kafka topic using consumer object – <mark>consumer.subscribe(Collections.singletonList(\"Kafka-Testing\"))</mark>\n",
    "* Now one can keep on polling and read messages from previous offset.\n",
    "* Here is the complete code example to consume messages from Kafka Topic using Consumer APIs."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// KafkaConsumerExample.scala\n",
    "\n",
    "import java.util.{Collections, Properties}\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "object KafkaConsumerExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ConsumerConfig.CLIENT_ID_CONFIG, \"KafkaConsumerExample\")\n",
    "    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"1\")\n",
    "\n",
    "    val consumer = new KafkaConsumer[String, String](props)\n",
    "    consumer.subscribe(Collections.singletonList(\"Kafka-Testing\"))\n",
    "    while(true){\n",
    "      val records = consumer.poll(500)\n",
    "      for (record <- records.iterator()) {\n",
    "        println(\"Received message: (\" + record.key() + \", \" + record.value() + \") at offset \" + record.offset())\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
