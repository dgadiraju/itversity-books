{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Dive into Producer and Consumer APIs – Apache Kafka\n",
    "\n",
    "As we have gone through the basics of Producer and Consumer APIs, now let us deep dive into Producer as well as Consumer APIs so that we can start developing applications leveraging core features of Kafka.\n",
    "\n",
    "* Characteristics of Topic\n",
    "* Producer APIs – Partitioned Topic\n",
    "* Consumer APIs – Partitioned Topic\n",
    "* Consumer APIs – Managing Commits\n",
    "* Running as Fat Jar\n",
    "* Serialization and Deserialization\n",
    "* Admin APIs – Managing Topics\n",
    "\n",
    "### Characteristics of Topic\n",
    "Let us review Topic before we deep dive into APIs to produce messages as well as consume messages using Topic. We shall also review scenarios which we have seen earlier.\n",
    "* We should produce or consume messages in batches.\n",
    "* By default message is sent immediately to the topic, however, we can batch the messages and it can improve performance significantly.\n",
    "* The network will be used more efficiently and compression rates will be better when batched.\n",
    "* Single Partition Topic\n",
    "    * No two consumers from same consumer group can read from the same partition.\n",
    "    * We can use multiple consumers belonging to different consumer groups to read the same copy of data from the Topic or * Topic partition multiple times using multiple threads.\n",
    "    * With a single partition topic, our options are limited to improve the performance.\n",
    "* Multi Partition Topic\n",
    "    * Multi-Partition Topic gives us different options to improve performance significantly.\n",
    "    * We can produce messages into partitions in round robin fashion. With CLI, we will not be able to write messages into a specific partition or any custom algorithm. But APIs gives us a lot more flexibility.\n",
    "    * We can consume messages from all partitions in round robin fashion or using group id or specifying the partition. However, with CLI, we will not be able to achieve other combinations such as some partitions are consumed by one consumer while other partitions are consumed by another consumer in the same group. APIs give us a lot more flexibility.\n",
    "    * Configuring consumers as **Consumer Group A** as shown in the diagram is possible using APIs.\n",
    "\n",
    "![](https://i0.wp.com/kaizen.itversity.com/wp-content/uploads/2018/05/consumer-group.png?ssl=1)\n",
    "\n",
    "### Producer APIs – Partitioned Topic\n",
    "Let us see some of the advanced options related to Producer APIs. We will see how to produce messages to a partitioned topic, using batch, compression algorithms\n",
    "* Let us start with deleting existing topic retail_multi and then recreating it with 4 partitions and replication factor as 1 in our local system.\n",
    "* Here are the steps to send messages into Kafka Topic\n",
    "    * Create Properties object with all relevant properties.\n",
    "    * Create KafkaProducer object by passing Properties object.\n",
    "    * Build ProducerRecord object using one of the constructors.\n",
    "        * Topic Name and Value\n",
    "        * Topic Name, Key and Value\n",
    "        * Topic Name, Partition Index, Key and Value\n",
    "        * and more with headers and timestamp.\n",
    "    * Use KafkaProducer object’s send by passing ProducerRecord object.\n",
    "    * Once the messages are sent make sure to close KafkaProducer.\n",
    "* Let us also review all the APIs that are available as part of ProducerRecord using :**javap -p ProducerRecord**\n",
    "\n",
    "***ProducerRecord with Value only***\n",
    "\n",
    "Let us see how we can use the simplest ProducerRecord using the constructor which takes topic name and value only.\n",
    "* We will be using access.log file that is being populated by gen_logs\n",
    "* Read data from access.log and create the collection out of it\n",
    "* Process collection using foreach, in which ProducerRecord object is created and then sent to Kafka Topic (retail_multi)\n",
    "* We can validate by running kafka-console-consumer.sh to consume messages from each partition and redirected to file to understand the behavior of data distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFile-OnlyValue.scala\n",
    "\n",
    "import java.util.Properties\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "val props = new Properties\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \n",
    "  \"localhost:9092\")\n",
    "props.put(ProducerConfig.CLIENT_ID_CONFIG, \n",
    "  \"Produce log messages from file\")\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val producer = new KafkaProducer[Nothing, String](props)\n",
    "\n",
    "import scala.io.Source\n",
    "val logMessages = Source.\n",
    "  fromFile(\"/opt/gen_logs/logs/access.log\").\n",
    "  getLines.\n",
    "  toList\n",
    "\n",
    "logMessages.foreach(message => {\n",
    "  val record = new ProducerRecord(\"retail_multi\", message)\n",
    "  producer.send(record)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ProducerRecord with Key***\n",
    "\n",
    "Now let us improvise and produce messages into Kafka Topic using key.\n",
    "\n",
    "* When we pass the key to produce messages into the Partitioned topic, by default it will compute the hash of the key and then apply mod using the number of partitions used while creating Topic.\n",
    "* It will ensure all the messages using the same key always go to the same partition.\n",
    "* Let us go ahead and make necessary changes to grab ip address as key and then build ProducerRecord object to send it to Kafka Topic (retail_multi). Make sure retail_multi topic is cleaned up. On Windows, Kafka gets corrupted quite often for some reason, if that is the case you can execute below script to clean up and recreate retail_multi with 4 partitions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --delete \\\n",
    "  --topic retail_multi\n",
    "  \n",
    "rm -rf /tmp/kafka-logs\n",
    "\n",
    "zookeeper-shell.sh localhost:2181 rmr /admin\n",
    "zookeeper-shell.sh localhost:2181 rmr /config\n",
    "zookeeper-shell.sh localhost:2181 rmr /brokers\n",
    "\n",
    "kafka-server-start.sh -daemon /opt/kafka/config/server.properties\n",
    "\n",
    "kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --create \\\n",
    "  --topic retail_multi \\\n",
    "  --partitions 4 \\\n",
    "  --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Selection of key is subjective to your requirement. It can be dense (like country, region etc) or sparse like (ip address)\n",
    "* We can validate by running kafka-console-consumer.sh to consume messages from each partition and redirected to file to understand the behavior of data distribution. Make sure to recreate retail_multi to validate successfully."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFile-UsingKey.scala\n",
    "\n",
    "import java.util.Properties\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "val props = new Properties\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \n",
    "  \"localhost:9092\")\n",
    "props.put(ProducerConfig.CLIENT_ID_CONFIG, \n",
    "  \"Produce log messages from file\")\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "import scala.io.Source\n",
    "val logMessages = Source.\n",
    "  fromFile(\"/opt/gen_logs/logs/access.log\").\n",
    "  getLines.\n",
    "  toList\n",
    "\n",
    "logMessages.foreach(message => {\n",
    "  val ipAddr = message.split(\" \")(0)\n",
    "  val record = new ProducerRecord(\"retail_multi\", ipAddr, message)\n",
    "  producer.send(record)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ProducerRecord with Partition***\n",
    "\n",
    "We can also assign a particular partition based on custom logic while producing messages into Topic\n",
    "* This is primarily useful with dense keys such as country or region.\n",
    "* We can either map each unique key to the different partition or define custom logic to load balance the traffic.\n",
    "* In scenarios like one country or region generating abnormally high traffic than others, we can have one or more partitions for that country or region and rest for other countries or regions.\n",
    "* We can also configure custom partitioner by using ProducerConfig.PARTITIONER_CLASS_CONFIG. This approach is useful to define reusable custom partitioning strategy with in the application.\n",
    "* Let us first cleanup before developing the logic. On Windows, Kafka gets corrupted quite often for some reason, if that is the case you can execute below script to clean up and recreate retail_multi with 4 partitions.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --delete \\\n",
    "  --topic retail_multi\n",
    "  \n",
    "rm -rf /tmp/kafka-logs\n",
    "\n",
    "zookeeper-shell.sh localhost:2181 rmr /admin\n",
    "zookeeper-shell.sh localhost:2181 rmr /config\n",
    "zookeeper-shell.sh localhost:2181 rmr /brokers\n",
    "\n",
    "kafka-server-start.sh -daemon /opt/kafka/config/server.properties\n",
    "\n",
    "kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --create \\\n",
    "  --topic retail_multi \\\n",
    "  --partitions 4 \\\n",
    "  --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ProducerRecord with Partition – Example***\n",
    "\n",
    "Now let us see an example for ProducerRecord with Partition.\n",
    "* As part of this program, we will extract ip address from each message and then get Country ISO code. If it is US, we will send messages to partition 0 and for other countries, we will send to the rest of the partitions using hash mod logic with partitions as 3 (which means data will for other Countries go into partition 1, 2, and 3). Also if there are any invalid ips, we will send it to a different topic called retail_multi_invalid. We will be using Java-based geoip2 provided by maxmind along with database with ip and country mapping."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name := \"KafkaWorkshop\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"1.0.0\"\n",
    "libraryDependencies += \"com.maxmind.geoip2\" % \"geoip2\" % \"2.12.0\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFile-UsePartition-02.scala\n",
    "\n",
    "import java.util.Properties\n",
    "import java.io.File\n",
    "import com.maxmind.geoip2.DatabaseReader\n",
    "import java.net.InetAddress\n",
    "\n",
    "import scala.io.Source\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "\n",
    "val props = new Properties()\n",
    "props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \n",
    "  \"localhost:9092\")\n",
    "props.put(ProducerConfig.CLIENT_ID_CONFIG, \n",
    "  \"Produce log messages from file\")\n",
    "props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \n",
    "  \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "val logMessages = Source.\n",
    "  fromFile(\"/opt/gen_logs/logs/access.log\").\n",
    "  getLines.\n",
    "  toList\n",
    "\n",
    "val database = new File(\"/opt/maxmind/GeoLite2-Country.mmdb\")\n",
    "\n",
    "val reader = new DatabaseReader.Builder(database).build\n",
    "\n",
    "logMessages.foreach(message => {\n",
    "  try {\n",
    "    val ipAddr = message.split(\" \")(0)\n",
    "    val countryIsoCode = reader.\n",
    "      country(InetAddress.getByName(ipAddr)).\n",
    "      getCountry.\n",
    "      getIsoCode\n",
    "    val partitionIndex = if (countryIsoCode == \"US\") 0\n",
    "      else countryIsoCode.hashCode() % 3 + 1\n",
    "    val record = new ProducerRecord[String, String](\"retail_multi\", partitionIndex, ipAddr, message)\n",
    "    producer.send(record)\n",
    "  } catch {\n",
    "    case e: Exception => {\n",
    "      val record = new ProducerRecord[String, String](\"retail_multi_invalid\", message)\n",
    "      producer.send(record)\n",
    "    }\n",
    "  }\n",
    "})\n",
    "\n",
    "producer.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Using ProducerConfig*** \n",
    "\n",
    "Let us see some of the additional properties from ProducerConfig that can be used for fine-tuning the performance of Producers.\n",
    "* We can use additional properties of ProducerConfig to control batch size, compressing data etc.\n",
    "* ProducerConfig.BATCH_SIZE_CONFIG can be used to control the batch size.\n",
    "    * On the server on which program is running, data will be grouped based on the partition it need to send the data.\n",
    "    * Producer will establish connection to the brokers who are leaders for corresponding partition via bootstrap servers configured as part of the program.\n",
    "    * When batch size is reached, corresponding data will be sent to the leader of each of the partition.\n",
    "    * Leader will then write the first copy to the log file of the partition it is managing and will send the data to other followers as well.\n",
    "* ProducerConfig.COMPRESSION_TYPE_CONFIG can be used to specify compression algorithm such as gzip, snappy, lz4 etc.\n",
    "* There are settings to fine tune send buffer, receive buffer, buffer size for the batch etc.\n",
    "\n",
    "***Build as Application***\n",
    "\n",
    "As we have explored Producer APIs with REPL, now it is time for us to develop applications.\n",
    "* Here is the code which produces messages to a partitioned topic in round robin fashion. We will validate by consuming each partition separately to see the behavior that not all messages corresponding to the same key are stored in the same partition.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "name := \"KafkaWorkshop\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"1.0.0\"\n",
    "libraryDependencies += \"com.maxmind.geoip2\" % \"geoip2\" % \"2.12.0\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dev.zookeeper = localhost:2181\n",
    "dev.bootstrap.server = localhost:9092\n",
    "\n",
    "prod.zookeeper = nn01.itversity.com:2181,nn02.itversity.com:2181,nn03.itversity.com:2181\n",
    "prod.bootstrap.server = wn01.itversity.com:6667,wn02.itversity.com:6667"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFile-03-Program.scala\n",
    "\n",
    "package retail\n",
    "\n",
    "import java.util.Properties\n",
    "\n",
    "import scala.io.Source\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 30/10/18.\n",
    "  */\n",
    "object ProduceLogMessagesFromFile {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ProducerConfig.CLIENT_ID_CONFIG, \"Produce log messages from file\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "    val inputDir = args(1)\n",
    "    val topicName = args(2)\n",
    "\n",
    "    val logMessages = Source.fromFile(inputDir).getLines.toList\n",
    "    logMessages.foreach(message => {\n",
    "      val record = new ProducerRecord[String, String](topicName, message)\n",
    "      producer.send(record)\n",
    "    })\n",
    "\n",
    "    producer.close\n",
    "\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the improvised code which produces messages to a partitioned topic using the key. By default, it will apply hash on key (IP address) and then mod using the number of partitions. We can validate by consuming each partition separately to see that all messages related to the same IP are in its corresponding partition."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFileKey.scala\n",
    "\n",
    "package retail\n",
    "\n",
    "import java.util.Properties\n",
    "\n",
    "import scala.io.Source\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 30/10/18.\n",
    "  */\n",
    "object ProduceLogMessagesFromFileKey {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \n",
    "      envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ProducerConfig.CLIENT_ID_CONFIG, \n",
    "      \"Produce log messages from file\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "   \n",
    "    val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "    val inputDir = args(1)\n",
    "    val topicName = args(2)\n",
    "\n",
    "    val logMessages = Source.fromFile(inputDir).getLines.toList\n",
    "    logMessages.foreach(message => {\n",
    "      //Use ip address as key\n",
    "      val ipAddr = message.split(\" \")(0)\n",
    "      val record = new ProducerRecord[String, String](topicName, ipAddr, message)\n",
    "      producer.send(record)\n",
    "    })\n",
    "    producer.close\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is the improvised code which produce messages to Kafka Topic as per custom logic. This code uses geoip database and plugin to push US data to one partition and rest to other partitions. Messages with invalid ips are also pushed to a different topic."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ProduceLogMessagesFromFilePartition.scala\n",
    "\n",
    "package retail\n",
    "\n",
    "import java.util.Properties\n",
    "import java.io.File\n",
    "import com.maxmind.geoip2.DatabaseReader\n",
    "import java.net.InetAddress\n",
    "\n",
    "import scala.io.Source\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 30/10/18.\n",
    "  */\n",
    "object ProduceLogMessagesFromFilePartition {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "   \n",
    "    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \n",
    "      envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ProducerConfig.CLIENT_ID_CONFIG, \n",
    "      \"Produce log messages from file\")\n",
    "    props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "\n",
    "    val producer = new KafkaProducer[String, String](props)\n",
    "\n",
    "    val inputDir = args(1)\n",
    "    val topicName = args(2)\n",
    "\n",
    "    val logMessages = Source.fromFile(inputDir).getLines.toList\n",
    "\n",
    "    val database = new File(\"src/main/resources/db/maxmind/GeoLite2-Country.mmdb\")\n",
    "    val reader = new DatabaseReader.Builder(database).build\n",
    "\n",
    "    logMessages.foreach(message => {\n",
    "      try {\n",
    "        val ipAddr = message.split(\" \")(0)\n",
    "        val countryIsoCode = reader.\n",
    "          country(InetAddress.getByName(ipAddr)).\n",
    "          getCountry.\n",
    "          getIsoCode\n",
    "        val partitionIndex = if (countryIsoCode == \"US\") 2\n",
    "          else countryIsoCode.hashCode() % 2\n",
    "        val record = new ProducerRecord[String, String](topicName, partitionIndex, ipAddr, message)\n",
    "        producer.send(record)\n",
    "      } catch {\n",
    "        case e: Exception => {\n",
    "          val record = new ProducerRecord[String, String](topicName + \"_invalid\", message)\n",
    "          producer.send(record)\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "    producer.close\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also pass timestamp as well as partition index while building ProducerRecord. However, we will leave it to you as an exercise to explore and see the behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Consumer APIs – Partitioned Topic***\n",
    "\n",
    "As we have understood APIs associated with Producer in detail now it is time for us to explore Consumer APIs in detail.\n",
    "\n",
    "* Here are the steps to consume messages from Kafka Topic\n",
    "    * Create a Properties object with all relevant properties.\n",
    "    * Create KafkaConsumer object by passing Properties object.\n",
    "    * Subscribe to Topic or Assign to Topic Partition\n",
    "    * The poll in an infinite loop\n",
    "    * The poll will return ConsumerRecords object. It is a collection of ConsumerRecord objects.\n",
    "    * In a for loop, we can iterate through ConsumerRecords and perform necessary action.\n",
    "* Important APIs\n",
    "    * poll – takes an integer as an argument, maximum number of records pulled in each iteration\n",
    "    * subscribe – get data from all partitions of a given topic\n",
    "    * assign – get data from some partitions of a given topic\n",
    "    * commitSync and commitAsync – APIs for manual commit\n",
    "    * seek* – to rebalance the data from past offset\n",
    "    * position – offset of next record\n",
    "* consumer.poll will take the duration of a poll as an argument and return ConsumerRecords\n",
    "* Let us also review all the APIs that are available as part of ConsumerRecord using :<mark>javap -p ConsumerRecord</mark>\n",
    "* Data can be consumed by multiple consumers within each consumer group. While consumers in each group work in tandem by processing mutually exclusive subsets of data, each consumer group will process the same data.\n",
    "* Multiple consumer groups are used to process the same data for different purposes.\n",
    "\n",
    "![](https://i0.wp.com/kaizen.itversity.com/wp-content/uploads/2018/05/consumer-group.png?ssl=1)\n",
    "\n",
    "***Subscribing to Topic***\n",
    "\n",
    "As we have understood important APIs to consume messages from Kafka Topic, let us create an application using IDE and understand different partition assignment strategies.\n",
    "* If you subscribe one consumer to a Multi Partitioned Topic, data will be read from all partitions in round robin fashion\n",
    "* As the data is consumed, offset will be committed. In the subsequent runs using same consumer group with offset being earliest, data will be consumed from the last offset not earliest.\n",
    "* We can use seek to reset the offset, but it is not effective with subscribe against the partitioned topic.\n",
    "* If you subscribe multiple consumers to a Multi Partitioned Topic, by default range of the subset of partitions will be processed by each consumer.\n",
    "* We can also specify round robin to change the partition assignment strategy using **ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG** or **partition.assignment.strategy**(e.g.: props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, “org.apache.kafka.clients.consumer.RoundRobinAssignor”))\n",
    "* We can implement custom partition assignment strategy and use it by specifying the fully qualified class name as part of the above-mentioned property.\n",
    "* Here is the example of a consumer subscribing to a Topic. We will open two sessions and run two separate jobs simultaneously to understand the behavior."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ConsumeLogMessagesFromTopicSubscribe.scala \n",
    "\n",
    "package retail\n",
    "\n",
    "import java.util.{Properties, Collections}\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "object ConsumeLogMessagesFromTopicSubscribe {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n",
    "      envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ConsumerConfig.CLIENT_ID_CONFIG,\n",
    "      \"Consume Messages from Topic\")\n",
    "    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"dg30\")\n",
    "    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n",
    "    // props.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG,\n",
    "    //  \"range\")\n",
    "\n",
    "    val consumer = new KafkaConsumer[String, String](props)\n",
    "    consumer.subscribe(Collections.singletonList(args(1)))\n",
    "\n",
    "    while(true) {\n",
    "      val records = consumer.poll(500)\n",
    "      for(record <- records) {\n",
    "        println(\"Received message: (\" + record.key()\n",
    "          + \", \" + record.value()\n",
    "          + \") from \" + record.partition()\n",
    "          + \" at offset \" + record.offset())\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Assigning Topic Partition***\n",
    "\n",
    "Let us understand how to assign a particular Topic Partition to a Consumer Group.\n",
    "\n",
    "* We can also assign specific partitions to each of the consumers in a consumer group using assign. It takes the collection of TopicPartition as an argument. We can build TopicPartition object using topic name along with the partition index.\n",
    "* Here is the example where a partition is assigned to a Consumer Group."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// ConsumeLogMessagesFromTopic-assign-partition.scala\n",
    "\n",
    "package retail\n",
    "\n",
    "import java.util.{Collections, Properties}\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n",
    "import org.apache.kafka.common.TopicPartition\n",
    "\n",
    "import scala.collection.JavaConversions._\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 31/10/18.\n",
    "  */\n",
    "object ConsumeLogMessagesFromTopic {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load\n",
    "    val envProps = conf.getConfig(args(0))\n",
    "    val props = new Properties()\n",
    "\n",
    "    props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n",
    "      envProps.getString(\"bootstrap.server\"))\n",
    "    props.put(ConsumerConfig.CLIENT_ID_CONFIG,\n",
    "      \"Consume messages from topic\")\n",
    "    props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "    props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\n",
    "      \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "\n",
    "    props.put(ConsumerConfig.GROUP_ID_CONFIG, \"cgb3\")\n",
    "    props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\")\n",
    "    props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\")\n",
    "\n",
    "    val consumer = new KafkaConsumer[String, String](props)\n",
    "    val partitions = Set(new TopicPartition(args(1), args(2).toInt))\n",
    "    consumer.assign(partitions)\n",
    "    consumer.seekToBeginning(consumer.assignment())\n",
    "\n",
    "    while(true) {\n",
    "      val records = consumer.poll(500)\n",
    "      for(record <- records) {\n",
    "        println(\"Received message: (\" + record.key()\n",
    "          + \", \" + record.value()\n",
    "          + \") from \" + record.partition()\n",
    "          + \" at offset \" + record.offset())\n",
    "      }\n",
    "      Thread.sleep(100)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also implement an assignment strategy of assigning all even partitions to one consumer while odd partitions to other. However, we are leaving it as an exercise for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer APIs – Managing Commits\n",
    "Now let us see how we can perform commit as part of consumers.\n",
    "\n",
    "* Commit is nothing but saving offset periodically. The offset will be committed to internal topics managed by Kafka.\n",
    "* By default auto-commit is enabled, however, we can disable by setting **enable.auto.commit** to false.\n",
    "* If you look at message structure, for each message being consumed include topic name, partition name, offset and other information.\n",
    "* If auto-commit is enabled, offset of the last message returned by-poll will be committed after processing all the messages.\n",
    "* If you poll every 5 seconds, then commit will happen every 5 seconds\n",
    "* This can cause duplicates while reprocessing of data if there are any failures as part of the batch. Reprocessing is also called as rebalancing.\n",
    "* However, based upon the nature of data we might want to commit as per pre-defined logic manually.\n",
    "* With manual commit, Kafka supports commitSync as well as commitAsync.\n",
    "* If your consumer needs to commit very often, then the overhead of committing using commitSync is considerable as the consumer need to wait until commit is complete to process new data.\n",
    "commitAsync will just submit commit request and proceed to process next batch of data.\n",
    "* We can pass the current offset as an argument to commitSync as well as commitAsync as part of the manual commit process.\n",
    "\n",
    "### Serialization and Deserialization\n",
    "Let us understand details with respect to Serialization and Deserialization.\n",
    "\n",
    "* Main package – **org.apache.kafka.common.serialization**\n",
    "* Serialization means converting an object to byte stream so that object can live outside the scope of JVM.\n",
    "* Once serialized we can either persist into files or we can copy over a network.\n",
    "* As Java classes for primitive types are not serialized, most of the technologies like Kafka, Hadoop etc come up with their own serialization mechanism.\n",
    "* You can get list of serialized classes from **org.apache.kafka.common.serialization**\n",
    "* Deserialization is the process of reconstructing the object using byte stream.\n",
    "* We use Serializers as part of producer application and Deserializers as part of consumer applications.\n",
    "\n",
    "### Running as Fat Jar\n",
    "Let us see how we can build fat jar and run application with out defining the dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
