{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Demo – Build Streaming Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we understand all the moving parts, such as Kafka, Spark Structured Streaming, HBase etc using Scala as programming language – let’s go ahead and build end to end streaming data pipeline.\n",
    "\n",
    "* Pre-Requisites\n",
    "* Problem Statement\n",
    "* Design the Solution\n",
    "* Setup Project\n",
    "* Data Ingestion – Kafka Connect\n",
    "* Data Processing – Spark Structured Streaming\n",
    "* Integrate with HBase\n",
    "\n",
    "### Pre-Requisites\n",
    "Let us see the Pre-Requisites to execute this demo in our PC.\n",
    "* 64 Bit Computer with 64 Bit Operating System\n",
    "* At least 4 GB RAM and Dual Core\n",
    "* If Windows, you need to have Ubuntu setup using Windows Subsystem\n",
    "* Setup all the required software to build streaming pipelines. Follow the video to understand the process of validation.\n",
    "    * gen_logs – an eCommerce application simulator which adds log messages to a log file\n",
    "    * Apache Spark – Ensure Spark is Setup\n",
    "    * Apache Kafka – Make sure Zookeeper and Kafka are running without any issues.\n",
    "    * Apache HBase – Make sure HBase is running with out any issues.\n",
    "* If you are using Ubuntu on Windows (setup with subsystem for Linux), you might run into issues such as corruption of Zookeeper and Kafka. You just have to clean up as demonstrated earlier and restart.\n",
    "* Make sure to have IntelliJ as IDE to develop the applications.\n",
    "* We will develop an application using IntelliJ and validate using Terminal.\n",
    "* Before we get into the execution we need to understand fundamentals about Spark, HBase, and Kafka and should be proficient in programming using Scala.\n",
    "\n",
    "### Problem Statement\n",
    "Let us go through the problem statement for the end to end data pipelines.\n",
    "* We have gen_logs which are generating log data in streaming fashion. It is a simulator for eCommerce Web application traffic.\n",
    "* Messages are logged using the standard log message format.\n",
    "* We want to get department wise traffic for every minute so that we can understand the performance of the department in real time.\n",
    "* Data should be stored in a database so that reports can be generated.\n",
    "\n",
    "### Design the Solution\n",
    "Let us come up with the design to get department wise count into HBase table at regular intervals.\n",
    "* Make sure data is generated to log files\n",
    "* Data Ingestion – Ingest data to Kafka Topic using Kafka Connect. We have chosen Kafka Connect over Flume or logstash because we want to ingest data into Kafka Topic.\n",
    "* We can also use Flume or logstash for this purpose.\n",
    "* Develop logic to consume data and process using Spark Structured Streaming. We can use technologies like Flink or Storm for the same purpose.\n",
    "* Processed Data should be persisted to HBase table. We have chosen HBase as it is already available in our cluster along with Spark.\n",
    "* HBase Data Model\n",
    "    * Row Key: Date:Department\n",
    "    * Column Key: Date with time up to a minute\n",
    "    * Column Value: Count for a given minute\n",
    "* This will facilitate us to use HBase filters to access data faster for reports such as minute wise traffic to day-wise traffic. We need to process data while building a report based on the requirements.\n",
    "\n",
    "### Setup Project\n",
    "We have 2 components for this project – Data Ingestion and Data Processing.\n",
    "* For Data Ingestion we are going to use Kafka Connect, which is available out of the box.\n",
    "* For Data Processing we will be using Spark Structured Streaming.\n",
    "    * Data will be read from Kafka Topic\n",
    "    * Data will be processed using Spark Structured Streaming APIs\n",
    "    * Data will be written to HBase\n",
    "* We will create a new project – streaming-pipelines\n",
    "* Add assembly plugin, so that we can build a fat jar\n",
    "    * Create **assembly.sbt** file in the **project** directory.\n",
    "    * Add this line of code – <mark>addSbtPlugin(\"com.eed3si9n\" % \"sbt-assembly\" % \"0.14.9\")</mark>\n",
    "* Add type safe config dependency so that we can externalize properties\n",
    "* Add necessary dependencies for Kafka and Spark Structured Streaming\n",
    "* Add necessary dependencies for HBase\n",
    "* Define all dependencies as part of build.sbt with merge strategy for assembly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// kafka-structured-streaming-build.sbt \n",
    "\n",
    "name := \"streaming-pipelines\"\n",
    "\n",
    "version := \"1.0\"\n",
    "\n",
    "scalaVersion := \"2.11.12\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-yarn\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % \"2.3.0\"\n",
    "libraryDependencies += \"org.apache.kafka\" % \"kafka-clients\" % \"1.0.0\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-client\" % \"1.1.8\"\n",
    "libraryDependencies += \"org.apache.hbase\" % \"hbase-common\" % \"1.1.8\"\n",
    "\n",
    "assemblyMergeStrategy in assembly := {\n",
    "  case m if m.toLowerCase.endsWith(\"manifest.mf\") => MergeStrategy.discard\n",
    "  case m if m.startsWith(\"META-INF\") => MergeStrategy.discard\n",
    "  case PathList(\"javax\", \"servlet\", xs@_*) => MergeStrategy.first\n",
    "  case PathList(\"org\", \"apache\", xs@_*) => MergeStrategy.first\n",
    "  case \"about.html\" => MergeStrategy.rename\n",
    "  case \"reference.conf\" => MergeStrategy.concat\n",
    "  case _ => MergeStrategy.first\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are using com.typesafe config for externalizing the properties. It can read the files from src/main/resources with names such as application.properties and build Config object.\n",
    "* Let us define all the properties that are required for our application to run in dev and prod."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-structured-streaming-application.properties\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.bootstrap.servers = localhost:9092\n",
    "dev.zookeeper.quorum = localhost\n",
    "dev.zookeeper.port = 2181\n",
    "\n",
    "prod.execution.mode = yarn\n",
    "prod.bootstrap.servers = wn01.itversity.com:6667,wn02.itversity.com:6667\n",
    "prod.zookeeper.quorum = nn01.itversity.com,nn02.itversity.com,rm01.itversity.com\n",
    "prod.zookeeper.port = 2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Ingestion – Kafka Connect\n",
    "Let us understand how we can get data from Web Server logs into Kafka Topic using Kafka Connect.\n",
    "* We can either use standalone or distributed mode for Kafka connect worker\n",
    "* Examples are available under ```$KAFKA_CONF_DIR```\n",
    "* Kafka support a bunch of sources and sinks as part of connect\n",
    "* To get data from log files we need to use the file as source\n",
    "* Example for file source is available under $KAFKA_CONF_DIR/connect-file-source.properties\n",
    "* We can start the worker by passing 2 arguments – one worker mode and another source type.\n",
    "* We can validate by consuming data using kafka-console-consumer.sh\n",
    "* Below code snippet is created for our labs, however, you can make cosmetic changes as demonstrated and use this for local setup as well. Instead of using DNS Aliases related to lab we need to use localhost to run these locally on our PC."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-connect-lab-01-create-topic.sh\n",
    "\n",
    "kafka-topics.sh \\\n",
    "  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \\\n",
    "  --delete \\\n",
    "  --topic retail\n",
    "  \n",
    "kafka-topics.sh \\\n",
    "  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \\\n",
    "  --create \\\n",
    "  --topic retail \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# File Name: connect-standalone.properties\n",
    "bootstrap.servers=wn01.itversity.com:6667,wn02.itversity.com:6667\n",
    "\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "\n",
    "key.converter.schemas.enable=true\n",
    "value.converter.schemas.enable=true\n",
    "\n",
    "internal.key.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "internal.value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "internal.key.converter.schemas.enable=false\n",
    "internal.value.converter.schemas.enable=false\n",
    "\n",
    "offset.storage.file.filename=/tmp/connect.offsets\n",
    "offset.flush.interval.ms=10000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# File Name: connect-retail-logs.properties\n",
    "name=retail-local\n",
    "connector.class=FileStreamSource\n",
    "tasks.max=1\n",
    "file=/opt/gen_logs/logs/access.log\n",
    "topic=retail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-connect-lab-04-start-kafka-connect.sh\n",
    "\n",
    "connect-standalone.sh \\\n",
    "  connect-standalone.properties \\\n",
    "  connect-retail-logs.properties"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-connect-lab-05-validate.sh\n",
    "\n",
    "kafka-console-consumer.sh \\\n",
    "  --bootstrap-server wn01.itversity.com:6667,wn02.itversity.com:6667 \\\n",
    "  --topic retail \\\n",
    "  --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing – Spark Structured Streaming\n",
    "Let us understand how we can apply Spark Structure Streaming APIs to get Streaming Department Count every minute.\n",
    "* There are 4 different discrete components as part of Streaming Application.\n",
    "    * Creating Context (SparkSession object is a wrapper for SparkContext)\n",
    "    * Reading Data\n",
    "    * Processing Data\n",
    "    * Writing Data\n",
    "    * You need to separate these 3 and focus on relevant areas.\n",
    "\n",
    "***Create SparkContext***\n",
    "\n",
    "We will see how to read externalized properties and create SparkSession object.\n",
    "* Make sure externalized properties are read using typesafe config APIs.\n",
    "* Create SparkSession object (spark)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-pipelines-create-spark-context.scala\n",
    "\n",
    "import com.typesafe.config._\n",
    "\n",
    "val conf = ConfigFactory.load.getConfig(\"dev\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  appName(\"Get Streaming Department Traffic\").\n",
    "  master(conf.getString(\"execution.mode\")).\n",
    "  getOrCreate\n",
    "  \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Read Data***\n",
    "Let us explore relevant APIs to read data in streaming fashion.\n",
    "* Read data from Kafka Topic using Spark Structured Streaming APIs – **spark.readStream**.\n",
    "* Let us read the data and print the lines without any data processing.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-reading-data-from-kafka.scala \n",
    "\n",
    "import com.typesafe.config._\n",
    "\n",
    "val conf = ConfigFactory.load.getConfig(\"dev\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  appName(\"Get Streaming Department Traffic\").\n",
    "  master(conf.getString(\"execution.mode\")).\n",
    "  getOrCreate\n",
    "  \n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val lines = spark.\n",
    "  readStream.\n",
    "  format(\"kafka\").\n",
    "  option(\"kafka.bootstrap.servers\", conf.getString(\"bootstrap.servers\")).\n",
    "  option(\"subscribe\", \"retail\").\n",
    "  load().\n",
    "  selectExpr(\"CAST(value AS STRING)\").\n",
    "  as[(String)]\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = lines.\n",
    "  writeStream.\n",
    "  format(\"console\").\n",
    "  outputMode(\"append\").\n",
    "  trigger(Trigger.ProcessingTime(\"10 seconds\")).\n",
    "  start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Process Data – Baseline***\n",
    "\n",
    "Let us explore relevant Data Frame APIs to process data. We can also use Spark SQL approach and write queries to process the data.\n",
    "* As we have to kill spark context to come out of streaming context, we will first apply data processing logic using batch approach and then we will add as part of the streaming pipeline.\n",
    "* We will use the date that is logged as part of log messages in the log file to get department wise count for every minute. However, the date is not logged in the standard timestamp format and hence we need to come up with logic to convert the date in string type to timestamp type.\n",
    "* Also, we will filter for department records and then extract department_name along with the timestamp.\n",
    "* We will also apply Data Frame APIs to get count by department every minute."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-pipelines-data-processing-logic.scala\n",
    "\n",
    "import com.typesafe.config._\n",
    "\n",
    "val conf = ConfigFactory.load.getConfig(\"dev\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  appName(\"Get Streaming Department Traffic\").\n",
    "  master(conf.getString(\"execution.mode\")).\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val lines = spark.read.text(\"/opt/gen_logs/logs/access.log\")\n",
    "\n",
    "import org.apache.spark.sql.functions.{split, to_timestamp, ltrim}\n",
    "\n",
    "val departmentLines = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).  \n",
    "  withColumn(\"visit_time\", to_timestamp(ltrim(split($\"value\", \" \")(3), \"[\"), \"dd/MMM/yyyy:HH:mm\")).\n",
    "  drop($\"value\")\n",
    "\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "val departmentTraffic = departmentLines.\n",
    "  groupBy(\"visit_time\", \"department_name\").\n",
    "  agg(count(\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "departmentTraffic.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Process Data – Streaming***\n",
    "\n",
    "* Now let us refactor the code to get the department wise traffic in streaming fashion.We need to use Window Operations.\n",
    "* We can use timestamp as part of window operation and get department wise count at regular intervals in streaming fashion."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-pipelines-data-processing-streaming.scala\n",
    "\n",
    "import com.typesafe.config._\n",
    "\n",
    "val conf = ConfigFactory.load.getConfig(\"dev\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  appName(\"Get Streaming Department Traffic\").\n",
    "  master(conf.getString(\"execution.mode\")).\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val lines = spark.\n",
    "  readStream.\n",
    "  format(\"kafka\").\n",
    "  option(\"kafka.bootstrap.servers\", conf.getString(\"bootstrap.servers\")).\n",
    "  option(\"subscribe\", \"retail\").\n",
    "  load.\n",
    "  select($\"value\".cast(\"string\").alias(\"value\"))\n",
    "\n",
    "import org.apache.spark.sql.functions.{split, to_timestamp, ltrim}\n",
    "\n",
    "val departmentLines = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).  \n",
    "  withColumn(\"visit_time\", to_timestamp(ltrim(split($\"value\", \" \")(3), \"[\"), \"dd/MMM/yyyy:HH:mm\")).\n",
    "  drop($\"value\")\n",
    "\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "val departmentTraffic = departmentLines.\n",
    "  groupBy(\"visit_time\", \"department_name\").\n",
    "  agg(count(\"department_name\").alias(\"department_count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Write Data***\n",
    "\n",
    "Let us see some of the key aspects of writing the data in streaming fashion after applying the processing logic.\n",
    "* Print the count on the screen every 20 seconds."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// streaming-pipelines-complete-life-cycle.scala \n",
    "\n",
    "import com.typesafe.config._\n",
    "\n",
    "val conf = ConfigFactory.load.getConfig(\"dev\")\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "  builder.\n",
    "  appName(\"Get Streaming Department Traffic\").\n",
    "  master(conf.getString(\"execution.mode\")).\n",
    "  getOrCreate\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val lines = spark.\n",
    "  readStream.\n",
    "  format(\"kafka\").\n",
    "  option(\"kafka.bootstrap.servers\", conf.getString(\"bootstrap.servers\")).\n",
    "  option(\"subscribe\", \"retail\").\n",
    "  load.\n",
    "  select($\"value\".cast(\"string\").alias(\"value\"))\n",
    "\n",
    "import org.apache.spark.sql.functions.{split, to_timestamp, ltrim}\n",
    "\n",
    "val departmentLines = lines.\n",
    "  where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "  withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).  \n",
    "  withColumn(\"visit_time\", to_timestamp(ltrim(split($\"value\", \" \")(3), \"[\"), \"dd/MMM/yyyy:HH:mm\")).\n",
    "  drop($\"value\")\n",
    "\n",
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "val departmentTraffic = departmentLines.\n",
    "  groupBy(\"visit_time\", \"department_name\").\n",
    "  agg(count(\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "val query = departmentTraffic.\n",
    "  writeStream.\n",
    "  format(\"console\").\n",
    "  option(\"truncate\", \"false\").\n",
    "  outputMode(\"update\").\n",
    "  trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "  start\n",
    "\n",
    "query.awaitTermination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate with HBase\n",
    "Let us understand how to load the data into HBase table instead of printing on to the console. We will start with code from the previous topic and then take care of the integration of output with HBase.\n",
    "\n",
    "***Using IDE***\n",
    "Let us take the code from the previous topic and create a program using IDE.\n",
    "* Create program – **GetStreamingDepartmentTrafficHBase**\n",
    "* Make sure all the properties defined in application.properties are correct.\n",
    "* Add the code to the program.\n",
    "* At this juncture, we are ready to refactor the code to save the output to HBase Table.\n",
    "\n",
    "***Implement ForeachWriter***\n",
    "\n",
    "Let us implement ForeachWriter to integrate with HBase. These steps can be followed to integrate with any database as long as plugins are available.\n",
    "\n",
    "* We need to use foreach to write data into the Database using Spark Structured streaming.\n",
    "* **foreach** is available as part of df.writeStream\n",
    "* We need to pass a custom writer of type **ForeachWriter[Row]**\n",
    "* We need to implement 3 APIs – **open, process** and **close**\n",
    "* For each Data Frame partition\n",
    "    * **open** – Database connection will be established\n",
    "    * **process** – Data will be processed one at a time\n",
    "    * **close** – Database connection will be closed\n",
    "\n",
    "***Validate Locally***\n",
    "\n",
    "As development is done, now we can build the jar file and validate using spark-submit.\n",
    "\n",
    "* Make necessary changes to the code related to connecting to the HBase cluster in distributed mode.\n",
    "* Go to the project directory and run <mark>sbt assembly</mark> to build a fat jar.\n",
    "* Ensure that Kafka connect is running and data is being ingested to Kafka Topic.\n",
    "* Ensure HBase is up and running\n",
    "* Create HBase table **department_count** with column family **cf – <mark>create 'department_count', 'cf'</mark>\n",
    "* Run spark-submit command to run the streaming pipeline where the output is stored in the HBase table.\n",
    "* Go to **hbase shell** and validate to ensure that data is flowing in without any issues.\n",
    "\n",
    "**Run on Cluster**\n",
    "\n",
    "Let us see the steps for validating streaming pipeline job on the cluster.\n",
    "* Ship the jar file to the cluster (using scp command)\n",
    "* Ensure that Kafka Connect is running and ingesting data from web server logs to Kafka Topic"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-connect-lab-01-create-topic.sh\n",
    "\n",
    "kafka-topics.sh \\\n",
    "  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \\\n",
    "  --delete \\\n",
    "  --topic retail\n",
    "  \n",
    "kafka-topics.sh \\\n",
    "  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \\\n",
    "  --create \\\n",
    "  --topic retail \\\n",
    "  --partitions 3 \\\n",
    "  --replication-factor 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# File Name: connect-standalone.properties\n",
    "bootstrap.servers=wn01.itversity.com:6667,wn02.itversity.com:6667\n",
    "\n",
    "key.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "value.converter=org.apache.kafka.connect.storage.StringConverter\n",
    "\n",
    "key.converter.schemas.enable=true\n",
    "value.converter.schemas.enable=true\n",
    "\n",
    "internal.key.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "internal.value.converter=org.apache.kafka.connect.json.JsonConverter\n",
    "internal.key.converter.schemas.enable=false\n",
    "internal.value.converter.schemas.enable=false\n",
    "\n",
    "offset.storage.file.filename=/tmp/connect.offsets\n",
    "offset.flush.interval.ms=10000"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#File Name: connect-retail-logs.properties\n",
    "name=retail-local\n",
    "connector.class=FileStreamSource\n",
    "tasks.max=1\n",
    "file=/opt/gen_logs/logs/access.log\n",
    "topic=retail"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "connect-standalone.sh \\\n",
    "  connect-standalone.properties \\\n",
    "  connect-retail-logs.properties"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "kafka-console-consumer.sh \\\n",
    "  --bootstrap-server wn01.itversity.com:6667,wn02.itversity.com:6667 \\\n",
    "  --topic retail \\\n",
    "  --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create HBase table department_count with column family cf – <mark>create 'department_count', 'cf'</mark>\n",
    "* Run spark-submit command to run the streaming pipeline where the output is stored in the HBase table.\n",
    "* Go to **hbase shell** and validate to ensure that data is flowing in without any issues."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// kafka-structured-streaming-01-GetStreamingDepartmentTrafficHBase.scala\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.sql.{ForeachWriter, SparkSession}\n",
    "import org.apache.spark.sql.functions.{count, ltrim, split, to_timestamp}\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.conf.Configuration\n",
    "import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}\n",
    "import org.apache.hadoop.hbase.client.{Connection, ConnectionFactory, Get, Put}\n",
    "import org.apache.hadoop.hbase.util.Bytes\n",
    "import org.apache.spark.sql.execution.streaming.FileStreamSource.Timestamp\n",
    "\n",
    "object GetStreamingDepartmentTrafficHBase {\n",
    "\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = ConfigFactory.load.getConfig(args(0))\n",
    "\n",
    "    val spark = SparkSession.\n",
    "      builder.\n",
    "      appName(\"Get Streaming Department Traffic\").\n",
    "      master(conf.getString(\"execution.mode\")).\n",
    "      getOrCreate\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val lines = spark.\n",
    "      readStream.\n",
    "      format(\"kafka\").\n",
    "      option(\"kafka.bootstrap.servers\", conf.getString(\"bootstrap.servers\")).\n",
    "      option(\"subscribe\", \"retail\").\n",
    "      load.\n",
    "      select($\"value\".cast(\"string\").alias(\"value\"))\n",
    "\n",
    "    val departmentLines = lines.\n",
    "      where(split(split($\"value\", \" \")(6), \"/\")(1) === \"department\").\n",
    "      withColumn(\"department_name\", split(split($\"value\", \" \")(6), \"/\")(2)).\n",
    "      withColumn(\"visit_time\", to_timestamp(ltrim(split($\"value\", \" \")(3), \"[\"), \"dd/MMM/yyyy:HH:mm\")).\n",
    "      drop($\"value\")\n",
    "\n",
    "    val departmentTraffic = departmentLines.\n",
    "      groupBy(\"visit_time\", \"department_name\").\n",
    "      agg(count(\"department_name\").alias(\"department_count\"))\n",
    "\n",
    "    val writer = new ForeachWriter[Row] {\n",
    "      var hbaseConf: Configuration = _\n",
    "      var connection: Connection = _\n",
    "\n",
    "      override def open(partitionId: Long, version: Long): Boolean = {\n",
    "        hbaseConf = HBaseConfiguration.create()\n",
    "        hbaseConf.set(\"hbase.zookeeper.quorum\", conf.getString(\"zookeeper.quorum\"))\n",
    "        hbaseConf.set(\"hbase.zookeeper.property.clientPort\", conf.getString(\"zookeeper.port\"))\n",
    "        \n",
    "        if(env != \"dev\") {\n",
    "          hbaseConf.set(\"zookeeper.znode.parent\", \"/hbase-unsecure\")\n",
    "          hbaseConf.set(\"hbase.cluster.distributed\", \"true\")\n",
    "        }\n",
    "        \n",
    "        connection = ConnectionFactory.createConnection(hbaseConf)\n",
    "        true\n",
    "      }\n",
    "\n",
    "      override def process(value: Row): Unit = {\n",
    "        val table = connection.getTable(TableName.valueOf(\"department_count\"))\n",
    "\n",
    "        val rowKey = Bytes.toBytes(value.\n",
    "          getAs[Timestamp](\"visit_time\").\n",
    "          toString.\n",
    "          split(\" \")(0) + \":\" +\n",
    "          value.getAs[String](\"department_name\"))\n",
    "        val row = new Put(rowKey)\n",
    "\n",
    "        row.addColumn(Bytes.toBytes(\"cf\"),\n",
    "          Bytes.toBytes(value.getAs[Timestamp](\"visit_time\").toString),\n",
    "          Bytes.toBytes(value.getAs[Long](\"department_count\").toString))\n",
    "\n",
    "        table.put(row)\n",
    "\n",
    "      }\n",
    "\n",
    "      override def close(errorOrNull: Throwable): Unit = {\n",
    "        connection.close()\n",
    "      }\n",
    "    }\n",
    "\n",
    "    val query = departmentTraffic.\n",
    "      writeStream.\n",
    "      foreach(writer).\n",
    "      outputMode(\"update\").\n",
    "      trigger(Trigger.ProcessingTime(\"20 seconds\")).\n",
    "      start\n",
    "\n",
    "    query.awaitTermination\n",
    "\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sbt assembly"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# kafka-structured-streaming-03-spark-submit.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --class GetStreamingDepartmentTrafficHBase \\\n",
    "  --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 \\\n",
    "  target/scala-2.11/StructuredStreamingDemo-assembly-1.0.jar prod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
