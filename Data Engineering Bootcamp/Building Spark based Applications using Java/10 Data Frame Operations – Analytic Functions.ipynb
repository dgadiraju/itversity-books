{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frame Operations – Analytic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session, we will see advanced operations such as aggregations, ranking, and windowing functions within each group using APIs such as over, partitionBy etc. We will also build a solution to the problem and run it on a multinode cluster.\n",
    "\n",
    "* Window Functions – APIs\n",
    "* Problem Statement – Get top n products per day\n",
    "* Creating Window Spec\n",
    "* Performing Aggregations\n",
    "* Using Windowing Functions\n",
    "* Ranking Functions\n",
    "* Development Life Cycle\n",
    "\n",
    "### Window Functions – APIs\n",
    "\n",
    "Let us understand APIs related to aggregations, ranking and windowing functions.\n",
    "\n",
    "* Main package **org.apache.spark.sql.expressions**\n",
    "* It has classes such as **Window** and **WindowSpec**\n",
    "* Window have APIs such as **partitionBy, orderBy** etc\n",
    "* These APIs (such as partitionBy) return WindowSpec object. We can pass **WindowSpec** object to **over** on functions such as **rank(), dense_rank(), sum()** etc\n",
    "* Syntax: **rank().over(spec) where spec = Window.partitionBy(‘ColumnName’)**\n",
    "* Aggregations – **sum, avg, min, max** etc\n",
    "* Ranking – **rank, dense_rank, row_number** etc\n",
    "* Windowing – **lead, lag** etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- order_item_order_id: long (nullable = true)\n",
      " |-- order_item_product_id: long (nullable = true)\n",
      " |-- order_item_product_price: double (nullable = true)\n",
      " |-- order_item_quantity: long (nullable = true)\n",
      " |-- order_item_subtotal: double (nullable = true)\n",
      " |-- order_revenue: double (nullable = true)\n",
      "\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+------------------+\n",
      "|order_item_id|order_item_order_id|order_item_product_id|order_item_product_price|order_item_quantity|order_item_subtotal|     order_revenue|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+------------------+\n",
      "|            2|                  2|                 1073|                  199.99|                  1|             199.99|            579.98|\n",
      "|            3|                  2|                  502|                    50.0|                  5|              250.0|            579.98|\n",
      "|            4|                  2|                  403|                  129.99|                  1|             129.99|            579.98|\n",
      "|            5|                  4|                  897|                   24.99|                  2|              49.98|            699.85|\n",
      "|            6|                  4|                  365|                   59.99|                  5|             299.95|            699.85|\n",
      "|            7|                  4|                  502|                    50.0|                  3|              150.0|            699.85|\n",
      "|            8|                  4|                 1014|                   49.98|                  4|             199.92|            699.85|\n",
      "|            9|                  5|                  957|                  299.98|                  1|             299.98|1129.8600000000001|\n",
      "|           10|                  5|                  365|                   59.99|                  5|             299.95|1129.8600000000001|\n",
      "|           11|                  5|                 1014|                   49.98|                  2|              99.96|1129.8600000000001|\n",
      "|           12|                  5|                  957|                  299.98|                  1|             299.98|1129.8600000000001|\n",
      "|           13|                  5|                  403|                  129.99|                  1|             129.99|1129.8600000000001|\n",
      "|           17|                  8|                  365|                   59.99|                  3|             179.97| 729.8399999999999|\n",
      "|           18|                  8|                  365|                   59.99|                  5|             299.95| 729.8399999999999|\n",
      "|           19|                  8|                 1014|                   49.98|                  4|             199.92| 729.8399999999999|\n",
      "|           20|                  8|                  502|                    50.0|                  1|               50.0| 729.8399999999999|\n",
      "|           34|                 12|                  957|                  299.98|                  1|             299.98|1299.8700000000001|\n",
      "|           35|                 12|                  134|                    25.0|                  4|              100.0|1299.8700000000001|\n",
      "|           36|                 12|                 1014|                   49.98|                  3|             149.94|1299.8700000000001|\n",
      "|           37|                 12|                  191|                   99.99|                  5|             499.95|1299.8700000000001|\n",
      "+-------------+-------------------+---------------------+------------------------+-------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderItems = [order_item_id: bigint, order_item_order_id: bigint ... 4 more fields]\n",
       "spec = org.apache.spark.sql.expressions.WindowSpec@7997266d\n",
       "orderItemsWithRevenue = [order_item_id: bigint, order_item_order_id: bigint ... 5 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_item_id: bigint, order_item_order_id: bigint ... 5 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItems = spark.\n",
    "  read.\n",
    "  json(\"/public/retail_db_json/order_items\")\n",
    "\n",
    "import org.apache.spark.sql.expressions._\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val spec = Window.partitionBy(\"order_item_order_id\")\n",
    "val orderItemsWithRevenue = orderItems.\n",
    "  withColumn(\"order_revenue\", sum($\"order_item_subtotal\").over(spec))\n",
    "\n",
    "orderItemsWithRevenue.printSchema\n",
    "orderItemsWithRevenue.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement – Get top n products per day\n",
    "\n",
    "Let us define the problem statement and see the real usage of analytics function.\n",
    "\n",
    "* Problem Statement – Get top N Products Per day\n",
    "* Get daily product revenue code from the previous topic\n",
    "* Use ranking functions and get the rank associated based on revenue for each day\n",
    "* Once we get rank, let us filter for top n products."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-01-build.sbt\n",
    "\n",
    "name := \"SparkDemo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.2\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-02-application.properties\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.input.base.dir = C:\\\\data\\\\retail_db\\\\\n",
    "dev.output.base.dir = C:\\\\data\\\\scalaspark\\\\\n",
    "\n",
    "devu.execution.mode = local\n",
    "devu.input.base.dir = /mnt/c/data/retail_db/\n",
    "devu.output.base.dir = /mnt/c/data/bootcamp/scalaspark/\n",
    "\n",
    "devm.execution.mode = local\n",
    "devm.input.base.dir = /Users/itversity/Research/data/retail_db/\n",
    "devm.output.base.dir = /Users/itversity/Research/data/scalaspark/\n",
    "\n",
    "prod.execution.mode = yarn-client\n",
    "prod.input.base.dir = /public/retail_db/\n",
    "prod.output.base.dir = /user/training/bootcamp/scalaspark/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-dataframes-03-daily-product-revenue.scala\n",
    "\n",
    "package retail_db_df\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{round, sum}\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 15/12/18.\n",
    "  */\n",
    "object GetDailyProductRevenueDF {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val props = ConfigFactory.load\n",
    "    val env = args(0)\n",
    "    val envProps = props.getConfig(env)\n",
    "\n",
    "    val spark = SparkSession.\n",
    "    builder.\n",
    "    appName(\"Daily Product Revenue using Data Frame Operations\").\n",
    "    master(envProps.getString(\"execution.mode\")).\n",
    "    getOrCreate\n",
    "\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "    val inputBaseDir = envProps.getString(\"input.base.dir\")\n",
    "    val ordersCSV = spark.read.\n",
    "      csv(inputBaseDir + \"orders\").\n",
    "      toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "    val orderItemsCSV = spark.read.\n",
    "      csv(inputBaseDir + \"order_items\").\n",
    "      toDF(\"order_item_id\", \"order_item_order_id\", \"order_item_product_id\",\n",
    "      \"order_item_quantity\", \"order_item_subtotal\", \"order_item_product_price\")\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val orders = ordersCSV.\n",
    "      withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "      withColumn(\"order_customer_id\", $\"order_customer_id\".cast(\"int\"))\n",
    "\n",
    "    val orderItems = orderItemsCSV.\n",
    "      withColumn(\"order_item_id\", $\"order_item_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_order_id\", $\"order_item_order_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_product_id\", $\"order_item_product_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_quantity\", $\"order_item_quantity\".cast(\"int\")).\n",
    "      withColumn(\"order_item_subtotal\", $\"order_item_subtotal\".cast(\"float\")).\n",
    "      withColumn(\"order_item_product_price\", $\"order_item_product_price\".cast(\"float\"))\n",
    "\n",
    "    val dailyProductRevenue = orders.\n",
    "      where(\"order_status in ('COMPLETE', 'CLOSED')\").\n",
    "      join(orderItems, $\"order_id\" === $\"order_item_order_id\").\n",
    "      groupBy(\"order_date\", \"order_item_product_id\").\n",
    "      agg(round(sum($\"order_item_subtotal\"), 2).alias(\"revenue\"))\n",
    "\n",
    "    val dailyProductRevenueSorted = dailyProductRevenue.\n",
    "      orderBy($\"order_date\", $\"revenue\".desc)\n",
    "\n",
    "    val outputBaseDir = envProps.getString(\"output.base.dir\")\n",
    "    dailyProductRevenueSorted.write.csv(outputBaseDir + \"daily_product_revenue\")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-04-daily-product-revenue.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --class retail_db_df.GetDailyProductRevenueDF \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  sparkdemo_2.11-0.1.jar prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Window Spec\n",
    "\n",
    "Let us see how to create Window Spec.\n",
    "\n",
    "* Window have APIs such as partitionBy, orderBy\n",
    "* For aggregations, we can define the group by using partitionBy\n",
    "* For ranking or windowing, we need to use partitionBy and then orderBy. partitionBy is to group the data and orderBy is to sort the data to assign rank.\n",
    "* partitionBy or orderBy returns WindowSpec object\n",
    "* WindowSpec object needs to be passed to over with ranking and aggregate functions.\n",
    "\n",
    "### Performing aggregations\n",
    "\n",
    "Let us see how to perform aggregations within each group.\n",
    "\n",
    "* We have functions such as sum, avg, min, max etc which can be used to aggregate the data.\n",
    "* We need to create WindowSpec object using partitionBy to get aggregations within each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesPath = /public/hr_db/employees\n",
       "employeesRaw = [value: string]\n",
       "employees = [employee_id: int, first_name: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 9 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-aggregations-01-read-data.scala\n",
    "\n",
    "val employeesPath = \"/public/hr_db/employees\"\n",
    "\n",
    "val employeesRaw = spark.\n",
    "  read.\n",
    "  text(employeesPath).\n",
    "  as[String]\n",
    "\n",
    "val employees = employeesRaw.map(rec => {\n",
    "  val r = rec.split(\"\\t\")\n",
    "  (r(0).toInt, r(1), r(2), r(3),\n",
    "   r(4), r(5), r(6), r(7).toFloat,\n",
    "   r(8), r(9), r(10)\n",
    "  )\n",
    "}).toDF(\"employee_id\", \"first_name\", \"last_name\", \"email\",\n",
    "        \"phone_number\", \"hire_date\", \"job_id\", \"salary\",\n",
    "        \"commission_pct\", \"manager_id\", \"department_id\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spec = org.apache.spark.sql.expressions.WindowSpec@2b63be8e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@2b63be8e"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-aggregations-02-define-spec.scala\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val spec = Window.partitionBy(\"department_id\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "// SELECT employee_id, salary, department_id,\n",
    "//   sum(salary) OVER (PARTITION BY department_id) department_salary_expense\n",
    "// FROM employees\n",
    "// ORDER BY department_id, salary DESC;\n",
    "\n",
    " \n",
    "val employeesSalary = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"department_salary_expense\", sum($\"salary\").over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// We can directly invoke over with WindowSpec object\n",
    "// without creating variable\n",
    "val employeesSalary = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"department_salary_expense\", sum($\"salary\").over(Window.partitionBy(\"department_id\"))).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Some realistic use cases\n",
    "    * Get the average salary for each department and get all employee details who earn more than the average salary\n",
    "    * Get average revenue for each day and get all the orders who earn revenue more than average revenue\n",
    "    * Get the highest order revenue and get all the orders which have revenue more than 75% of the revenue\n",
    "\n",
    "### Using Windowing Functions\n",
    "\n",
    "Let us see details about windowing functions within each group\n",
    "\n",
    "* We have functions such as lead, lag, first, last etc\n",
    "* We need to create WindowSpec object using partitionBy and then orderBy for most of the windowing functions\n",
    "* lead and lag take any column using which you want to get information based on partition and order columns.\n",
    "* Some realistic use cases\n",
    "    * The salary difference between current and next/previous employee within each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesPath = /public/hr_db/employees\n",
       "employeesRaw = [value: string]\n",
       "employees = [employee_id: int, first_name: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 9 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-windowing-01-read-data.scala\n",
    "\n",
    "// val employeesPath = \"/Users/itversity/Research/data/hr_db/employees/part-00000\"\n",
    "val employeesPath = \"/public/hr_db/employees\"\n",
    "\n",
    "val employeesRaw = spark.\n",
    "  read.\n",
    "  text(employeesPath).\n",
    "  as[String]\n",
    "\n",
    "val employees = employeesRaw.map(rec => {\n",
    "  val r = rec.split(\"\\t\")\n",
    "  (r(0).toInt, r(1), r(2), r(3),\n",
    "   r(4), r(5), r(6), r(7).toFloat,\n",
    "   r(8), r(9), r(10)\n",
    "  )\n",
    "}).toDF(\"employee_id\", \"first_name\", \"last_name\", \"email\",\n",
    "        \"phone_number\", \"hire_date\", \"job_id\", \"salary\",\n",
    "        \"commission_pct\", \"manager_id\", \"department_id\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spec = org.apache.spark.sql.expressions.WindowSpec@2ff4441e\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@2ff4441e"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-windowing-02-define-spec.scala\n",
    "\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val spec = Window.\n",
    "  partitionBy(\"department_id\").\n",
    "  orderBy($\"salary\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n",
      "|employee_id| salary|department_id|lead_salary|\n",
      "+-----------+-------+-------------+-----------+\n",
      "|        200| 4400.0|           10|       null|\n",
      "|        108|12000.0|          100|     9000.0|\n",
      "|        109| 9000.0|          100|     8200.0|\n",
      "|        110| 8200.0|          100|     7800.0|\n",
      "|        112| 7800.0|          100|     7700.0|\n",
      "|        111| 7700.0|          100|     6900.0|\n",
      "|        113| 6900.0|          100|       null|\n",
      "|        205|12000.0|          110|     8300.0|\n",
      "|        206| 8300.0|          110|       null|\n",
      "|        201|13000.0|           20|     6000.0|\n",
      "|        202| 6000.0|           20|       null|\n",
      "|        114|11000.0|           30|     3100.0|\n",
      "|        115| 3100.0|           30|     2900.0|\n",
      "|        116| 2900.0|           30|     2800.0|\n",
      "|        117| 2800.0|           30|     2600.0|\n",
      "|        118| 2600.0|           30|     2500.0|\n",
      "|        119| 2500.0|           30|       null|\n",
      "|        203| 6500.0|           40|       null|\n",
      "|        121| 8200.0|           50|     8000.0|\n",
      "|        120| 8000.0|           50|     7900.0|\n",
      "|        122| 7900.0|           50|     6500.0|\n",
      "|        123| 6500.0|           50|     5800.0|\n",
      "|        124| 5800.0|           50|     4200.0|\n",
      "|        184| 4200.0|           50|     4100.0|\n",
      "|        185| 4100.0|           50|     4000.0|\n",
      "|        192| 4000.0|           50|     3900.0|\n",
      "|        193| 3900.0|           50|     3800.0|\n",
      "|        188| 3800.0|           50|     3600.0|\n",
      "|        189| 3600.0|           50|     3500.0|\n",
      "|        137| 3600.0|           50|     3600.0|\n",
      "|        141| 3500.0|           50|     3400.0|\n",
      "|        186| 3400.0|           50|     3300.0|\n",
      "|        133| 3300.0|           50|     3200.0|\n",
      "|        129| 3300.0|           50|     3300.0|\n",
      "|        194| 3200.0|           50|     3100.0|\n",
      "|        180| 3200.0|           50|     3200.0|\n",
      "|        125| 3200.0|           50|     3200.0|\n",
      "|        138| 3200.0|           50|     3200.0|\n",
      "|        181| 3100.0|           50|     3100.0|\n",
      "|        196| 3100.0|           50|     3000.0|\n",
      "|        142| 3100.0|           50|     3100.0|\n",
      "|        187| 3000.0|           50|     3000.0|\n",
      "|        197| 3000.0|           50|     2900.0|\n",
      "|        190| 2900.0|           50|     2800.0|\n",
      "|        134| 2900.0|           50|     2900.0|\n",
      "|        130| 2800.0|           50|     2800.0|\n",
      "|        183| 2800.0|           50|     2800.0|\n",
      "|        195| 2800.0|           50|     2700.0|\n",
      "|        139| 2700.0|           50|     2700.0|\n",
      "|        126| 2700.0|           50|     2600.0|\n",
      "|        199| 2600.0|           50|     2500.0|\n",
      "|        198| 2600.0|           50|     2600.0|\n",
      "|        143| 2600.0|           50|     2600.0|\n",
      "|        191| 2500.0|           50|     2400.0|\n",
      "|        140| 2500.0|           50|     2500.0|\n",
      "|        144| 2500.0|           50|     2500.0|\n",
      "|        182| 2500.0|           50|     2500.0|\n",
      "|        131| 2500.0|           50|     2500.0|\n",
      "|        127| 2400.0|           50|     2400.0|\n",
      "|        135| 2400.0|           50|     2200.0|\n",
      "|        128| 2200.0|           50|     2200.0|\n",
      "|        136| 2200.0|           50|     2100.0|\n",
      "|        132| 2100.0|           50|       null|\n",
      "|        103| 9000.0|           60|     6000.0|\n",
      "|        104| 6000.0|           60|     4800.0|\n",
      "|        106| 4800.0|           60|     4200.0|\n",
      "|        105| 4800.0|           60|     4800.0|\n",
      "|        107| 4200.0|           60|       null|\n",
      "|        204|10000.0|           70|       null|\n",
      "|        145|14000.0|           80|    13500.0|\n",
      "|        146|13500.0|           80|    12000.0|\n",
      "|        147|12000.0|           80|    11500.0|\n",
      "|        168|11500.0|           80|    11000.0|\n",
      "|        148|11000.0|           80|    11000.0|\n",
      "|        174|11000.0|           80|    10500.0|\n",
      "|        162|10500.0|           80|    10000.0|\n",
      "|        149|10500.0|           80|    10500.0|\n",
      "|        150|10000.0|           80|    10000.0|\n",
      "|        156|10000.0|           80|    10000.0|\n",
      "|        169|10000.0|           80|     9600.0|\n",
      "|        170| 9600.0|           80|     9500.0|\n",
      "|        151| 9500.0|           80|     9500.0|\n",
      "|        157| 9500.0|           80|     9500.0|\n",
      "|        163| 9500.0|           80|     9000.0|\n",
      "|        158| 9000.0|           80|     8800.0|\n",
      "|        152| 9000.0|           80|     9000.0|\n",
      "|        175| 8800.0|           80|     8600.0|\n",
      "|        176| 8600.0|           80|     8400.0|\n",
      "|        177| 8400.0|           80|     8000.0|\n",
      "|        153| 8000.0|           80|     8000.0|\n",
      "|        159| 8000.0|           80|     7500.0|\n",
      "|        160| 7500.0|           80|     7400.0|\n",
      "|        154| 7500.0|           80|     7500.0|\n",
      "|        171| 7400.0|           80|     7300.0|\n",
      "|        172| 7300.0|           80|     7200.0|\n",
      "|        164| 7200.0|           80|     7000.0|\n",
      "|        155| 7000.0|           80|     7000.0|\n",
      "|        161| 7000.0|           80|     6800.0|\n",
      "|        165| 6800.0|           80|     6400.0|\n",
      "|        166| 6400.0|           80|     6200.0|\n",
      "|        167| 6200.0|           80|     6200.0|\n",
      "|        179| 6200.0|           80|     6100.0|\n",
      "|        173| 6100.0|           80|       null|\n",
      "|        100|24000.0|           90|    17000.0|\n",
      "|        102|17000.0|           90|       null|\n",
      "|        101|17000.0|           90|    17000.0|\n",
      "|        178| 7000.0|         null|       null|\n",
      "+-----------+-------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesLead = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-windowing-03-lead.scala\n",
    "\n",
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  lead(salary, 1) OVER (PARTITION BY department_id ORDER BY salary DESC) lead_salary\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesLead = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"lead_salary\", lead($\"salary\", 1).over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesLead.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+----------+\n",
      "|employee_id| salary|department_id|lag_salary|\n",
      "+-----------+-------+-------------+----------+\n",
      "|        200| 4400.0|           10|      null|\n",
      "|        108|12000.0|          100|      null|\n",
      "|        109| 9000.0|          100|   12000.0|\n",
      "|        110| 8200.0|          100|    9000.0|\n",
      "|        112| 7800.0|          100|    8200.0|\n",
      "|        111| 7700.0|          100|    7800.0|\n",
      "|        113| 6900.0|          100|    7700.0|\n",
      "|        205|12000.0|          110|      null|\n",
      "|        206| 8300.0|          110|   12000.0|\n",
      "|        201|13000.0|           20|      null|\n",
      "|        202| 6000.0|           20|   13000.0|\n",
      "|        114|11000.0|           30|      null|\n",
      "|        115| 3100.0|           30|   11000.0|\n",
      "|        116| 2900.0|           30|    3100.0|\n",
      "|        117| 2800.0|           30|    2900.0|\n",
      "|        118| 2600.0|           30|    2800.0|\n",
      "|        119| 2500.0|           30|    2600.0|\n",
      "|        203| 6500.0|           40|      null|\n",
      "|        121| 8200.0|           50|      null|\n",
      "|        120| 8000.0|           50|    8200.0|\n",
      "|        122| 7900.0|           50|    8000.0|\n",
      "|        123| 6500.0|           50|    7900.0|\n",
      "|        124| 5800.0|           50|    6500.0|\n",
      "|        184| 4200.0|           50|    5800.0|\n",
      "|        185| 4100.0|           50|    4200.0|\n",
      "|        192| 4000.0|           50|    4100.0|\n",
      "|        193| 3900.0|           50|    4000.0|\n",
      "|        188| 3800.0|           50|    3900.0|\n",
      "|        189| 3600.0|           50|    3600.0|\n",
      "|        137| 3600.0|           50|    3800.0|\n",
      "|        141| 3500.0|           50|    3600.0|\n",
      "|        186| 3400.0|           50|    3500.0|\n",
      "|        133| 3300.0|           50|    3300.0|\n",
      "|        129| 3300.0|           50|    3400.0|\n",
      "|        194| 3200.0|           50|    3200.0|\n",
      "|        180| 3200.0|           50|    3200.0|\n",
      "|        125| 3200.0|           50|    3200.0|\n",
      "|        138| 3200.0|           50|    3300.0|\n",
      "|        181| 3100.0|           50|    3100.0|\n",
      "|        196| 3100.0|           50|    3100.0|\n",
      "|        142| 3100.0|           50|    3200.0|\n",
      "|        187| 3000.0|           50|    3100.0|\n",
      "|        197| 3000.0|           50|    3000.0|\n",
      "|        190| 2900.0|           50|    2900.0|\n",
      "|        134| 2900.0|           50|    3000.0|\n",
      "|        130| 2800.0|           50|    2900.0|\n",
      "|        183| 2800.0|           50|    2800.0|\n",
      "|        195| 2800.0|           50|    2800.0|\n",
      "|        139| 2700.0|           50|    2800.0|\n",
      "|        126| 2700.0|           50|    2700.0|\n",
      "|        199| 2600.0|           50|    2600.0|\n",
      "|        198| 2600.0|           50|    2600.0|\n",
      "|        143| 2600.0|           50|    2700.0|\n",
      "|        191| 2500.0|           50|    2500.0|\n",
      "|        140| 2500.0|           50|    2500.0|\n",
      "|        144| 2500.0|           50|    2500.0|\n",
      "|        182| 2500.0|           50|    2500.0|\n",
      "|        131| 2500.0|           50|    2600.0|\n",
      "|        127| 2400.0|           50|    2500.0|\n",
      "|        135| 2400.0|           50|    2400.0|\n",
      "|        128| 2200.0|           50|    2400.0|\n",
      "|        136| 2200.0|           50|    2200.0|\n",
      "|        132| 2100.0|           50|    2200.0|\n",
      "|        103| 9000.0|           60|      null|\n",
      "|        104| 6000.0|           60|    9000.0|\n",
      "|        106| 4800.0|           60|    4800.0|\n",
      "|        105| 4800.0|           60|    6000.0|\n",
      "|        107| 4200.0|           60|    4800.0|\n",
      "|        204|10000.0|           70|      null|\n",
      "|        145|14000.0|           80|      null|\n",
      "|        146|13500.0|           80|   14000.0|\n",
      "|        147|12000.0|           80|   13500.0|\n",
      "|        168|11500.0|           80|   12000.0|\n",
      "|        148|11000.0|           80|   11500.0|\n",
      "|        174|11000.0|           80|   11000.0|\n",
      "|        162|10500.0|           80|   10500.0|\n",
      "|        149|10500.0|           80|   11000.0|\n",
      "|        150|10000.0|           80|   10500.0|\n",
      "|        156|10000.0|           80|   10000.0|\n",
      "|        169|10000.0|           80|   10000.0|\n",
      "|        170| 9600.0|           80|   10000.0|\n",
      "|        151| 9500.0|           80|    9600.0|\n",
      "|        157| 9500.0|           80|    9500.0|\n",
      "|        163| 9500.0|           80|    9500.0|\n",
      "|        158| 9000.0|           80|    9000.0|\n",
      "|        152| 9000.0|           80|    9500.0|\n",
      "|        175| 8800.0|           80|    9000.0|\n",
      "|        176| 8600.0|           80|    8800.0|\n",
      "|        177| 8400.0|           80|    8600.0|\n",
      "|        153| 8000.0|           80|    8400.0|\n",
      "|        159| 8000.0|           80|    8000.0|\n",
      "|        160| 7500.0|           80|    7500.0|\n",
      "|        154| 7500.0|           80|    8000.0|\n",
      "|        171| 7400.0|           80|    7500.0|\n",
      "|        172| 7300.0|           80|    7400.0|\n",
      "|        164| 7200.0|           80|    7300.0|\n",
      "|        155| 7000.0|           80|    7200.0|\n",
      "|        161| 7000.0|           80|    7000.0|\n",
      "|        165| 6800.0|           80|    7000.0|\n",
      "|        166| 6400.0|           80|    6800.0|\n",
      "|        167| 6200.0|           80|    6400.0|\n",
      "|        179| 6200.0|           80|    6200.0|\n",
      "|        173| 6100.0|           80|    6200.0|\n",
      "|        100|24000.0|           90|      null|\n",
      "|        102|17000.0|           90|   17000.0|\n",
      "|        101|17000.0|           90|   24000.0|\n",
      "|        178| 7000.0|         null|      null|\n",
      "+-----------+-------+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesLag = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-dataframes-windowing-04-lag.scala\n",
    "\n",
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  lag(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) lag_salary\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesLag = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"lag_salary\", lag(\"salary\", 1).over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesLag.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+------------+\n",
      "|employee_id| salary|department_id|first_salary|\n",
      "+-----------+-------+-------------+------------+\n",
      "|        200| 4400.0|           10|      4400.0|\n",
      "|        108|12000.0|          100|     12000.0|\n",
      "|        109| 9000.0|          100|     12000.0|\n",
      "|        110| 8200.0|          100|     12000.0|\n",
      "|        112| 7800.0|          100|     12000.0|\n",
      "|        111| 7700.0|          100|     12000.0|\n",
      "|        113| 6900.0|          100|     12000.0|\n",
      "|        205|12000.0|          110|     12000.0|\n",
      "|        206| 8300.0|          110|     12000.0|\n",
      "|        201|13000.0|           20|     13000.0|\n",
      "|        202| 6000.0|           20|     13000.0|\n",
      "|        114|11000.0|           30|     11000.0|\n",
      "|        115| 3100.0|           30|     11000.0|\n",
      "|        116| 2900.0|           30|     11000.0|\n",
      "|        117| 2800.0|           30|     11000.0|\n",
      "|        118| 2600.0|           30|     11000.0|\n",
      "|        119| 2500.0|           30|     11000.0|\n",
      "|        203| 6500.0|           40|      6500.0|\n",
      "|        121| 8200.0|           50|      8200.0|\n",
      "|        120| 8000.0|           50|      8200.0|\n",
      "|        122| 7900.0|           50|      8200.0|\n",
      "|        123| 6500.0|           50|      8200.0|\n",
      "|        124| 5800.0|           50|      8200.0|\n",
      "|        184| 4200.0|           50|      8200.0|\n",
      "|        185| 4100.0|           50|      8200.0|\n",
      "|        192| 4000.0|           50|      8200.0|\n",
      "|        193| 3900.0|           50|      8200.0|\n",
      "|        188| 3800.0|           50|      8200.0|\n",
      "|        189| 3600.0|           50|      8200.0|\n",
      "|        137| 3600.0|           50|      8200.0|\n",
      "|        141| 3500.0|           50|      8200.0|\n",
      "|        186| 3400.0|           50|      8200.0|\n",
      "|        133| 3300.0|           50|      8200.0|\n",
      "|        129| 3300.0|           50|      8200.0|\n",
      "|        194| 3200.0|           50|      8200.0|\n",
      "|        180| 3200.0|           50|      8200.0|\n",
      "|        125| 3200.0|           50|      8200.0|\n",
      "|        138| 3200.0|           50|      8200.0|\n",
      "|        181| 3100.0|           50|      8200.0|\n",
      "|        196| 3100.0|           50|      8200.0|\n",
      "|        142| 3100.0|           50|      8200.0|\n",
      "|        187| 3000.0|           50|      8200.0|\n",
      "|        197| 3000.0|           50|      8200.0|\n",
      "|        190| 2900.0|           50|      8200.0|\n",
      "|        134| 2900.0|           50|      8200.0|\n",
      "|        130| 2800.0|           50|      8200.0|\n",
      "|        183| 2800.0|           50|      8200.0|\n",
      "|        195| 2800.0|           50|      8200.0|\n",
      "|        139| 2700.0|           50|      8200.0|\n",
      "|        126| 2700.0|           50|      8200.0|\n",
      "|        199| 2600.0|           50|      8200.0|\n",
      "|        198| 2600.0|           50|      8200.0|\n",
      "|        143| 2600.0|           50|      8200.0|\n",
      "|        191| 2500.0|           50|      8200.0|\n",
      "|        140| 2500.0|           50|      8200.0|\n",
      "|        144| 2500.0|           50|      8200.0|\n",
      "|        182| 2500.0|           50|      8200.0|\n",
      "|        131| 2500.0|           50|      8200.0|\n",
      "|        127| 2400.0|           50|      8200.0|\n",
      "|        135| 2400.0|           50|      8200.0|\n",
      "|        128| 2200.0|           50|      8200.0|\n",
      "|        136| 2200.0|           50|      8200.0|\n",
      "|        132| 2100.0|           50|      8200.0|\n",
      "|        103| 9000.0|           60|      9000.0|\n",
      "|        104| 6000.0|           60|      9000.0|\n",
      "|        106| 4800.0|           60|      9000.0|\n",
      "|        105| 4800.0|           60|      9000.0|\n",
      "|        107| 4200.0|           60|      9000.0|\n",
      "|        204|10000.0|           70|     10000.0|\n",
      "|        145|14000.0|           80|     14000.0|\n",
      "|        146|13500.0|           80|     14000.0|\n",
      "|        147|12000.0|           80|     14000.0|\n",
      "|        168|11500.0|           80|     14000.0|\n",
      "|        148|11000.0|           80|     14000.0|\n",
      "|        174|11000.0|           80|     14000.0|\n",
      "|        162|10500.0|           80|     14000.0|\n",
      "|        149|10500.0|           80|     14000.0|\n",
      "|        150|10000.0|           80|     14000.0|\n",
      "|        156|10000.0|           80|     14000.0|\n",
      "|        169|10000.0|           80|     14000.0|\n",
      "|        170| 9600.0|           80|     14000.0|\n",
      "|        151| 9500.0|           80|     14000.0|\n",
      "|        157| 9500.0|           80|     14000.0|\n",
      "|        163| 9500.0|           80|     14000.0|\n",
      "|        158| 9000.0|           80|     14000.0|\n",
      "|        152| 9000.0|           80|     14000.0|\n",
      "|        175| 8800.0|           80|     14000.0|\n",
      "|        176| 8600.0|           80|     14000.0|\n",
      "|        177| 8400.0|           80|     14000.0|\n",
      "|        153| 8000.0|           80|     14000.0|\n",
      "|        159| 8000.0|           80|     14000.0|\n",
      "|        160| 7500.0|           80|     14000.0|\n",
      "|        154| 7500.0|           80|     14000.0|\n",
      "|        171| 7400.0|           80|     14000.0|\n",
      "|        172| 7300.0|           80|     14000.0|\n",
      "|        164| 7200.0|           80|     14000.0|\n",
      "|        155| 7000.0|           80|     14000.0|\n",
      "|        161| 7000.0|           80|     14000.0|\n",
      "|        165| 6800.0|           80|     14000.0|\n",
      "|        166| 6400.0|           80|     14000.0|\n",
      "|        167| 6200.0|           80|     14000.0|\n",
      "|        179| 6200.0|           80|     14000.0|\n",
      "|        173| 6100.0|           80|     14000.0|\n",
      "|        100|24000.0|           90|     24000.0|\n",
      "|        102|17000.0|           90|     24000.0|\n",
      "|        101|17000.0|           90|     24000.0|\n",
      "|        178| 7000.0|         null|      7000.0|\n",
      "+-----------+-------+-------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesFirst = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  first_value(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) first_salary\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesFirst = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"first_salary\", first(\"salary\").over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesFirst.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+-----------+\n",
      "|employee_id| salary|department_id|last_salary|\n",
      "+-----------+-------+-------------+-----------+\n",
      "|        200| 4400.0|           10|     4400.0|\n",
      "|        108|12000.0|          100|     6900.0|\n",
      "|        109| 9000.0|          100|     6900.0|\n",
      "|        110| 8200.0|          100|     6900.0|\n",
      "|        112| 7800.0|          100|     6900.0|\n",
      "|        111| 7700.0|          100|     6900.0|\n",
      "|        113| 6900.0|          100|     6900.0|\n",
      "|        205|12000.0|          110|     8300.0|\n",
      "|        206| 8300.0|          110|     8300.0|\n",
      "|        201|13000.0|           20|     6000.0|\n",
      "|        202| 6000.0|           20|     6000.0|\n",
      "|        114|11000.0|           30|     2500.0|\n",
      "|        115| 3100.0|           30|     2500.0|\n",
      "|        116| 2900.0|           30|     2500.0|\n",
      "|        117| 2800.0|           30|     2500.0|\n",
      "|        118| 2600.0|           30|     2500.0|\n",
      "|        119| 2500.0|           30|     2500.0|\n",
      "|        203| 6500.0|           40|     6500.0|\n",
      "|        121| 8200.0|           50|     2100.0|\n",
      "|        120| 8000.0|           50|     2100.0|\n",
      "|        122| 7900.0|           50|     2100.0|\n",
      "|        123| 6500.0|           50|     2100.0|\n",
      "|        124| 5800.0|           50|     2100.0|\n",
      "|        184| 4200.0|           50|     2100.0|\n",
      "|        185| 4100.0|           50|     2100.0|\n",
      "|        192| 4000.0|           50|     2100.0|\n",
      "|        193| 3900.0|           50|     2100.0|\n",
      "|        188| 3800.0|           50|     2100.0|\n",
      "|        189| 3600.0|           50|     2100.0|\n",
      "|        137| 3600.0|           50|     2100.0|\n",
      "|        141| 3500.0|           50|     2100.0|\n",
      "|        186| 3400.0|           50|     2100.0|\n",
      "|        133| 3300.0|           50|     2100.0|\n",
      "|        129| 3300.0|           50|     2100.0|\n",
      "|        194| 3200.0|           50|     2100.0|\n",
      "|        180| 3200.0|           50|     2100.0|\n",
      "|        125| 3200.0|           50|     2100.0|\n",
      "|        138| 3200.0|           50|     2100.0|\n",
      "|        181| 3100.0|           50|     2100.0|\n",
      "|        196| 3100.0|           50|     2100.0|\n",
      "|        142| 3100.0|           50|     2100.0|\n",
      "|        187| 3000.0|           50|     2100.0|\n",
      "|        197| 3000.0|           50|     2100.0|\n",
      "|        190| 2900.0|           50|     2100.0|\n",
      "|        134| 2900.0|           50|     2100.0|\n",
      "|        130| 2800.0|           50|     2100.0|\n",
      "|        183| 2800.0|           50|     2100.0|\n",
      "|        195| 2800.0|           50|     2100.0|\n",
      "|        139| 2700.0|           50|     2100.0|\n",
      "|        126| 2700.0|           50|     2100.0|\n",
      "|        199| 2600.0|           50|     2100.0|\n",
      "|        198| 2600.0|           50|     2100.0|\n",
      "|        143| 2600.0|           50|     2100.0|\n",
      "|        191| 2500.0|           50|     2100.0|\n",
      "|        140| 2500.0|           50|     2100.0|\n",
      "|        144| 2500.0|           50|     2100.0|\n",
      "|        182| 2500.0|           50|     2100.0|\n",
      "|        131| 2500.0|           50|     2100.0|\n",
      "|        127| 2400.0|           50|     2100.0|\n",
      "|        135| 2400.0|           50|     2100.0|\n",
      "|        128| 2200.0|           50|     2100.0|\n",
      "|        136| 2200.0|           50|     2100.0|\n",
      "|        132| 2100.0|           50|     2100.0|\n",
      "|        103| 9000.0|           60|     4200.0|\n",
      "|        104| 6000.0|           60|     4200.0|\n",
      "|        106| 4800.0|           60|     4200.0|\n",
      "|        105| 4800.0|           60|     4200.0|\n",
      "|        107| 4200.0|           60|     4200.0|\n",
      "|        204|10000.0|           70|    10000.0|\n",
      "|        145|14000.0|           80|     6100.0|\n",
      "|        146|13500.0|           80|     6100.0|\n",
      "|        147|12000.0|           80|     6100.0|\n",
      "|        168|11500.0|           80|     6100.0|\n",
      "|        148|11000.0|           80|     6100.0|\n",
      "|        174|11000.0|           80|     6100.0|\n",
      "|        162|10500.0|           80|     6100.0|\n",
      "|        149|10500.0|           80|     6100.0|\n",
      "|        150|10000.0|           80|     6100.0|\n",
      "|        156|10000.0|           80|     6100.0|\n",
      "|        169|10000.0|           80|     6100.0|\n",
      "|        170| 9600.0|           80|     6100.0|\n",
      "|        151| 9500.0|           80|     6100.0|\n",
      "|        157| 9500.0|           80|     6100.0|\n",
      "|        163| 9500.0|           80|     6100.0|\n",
      "|        158| 9000.0|           80|     6100.0|\n",
      "|        152| 9000.0|           80|     6100.0|\n",
      "|        175| 8800.0|           80|     6100.0|\n",
      "|        176| 8600.0|           80|     6100.0|\n",
      "|        177| 8400.0|           80|     6100.0|\n",
      "|        153| 8000.0|           80|     6100.0|\n",
      "|        159| 8000.0|           80|     6100.0|\n",
      "|        160| 7500.0|           80|     6100.0|\n",
      "|        154| 7500.0|           80|     6100.0|\n",
      "|        171| 7400.0|           80|     6100.0|\n",
      "|        172| 7300.0|           80|     6100.0|\n",
      "|        164| 7200.0|           80|     6100.0|\n",
      "|        155| 7000.0|           80|     6100.0|\n",
      "|        161| 7000.0|           80|     6100.0|\n",
      "|        165| 6800.0|           80|     6100.0|\n",
      "|        166| 6400.0|           80|     6100.0|\n",
      "|        167| 6200.0|           80|     6100.0|\n",
      "|        179| 6200.0|           80|     6100.0|\n",
      "|        173| 6100.0|           80|     6100.0|\n",
      "|        100|24000.0|           90|    17000.0|\n",
      "|        102|17000.0|           90|    17000.0|\n",
      "|        101|17000.0|           90|    17000.0|\n",
      "|        178| 7000.0|         null|     7000.0|\n",
      "+-----------+-------+-------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spec = org.apache.spark.sql.expressions.WindowSpec@6b30d51e\n",
       "employeesLast = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  last_value(salary) OVER \n",
    "    (PARTITION BY department_id ORDER BY salary DESC\n",
    "     ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) last_salary\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val spec = Window.\n",
    "  partitionBy(\"department_id\").\n",
    "  orderBy($\"salary\".desc).\n",
    "  rangeBetween(unboundedPreceding, unboundedFollowing)\n",
    "\n",
    "val employeesLast = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"last_salary\", last(\"salary\", false).over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesLast.show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Functions\n",
    "\n",
    "Let us talk about ranking functions within each group.\n",
    "\n",
    "* We have functions like rank, dense_rank, row_number etc\n",
    "* We need to create WindowSpec object using partitionBy and then orderBy for most of the ranking functions\n",
    "* Some realistic use cases\n",
    "    * Assign rank to employees based on salary within each department\n",
    "    * Assign ranks to products based on revenue each day or month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "employeesPath = /public/hr_db/employees\n",
       "employeesRaw = [value: string]\n",
       "employees = [employee_id: int, first_name: string ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, first_name: string ... 9 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val employeesPath = \"/public/hr_db/employees\"\n",
    "\n",
    "val employeesRaw = spark.\n",
    "  read.\n",
    "  text(employeesPath).\n",
    "  as[String]\n",
    "\n",
    "val employees = employeesRaw.map(rec => {\n",
    "  val r = rec.split(\"\\t\")\n",
    "  (r(0).toInt, r(1), r(2), r(3),\n",
    "   r(4), r(5), r(6), r(7).toFloat,\n",
    "   r(8), r(9), r(10)\n",
    "  )\n",
    "}).toDF(\"employee_id\", \"first_name\", \"last_name\", \"email\",\n",
    "        \"phone_number\", \"hire_date\", \"job_id\", \"salary\",\n",
    "        \"commission_pct\", \"manager_id\", \"department_id\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spec = org.apache.spark.sql.expressions.WindowSpec@1a337e8d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.WindowSpec@1a337e8d"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "val spec = Window.\n",
    "  partitionBy(\"department_id\").\n",
    "  orderBy($\"salary\".desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+----+\n",
      "|employee_id| salary|department_id|rank|\n",
      "+-----------+-------+-------------+----+\n",
      "|        200| 4400.0|           10|   1|\n",
      "|        108|12000.0|          100|   1|\n",
      "|        109| 9000.0|          100|   2|\n",
      "|        110| 8200.0|          100|   3|\n",
      "|        112| 7800.0|          100|   4|\n",
      "|        111| 7700.0|          100|   5|\n",
      "|        113| 6900.0|          100|   6|\n",
      "|        205|12000.0|          110|   1|\n",
      "|        206| 8300.0|          110|   2|\n",
      "|        201|13000.0|           20|   1|\n",
      "|        202| 6000.0|           20|   2|\n",
      "|        114|11000.0|           30|   1|\n",
      "|        115| 3100.0|           30|   2|\n",
      "|        116| 2900.0|           30|   3|\n",
      "|        117| 2800.0|           30|   4|\n",
      "|        118| 2600.0|           30|   5|\n",
      "|        119| 2500.0|           30|   6|\n",
      "|        203| 6500.0|           40|   1|\n",
      "|        121| 8200.0|           50|   1|\n",
      "|        120| 8000.0|           50|   2|\n",
      "|        122| 7900.0|           50|   3|\n",
      "|        123| 6500.0|           50|   4|\n",
      "|        124| 5800.0|           50|   5|\n",
      "|        184| 4200.0|           50|   6|\n",
      "|        185| 4100.0|           50|   7|\n",
      "|        192| 4000.0|           50|   8|\n",
      "|        193| 3900.0|           50|   9|\n",
      "|        188| 3800.0|           50|  10|\n",
      "|        189| 3600.0|           50|  11|\n",
      "|        137| 3600.0|           50|  11|\n",
      "|        141| 3500.0|           50|  13|\n",
      "|        186| 3400.0|           50|  14|\n",
      "|        133| 3300.0|           50|  15|\n",
      "|        129| 3300.0|           50|  15|\n",
      "|        180| 3200.0|           50|  17|\n",
      "|        194| 3200.0|           50|  17|\n",
      "|        125| 3200.0|           50|  17|\n",
      "|        138| 3200.0|           50|  17|\n",
      "|        181| 3100.0|           50|  21|\n",
      "|        142| 3100.0|           50|  21|\n",
      "|        196| 3100.0|           50|  21|\n",
      "|        187| 3000.0|           50|  24|\n",
      "|        197| 3000.0|           50|  24|\n",
      "|        190| 2900.0|           50|  26|\n",
      "|        134| 2900.0|           50|  26|\n",
      "|        130| 2800.0|           50|  28|\n",
      "|        183| 2800.0|           50|  28|\n",
      "|        195| 2800.0|           50|  28|\n",
      "|        139| 2700.0|           50|  31|\n",
      "|        126| 2700.0|           50|  31|\n",
      "|        199| 2600.0|           50|  33|\n",
      "|        198| 2600.0|           50|  33|\n",
      "|        143| 2600.0|           50|  33|\n",
      "|        182| 2500.0|           50|  36|\n",
      "|        140| 2500.0|           50|  36|\n",
      "|        144| 2500.0|           50|  36|\n",
      "|        191| 2500.0|           50|  36|\n",
      "|        131| 2500.0|           50|  36|\n",
      "|        127| 2400.0|           50|  41|\n",
      "|        135| 2400.0|           50|  41|\n",
      "|        136| 2200.0|           50|  43|\n",
      "|        128| 2200.0|           50|  43|\n",
      "|        132| 2100.0|           50|  45|\n",
      "|        103| 9000.0|           60|   1|\n",
      "|        104| 6000.0|           60|   2|\n",
      "|        106| 4800.0|           60|   3|\n",
      "|        105| 4800.0|           60|   3|\n",
      "|        107| 4200.0|           60|   5|\n",
      "|        204|10000.0|           70|   1|\n",
      "|        145|14000.0|           80|   1|\n",
      "|        146|13500.0|           80|   2|\n",
      "|        147|12000.0|           80|   3|\n",
      "|        168|11500.0|           80|   4|\n",
      "|        174|11000.0|           80|   5|\n",
      "|        148|11000.0|           80|   5|\n",
      "|        149|10500.0|           80|   7|\n",
      "|        162|10500.0|           80|   7|\n",
      "|        156|10000.0|           80|   9|\n",
      "|        150|10000.0|           80|   9|\n",
      "|        169|10000.0|           80|   9|\n",
      "|        170| 9600.0|           80|  12|\n",
      "|        151| 9500.0|           80|  13|\n",
      "|        163| 9500.0|           80|  13|\n",
      "|        157| 9500.0|           80|  13|\n",
      "|        152| 9000.0|           80|  16|\n",
      "|        158| 9000.0|           80|  16|\n",
      "|        175| 8800.0|           80|  18|\n",
      "|        176| 8600.0|           80|  19|\n",
      "|        177| 8400.0|           80|  20|\n",
      "|        159| 8000.0|           80|  21|\n",
      "|        153| 8000.0|           80|  21|\n",
      "|        154| 7500.0|           80|  23|\n",
      "|        160| 7500.0|           80|  23|\n",
      "|        171| 7400.0|           80|  25|\n",
      "|        172| 7300.0|           80|  26|\n",
      "|        164| 7200.0|           80|  27|\n",
      "|        161| 7000.0|           80|  28|\n",
      "|        155| 7000.0|           80|  28|\n",
      "|        165| 6800.0|           80|  30|\n",
      "|        166| 6400.0|           80|  31|\n",
      "|        167| 6200.0|           80|  32|\n",
      "|        179| 6200.0|           80|  32|\n",
      "|        173| 6100.0|           80|  34|\n",
      "|        100|24000.0|           90|   1|\n",
      "|        101|17000.0|           90|   2|\n",
      "|        102|17000.0|           90|   2|\n",
      "|        178| 7000.0|         null|   1|\n",
      "+-----------+-------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesRanked = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  rank() OVER (PARTITION BY department_id ORDER BY salary DESC) AS rank\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesRanked = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"rank\", rank.over(spec)).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesRanked.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+----+\n",
      "|employee_id| salary|department_id|rank|\n",
      "+-----------+-------+-------------+----+\n",
      "|        200| 4400.0|           10|   1|\n",
      "|        108|12000.0|          100|   1|\n",
      "|        109| 9000.0|          100|   2|\n",
      "|        110| 8200.0|          100|   3|\n",
      "|        112| 7800.0|          100|   4|\n",
      "|        111| 7700.0|          100|   5|\n",
      "|        113| 6900.0|          100|   6|\n",
      "|        205|12000.0|          110|   1|\n",
      "|        206| 8300.0|          110|   2|\n",
      "|        201|13000.0|           20|   1|\n",
      "|        202| 6000.0|           20|   2|\n",
      "|        114|11000.0|           30|   1|\n",
      "|        115| 3100.0|           30|   2|\n",
      "|        116| 2900.0|           30|   3|\n",
      "|        117| 2800.0|           30|   4|\n",
      "|        118| 2600.0|           30|   5|\n",
      "|        119| 2500.0|           30|   6|\n",
      "|        203| 6500.0|           40|   1|\n",
      "|        121| 8200.0|           50|   1|\n",
      "|        120| 8000.0|           50|   2|\n",
      "|        122| 7900.0|           50|   3|\n",
      "|        123| 6500.0|           50|   4|\n",
      "|        124| 5800.0|           50|   5|\n",
      "|        184| 4200.0|           50|   6|\n",
      "|        185| 4100.0|           50|   7|\n",
      "|        192| 4000.0|           50|   8|\n",
      "|        193| 3900.0|           50|   9|\n",
      "|        188| 3800.0|           50|  10|\n",
      "|        189| 3600.0|           50|  11|\n",
      "|        137| 3600.0|           50|  11|\n",
      "|        141| 3500.0|           50|  12|\n",
      "|        186| 3400.0|           50|  13|\n",
      "|        133| 3300.0|           50|  14|\n",
      "|        129| 3300.0|           50|  14|\n",
      "|        180| 3200.0|           50|  15|\n",
      "|        194| 3200.0|           50|  15|\n",
      "|        125| 3200.0|           50|  15|\n",
      "|        138| 3200.0|           50|  15|\n",
      "|        181| 3100.0|           50|  16|\n",
      "|        142| 3100.0|           50|  16|\n",
      "|        196| 3100.0|           50|  16|\n",
      "|        187| 3000.0|           50|  17|\n",
      "|        197| 3000.0|           50|  17|\n",
      "|        190| 2900.0|           50|  18|\n",
      "|        134| 2900.0|           50|  18|\n",
      "|        130| 2800.0|           50|  19|\n",
      "|        183| 2800.0|           50|  19|\n",
      "|        195| 2800.0|           50|  19|\n",
      "|        139| 2700.0|           50|  20|\n",
      "|        126| 2700.0|           50|  20|\n",
      "|        199| 2600.0|           50|  21|\n",
      "|        198| 2600.0|           50|  21|\n",
      "|        143| 2600.0|           50|  21|\n",
      "|        182| 2500.0|           50|  22|\n",
      "|        140| 2500.0|           50|  22|\n",
      "|        144| 2500.0|           50|  22|\n",
      "|        191| 2500.0|           50|  22|\n",
      "|        131| 2500.0|           50|  22|\n",
      "|        127| 2400.0|           50|  23|\n",
      "|        135| 2400.0|           50|  23|\n",
      "|        136| 2200.0|           50|  24|\n",
      "|        128| 2200.0|           50|  24|\n",
      "|        132| 2100.0|           50|  25|\n",
      "|        103| 9000.0|           60|   1|\n",
      "|        104| 6000.0|           60|   2|\n",
      "|        106| 4800.0|           60|   3|\n",
      "|        105| 4800.0|           60|   3|\n",
      "|        107| 4200.0|           60|   4|\n",
      "|        204|10000.0|           70|   1|\n",
      "|        145|14000.0|           80|   1|\n",
      "|        146|13500.0|           80|   2|\n",
      "|        147|12000.0|           80|   3|\n",
      "|        168|11500.0|           80|   4|\n",
      "|        174|11000.0|           80|   5|\n",
      "|        148|11000.0|           80|   5|\n",
      "|        149|10500.0|           80|   6|\n",
      "|        162|10500.0|           80|   6|\n",
      "|        156|10000.0|           80|   7|\n",
      "|        150|10000.0|           80|   7|\n",
      "|        169|10000.0|           80|   7|\n",
      "|        170| 9600.0|           80|   8|\n",
      "|        151| 9500.0|           80|   9|\n",
      "|        163| 9500.0|           80|   9|\n",
      "|        157| 9500.0|           80|   9|\n",
      "|        152| 9000.0|           80|  10|\n",
      "|        158| 9000.0|           80|  10|\n",
      "|        175| 8800.0|           80|  11|\n",
      "|        176| 8600.0|           80|  12|\n",
      "|        177| 8400.0|           80|  13|\n",
      "|        159| 8000.0|           80|  14|\n",
      "|        153| 8000.0|           80|  14|\n",
      "|        154| 7500.0|           80|  15|\n",
      "|        160| 7500.0|           80|  15|\n",
      "|        171| 7400.0|           80|  16|\n",
      "|        172| 7300.0|           80|  17|\n",
      "|        164| 7200.0|           80|  18|\n",
      "|        161| 7000.0|           80|  19|\n",
      "|        155| 7000.0|           80|  19|\n",
      "|        165| 6800.0|           80|  20|\n",
      "|        166| 6400.0|           80|  21|\n",
      "|        167| 6200.0|           80|  22|\n",
      "|        179| 6200.0|           80|  22|\n",
      "|        173| 6100.0|           80|  23|\n",
      "|        100|24000.0|           90|   1|\n",
      "|        101|17000.0|           90|   2|\n",
      "|        102|17000.0|           90|   2|\n",
      "|        178| 7000.0|         null|   1|\n",
      "+-----------+-------+-------------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesDenseRanked = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  dense_rank() OVER (PARTITION BY department_id ORDER BY salary DESC) AS rank\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesDenseRanked = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"rank\", dense_rank over spec ).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesDenseRanked.show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------------+---+\n",
      "|employee_id| salary|department_id| rn|\n",
      "+-----------+-------+-------------+---+\n",
      "|        200| 4400.0|           10|  1|\n",
      "|        108|12000.0|          100|  1|\n",
      "|        109| 9000.0|          100|  2|\n",
      "|        110| 8200.0|          100|  3|\n",
      "|        112| 7800.0|          100|  4|\n",
      "|        111| 7700.0|          100|  5|\n",
      "|        113| 6900.0|          100|  6|\n",
      "|        205|12000.0|          110|  1|\n",
      "|        206| 8300.0|          110|  2|\n",
      "|        201|13000.0|           20|  1|\n",
      "|        202| 6000.0|           20|  2|\n",
      "|        114|11000.0|           30|  1|\n",
      "|        115| 3100.0|           30|  2|\n",
      "|        116| 2900.0|           30|  3|\n",
      "|        117| 2800.0|           30|  4|\n",
      "|        118| 2600.0|           30|  5|\n",
      "|        119| 2500.0|           30|  6|\n",
      "|        203| 6500.0|           40|  1|\n",
      "|        121| 8200.0|           50|  1|\n",
      "|        120| 8000.0|           50|  2|\n",
      "|        122| 7900.0|           50|  3|\n",
      "|        123| 6500.0|           50|  4|\n",
      "|        124| 5800.0|           50|  5|\n",
      "|        184| 4200.0|           50|  6|\n",
      "|        185| 4100.0|           50|  7|\n",
      "|        192| 4000.0|           50|  8|\n",
      "|        193| 3900.0|           50|  9|\n",
      "|        188| 3800.0|           50| 10|\n",
      "|        189| 3600.0|           50| 12|\n",
      "|        137| 3600.0|           50| 11|\n",
      "|        141| 3500.0|           50| 13|\n",
      "|        186| 3400.0|           50| 14|\n",
      "|        133| 3300.0|           50| 16|\n",
      "|        129| 3300.0|           50| 15|\n",
      "|        194| 3200.0|           50| 20|\n",
      "|        180| 3200.0|           50| 19|\n",
      "|        125| 3200.0|           50| 18|\n",
      "|        138| 3200.0|           50| 17|\n",
      "|        181| 3100.0|           50| 22|\n",
      "|        196| 3100.0|           50| 23|\n",
      "|        142| 3100.0|           50| 21|\n",
      "|        187| 3000.0|           50| 24|\n",
      "|        197| 3000.0|           50| 25|\n",
      "|        190| 2900.0|           50| 27|\n",
      "|        134| 2900.0|           50| 26|\n",
      "|        130| 2800.0|           50| 28|\n",
      "|        183| 2800.0|           50| 29|\n",
      "|        195| 2800.0|           50| 30|\n",
      "|        139| 2700.0|           50| 31|\n",
      "|        126| 2700.0|           50| 32|\n",
      "|        199| 2600.0|           50| 35|\n",
      "|        198| 2600.0|           50| 34|\n",
      "|        143| 2600.0|           50| 33|\n",
      "|        191| 2500.0|           50| 40|\n",
      "|        140| 2500.0|           50| 37|\n",
      "|        144| 2500.0|           50| 38|\n",
      "|        182| 2500.0|           50| 39|\n",
      "|        131| 2500.0|           50| 36|\n",
      "|        127| 2400.0|           50| 41|\n",
      "|        135| 2400.0|           50| 42|\n",
      "|        128| 2200.0|           50| 43|\n",
      "|        136| 2200.0|           50| 44|\n",
      "|        132| 2100.0|           50| 45|\n",
      "|        103| 9000.0|           60|  1|\n",
      "|        104| 6000.0|           60|  2|\n",
      "|        106| 4800.0|           60|  4|\n",
      "|        105| 4800.0|           60|  3|\n",
      "|        107| 4200.0|           60|  5|\n",
      "|        204|10000.0|           70|  1|\n",
      "|        145|14000.0|           80|  1|\n",
      "|        146|13500.0|           80|  2|\n",
      "|        147|12000.0|           80|  3|\n",
      "|        168|11500.0|           80|  4|\n",
      "|        148|11000.0|           80|  5|\n",
      "|        174|11000.0|           80|  6|\n",
      "|        162|10500.0|           80|  8|\n",
      "|        149|10500.0|           80|  7|\n",
      "|        150|10000.0|           80|  9|\n",
      "|        156|10000.0|           80| 10|\n",
      "|        169|10000.0|           80| 11|\n",
      "|        170| 9600.0|           80| 12|\n",
      "|        151| 9500.0|           80| 13|\n",
      "|        157| 9500.0|           80| 14|\n",
      "|        163| 9500.0|           80| 15|\n",
      "|        158| 9000.0|           80| 17|\n",
      "|        152| 9000.0|           80| 16|\n",
      "|        175| 8800.0|           80| 18|\n",
      "|        176| 8600.0|           80| 19|\n",
      "|        177| 8400.0|           80| 20|\n",
      "|        153| 8000.0|           80| 21|\n",
      "|        159| 8000.0|           80| 22|\n",
      "|        160| 7500.0|           80| 24|\n",
      "|        154| 7500.0|           80| 23|\n",
      "|        171| 7400.0|           80| 25|\n",
      "|        172| 7300.0|           80| 26|\n",
      "|        164| 7200.0|           80| 27|\n",
      "|        155| 7000.0|           80| 28|\n",
      "|        161| 7000.0|           80| 29|\n",
      "|        165| 6800.0|           80| 30|\n",
      "|        166| 6400.0|           80| 31|\n",
      "|        167| 6200.0|           80| 32|\n",
      "|        179| 6200.0|           80| 33|\n",
      "|        173| 6100.0|           80| 34|\n",
      "|        100|24000.0|           90|  1|\n",
      "|        102|17000.0|           90|  3|\n",
      "|        101|17000.0|           90|  2|\n",
      "|        178| 7000.0|         null|  1|\n",
      "+-----------+-------+-------------+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "employeesRowNumbered = [employee_id: int, salary: float ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[employee_id: int, salary: float ... 2 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*\n",
    "SELECT employee_id, salary, department_id,\n",
    "  row_number() OVER (PARTITION BY department_id ORDER BY salary DESC) AS rn\n",
    "FROM employees\n",
    "ORDER BY department_id, salary DESC;\n",
    " */\n",
    "\n",
    "val employeesRowNumbered = employees.\n",
    "  select(\"employee_id\", \"salary\", \"department_id\").\n",
    "  withColumn(\"rn\", row_number over spec ).\n",
    "  orderBy($\"department_id\", $\"salary\".desc)\n",
    "\n",
    "employeesRowNumbered.show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development Life Cycle\n",
    "\n",
    "Let us talk about the development lifecycle.\n",
    "\n",
    "* Take the DailyProductRevenue code which gives us order_date, order_item_product_id, and revenue\n",
    "* Import Window and create a spec to partition by date and order by revenue in descending order.\n",
    "* Use withColumn and assign the rank\n",
    "* Filter data where rank is less than or equal to topN passed as an argument to the program\n",
    "* Drop rank field as we do not want to save the data and then sort in ascending order by date and descending order by revenue\n",
    "* Save the data frame into a file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-01-build.sbt\n",
    "\n",
    "name := \"SparkDemo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "libraryDependencies += \"org.apache.spark\" %% \"spark-sql\" % \"2.3.2\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-02-application.properties\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.input.base.dir = C:\\\\data\\\\retail_db\\\\\n",
    "dev.output.base.dir = C:\\\\data\\\\scalaspark\\\\\n",
    "\n",
    "devu.execution.mode = local\n",
    "devu.input.base.dir = /mnt/c/data/retail_db/\n",
    "devu.output.base.dir = /mnt/c/data/bootcamp/scalaspark/\n",
    "\n",
    "devm.execution.mode = local\n",
    "devm.input.base.dir = /Users/itversity/Research/data/retail_db/\n",
    "devm.output.base.dir = /Users/itversity/Research/data/scalaspark/\n",
    "\n",
    "prod.execution.mode = yarn-client\n",
    "prod.input.base.dir = /public/retail_db/\n",
    "prod.output.base.dir = /user/training/bootcamp/scalaspark/\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-dataframes-03-top-n-daily-products.scala\n",
    "\n",
    "package retail_db_df\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.{round, sum, dense_rank}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 15/12/18.\n",
    "  */\n",
    "object GetTopNDailyProductsDF {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "\n",
    "    val props = ConfigFactory.load\n",
    "    val env = args(0)\n",
    "    val envProps = props.getConfig(env)\n",
    "    val topN = args(1).toInt\n",
    "\n",
    "    val spark = SparkSession.\n",
    "      builder.\n",
    "      appName(\"Daily Product Revenue using Data Frame Operations\").\n",
    "      master(envProps.getString(\"execution.mode\")).\n",
    "      getOrCreate\n",
    "\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "    val inputBaseDir = envProps.getString(\"input.base.dir\")\n",
    "    val ordersCSV = spark.read.\n",
    "      csv(inputBaseDir + \"orders\").\n",
    "      toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "    val orderItemsCSV = spark.read.\n",
    "      csv(inputBaseDir + \"order_items\").\n",
    "      toDF(\"order_item_id\", \"order_item_order_id\", \"order_item_product_id\",\n",
    "        \"order_item_quantity\", \"order_item_subtotal\", \"order_item_product_price\")\n",
    "\n",
    "    import spark.implicits._\n",
    "\n",
    "    val orders = ordersCSV.\n",
    "      withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "      withColumn(\"order_customer_id\", $\"order_customer_id\".cast(\"int\"))\n",
    "\n",
    "    val orderItems = orderItemsCSV.\n",
    "      withColumn(\"order_item_id\", $\"order_item_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_order_id\", $\"order_item_order_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_product_id\", $\"order_item_product_id\".cast(\"int\")).\n",
    "      withColumn(\"order_item_quantity\", $\"order_item_quantity\".cast(\"int\")).\n",
    "      withColumn(\"order_item_subtotal\", $\"order_item_subtotal\".cast(\"float\")).\n",
    "      withColumn(\"order_item_product_price\", $\"order_item_product_price\".cast(\"float\"))\n",
    "\n",
    "    val dailyProductRevenue = orders.\n",
    "      where(\"order_status in ('COMPLETE', 'CLOSED')\").\n",
    "      join(orderItems, $\"order_id\" === $\"order_item_order_id\").\n",
    "      groupBy(\"order_date\", \"order_item_product_id\").\n",
    "      agg(round(sum($\"order_item_subtotal\"), 2).alias(\"revenue\"))\n",
    "\n",
    "    val spec = Window.\n",
    "      partitionBy(\"order_date\").\n",
    "      orderBy($\"revenue\".desc)\n",
    "\n",
    "    val dailyProductRevenueRanked = dailyProductRevenue.\n",
    "    withColumn(\"rnk\", dense_rank().over(spec))\n",
    "\n",
    "    val topNDailyProducts = dailyProductRevenueRanked.\n",
    "      where($\"rnk\" <= topN).\n",
    "      drop($\"rnk\").\n",
    "      orderBy($\"order_date\", $\"revenue\".desc)\n",
    "\n",
    "    val outputBaseDir = envProps.getString(\"output.base.dir\")\n",
    "    topNDailyProducts.write.csv(outputBaseDir + \"topn_daily_products\")\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-dataframes-04-top-n-daily-products.sh\n",
    "\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --class retail_db_df.GetTopNDailyProductsDF \\\n",
    "  --deploy-mode client \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  sparkdemo_2.11-0.1.jar prod 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
