{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different compression algorithms with file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this topic let us see different compression algorithms and their relationship with different file formats.\n",
    "\n",
    "* Different Compression Algorithms\n",
    "* Splittable and Non Splittable\n",
    "* Details about supported compression algorithms on cluster (look at core-site.xml)\n",
    "* Different file formats work with different compression algorithms.\n",
    "* Compressing text files\n",
    "    * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    "    * Writing\n",
    "        * Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "        * Use option on spark.write before csv – <mark>df.write.option(\"codec\", \"gzip\").csv(\"PATH\")</mark>\n",
    "    \n",
    "        * Also option with compression work fine\n",
    "        \n",
    "* Compressing json files\n",
    "    * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    "    * Writing\n",
    "        * Can compress to most of the algorithms (bzip2, deflate, uncompressed, lz4, gzip, snappy, none)\n",
    "        * Use option with compression – <mark>option(\"compression\", \"gzip\")</mark>\n",
    "* Compressing orc files\n",
    "    * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    "    * Writing\n",
    "        * Default – snappy\n",
    "        * Could not figure out how I can write in other file formats\n",
    "* Compressing parquet files\n",
    "    * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    "    * Writing\n",
    "        * Default – snappy\n",
    "        * Supported codecs – uncompressed, snappy, gzip, lzo\n",
    "        * Set spark.sql.parquet.compression.codec to the appropriate algorithm\n",
    "        \n",
    "* Compressing avrò files\n",
    "    * Reading – No special action need to be taken as long as we use supported algorithms.\n",
    "    * Writing\n",
    "    * Default – uncompress\n",
    "    * Supported codecs – uncompressed, snappy, deflate\n",
    "    * Set spark.sql.avro.compression.codec to the appropriate algorithm        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
