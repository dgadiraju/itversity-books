{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Development and Execution life cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you want to understand Spark execution life cycle and understand different terms such as executor, executor tasks, driver program and more?\n",
    "\n",
    "* Develop Spark Application – Get Monthly Product Revenue\n",
    "* Build and Deploy\n",
    "* Local mode vs. YARN mode\n",
    "* Quick walk through of Spark UI\n",
    "* YARN deployment modes\n",
    "* Spark Execution Cycle\n",
    "\n",
    "### Develop Spark Application – Get Monthly Product Revenue\n",
    "\n",
    "Let us start with details with respect to problem statement, design and then implementation.\n",
    "\n",
    "**Problem Statement**\n",
    "\n",
    "Using retail db dataset, we need to compute Monthly Product Revenue for given month.\n",
    "\n",
    "* We need to consider only completed and closed orders to compute revenue.\n",
    "* Also we need to consider only those transactions for a given month passed as argument.\n",
    "* We need to sort the data in descending order by revenue while saving the output to HDFS Path.\n",
    "\n",
    "**Design** \n",
    "\n",
    "Let us see the design for the given Problem Statement.\n",
    "\n",
    "* Filter for orders which fall in the month passed as the argument\n",
    "* Join filtered orders and order_items to get order_item details for a given month\n",
    "* Get revenue for each product_id\n",
    "* We need to read products from the local file system\n",
    "* Convert into RDD and extract product_id and product_name\n",
    "* Join it with aggregated order_items (product_id, revenue)\n",
    "* Get product_name and revenue for each product\n",
    "\n",
    "**Development**\n",
    "\n",
    "Let us create a new project and develop the logic.\n",
    "\n",
    "* Setup Project\n",
    "    * Scala Version: 2.11.8 (on windows, latest of 2.11 in other environments)\n",
    "    * sbt Version: 0.13.x\n",
    "    * JDK: 1.8.x\n",
    "    * Project Name: SparkDemo\n",
    "* Update build.sbt\n",
    "    * typesafe config\n",
    "    * Spark Core\n",
    "* Update application.properties\n",
    "* Develop Logic to compute revenue per product for given month.\n",
    "* Once the project is setup we can launch Scala REPL with Spark as well as typesafe config dependencies using **sbt console**\n",
    "* Once we get the logic, we can update as part of Program called **GetMonthlyProductRevenue**\n",
    "\n",
    "**USING SBT CONSOLE**\n",
    "\n",
    "As part of this topic, we will see how to access sbt console and use it for exploring Spark based APIs.\n",
    "\n",
    "* Go to the working directory of the project.\n",
    "* Run sbt console\n",
    "* We should be able to use typesafe config APIs as well as Spark APIs.\n",
    "* Create Spark Conf and Spark Context objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "val props = ConfigFactory.load()\n",
    "\n",
    "val envProps = props.getConfig(\"devu\")\n",
    "\n",
    "// As part of the video, we have passed dev.\n",
    "// But to make the code compatible with windows, \n",
    "// there are some changes to the code.\n",
    "// Make sure to pass devu in Ubuntu\n",
    "\n",
    "val inputPath = envProps.getString(\"input.base.dir\")\n",
    "\n",
    "val outputPath = envProps.getString(\"output.base.dir\") + \"monthly_product_revenue\"\n",
    "\n",
    "val month = \"2014-01\"\n",
    "\n",
    "val conf = new SparkConf().\n",
    "\n",
    "  setAppName(\"Revenue Per Product for \" + month).\n",
    "  \n",
    "  setMaster(envProps.getString(\"execution.mode\"))\n",
    "  \n",
    "val sc = new SparkContext(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HADOOP CONFIGURATION**\n",
    "\n",
    "Let us see how we can access Hadoop Configuration.\n",
    "\n",
    "* Spark uses HDFS APIs to read files from supported file systems.\n",
    "* As part of Spark dependencies, we get HDFS APIs as well.\n",
    "* We can get Hadoop Configuration using sc.hadoopConfiguration\n",
    "* Using it, we will be able to create FileSystem Object. It will explose APIs such as exists, delete etc.\n",
    "* We can use those to validate as well as manage input and/or output directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/ Make sure to run earlier code to create Spark Context\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration)\n",
    "\n",
    "if (!fs.exists(new Path(inputPath))) {\n",
    "\n",
    "  println(\"Input path does not exist\")\n",
    "  \n",
    "} else {\n",
    "\n",
    "  if (fs.exists(new Path(outputPath)))\n",
    "  \n",
    "  \n",
    "fs.delete(new Path(outputPath), true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**READ AND FILTER ORDERS**\n",
    "\n",
    "As we are able to create Spark Context, now let us actually read and manipulate data from orders.\n",
    "\n",
    "* Read data from orders\n",
    "* Use filter and validate for COMPLETE or CLOSED as well as passed month\n",
    "* Use map to extract order_id and hard coded value 1 so that we can use it to join later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Filter for orders which fall in the month passed as argument\n",
    "\n",
    "val orders = inputPath + \"orders\"\n",
    "\n",
    "val ordersFiltered = sc.textFile(orders).\n",
    "\n",
    "  filter(order => {\n",
    "      \n",
    "    order.split(\",\")(1).contains(month) &&\n",
    "      \n",
    "      List(\"COMPLETE\", \"CLOSED\").contains(order.split(\",\")(3))\n",
    "  }).\n",
    "\n",
    "  map(order => (order.split(\",\")(0).toInt, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***JOIN ORDERS AND ORDER ITEMS***\n",
    "\n",
    "Now let us join order_items with orders and get product_id and order_item_subtotal.\n",
    "\n",
    "* Read data from order_items\n",
    "* Extract order_id, product_id and order_item_subtotal as a tuple.\n",
    "* First element is order_id and second element is nested tuple which contain product_id and order_item_subtotal.\n",
    "* Join the data set with orders filtered using order_id as key.\n",
    "* It will generate RDD of tuples – **(order_id, ((product_id, order_item_subtotal), 1))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Join filtered orders and order_items to get order_item details for a given month\n",
    "\n",
    "// Get revenue for each product_id\n",
    "\n",
    "val orderItems = inputPath + \"order_items\"\n",
    "\n",
    "val revenueByProductId = sc.textFile(orderItems).\n",
    "\n",
    "  map(orderItem => {\n",
    "      \n",
    "    val oi = orderItem.split(\",\")\n",
    "      \n",
    "    (oi(1).toInt, (oi(2).toInt, oi(4).toFloat)\n",
    "     \n",
    "  }).\n",
    "      \n",
    "  join(ordersFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE REVENUE PER PRODUCT ID\n",
    "\n",
    "Now we can extract product_id and order_item_subtotal and compute revenue for each product_id.\n",
    "\n",
    "* We can discard order_id and 1 from the join ouput.\n",
    "* We can use map and get the required information – product_id and order_item_subtotal.\n",
    "* Using reduceByKey, we should be able to compute revenue for each product_id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// Join filtered orders and order_items to get order_item details for a given month\n",
    "\n",
    "// Get revenue for each product_id\n",
    "\n",
    "val orderItems = inputPath + \"order_items\"\n",
    "\n",
    "val revenueByProductId = sc.textFile(orderItems).\n",
    "\n",
    "  map(orderItem => {\n",
    "      \n",
    "    val oi = orderItem.split(\",\")\n",
    "      \n",
    "    (oi(1).toInt, (oi(2).toInt, oi(4).toFloat))\n",
    "      \n",
    "  }).\n",
    "\n",
    "  join(ordersFiltered).\n",
    "\n",
    "  map(rec => rec._2._1).\n",
    "\n",
    "  reduceByKey(_ + _)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Join Products and Get Product Name***\n",
    "\n",
    "***Update Application***\n",
    "\n",
    "Here is the complete working code. We can copy paste the code to IntelliJ and then validate locally before building the jar file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDemo-01-build.sbt\n",
    "\n",
    "\n",
    "\n",
    "name := \"SparkDemo\"\n",
    "\n",
    "version := \"0.1\"\n",
    "\n",
    "scalaVersion := \"2.11.8\"\n",
    "\n",
    "libraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.2\"\n",
    "\n",
    "libraryDependencies += \"org.apache.spark\" % \"spark-core_2.11\" % \"2.3.2\"\n",
    "\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDemo-02-application.properties\n",
    "\n",
    "\n",
    "\n",
    "dev.execution.mode = local\n",
    "dev.input.base.dir = C:\\data\\retail_db\\\n",
    "dev.output.base.dir = C:\\data\\\n",
    "\n",
    "devu.execution.mode = local\n",
    "devu.input.base.dir = /mnt/c/data/retail_db/\n",
    "devu.output.base.dir = /mnt/c/data/\n",
    "\n",
    "prod.execution.mode = yarn-client\n",
    "prod.input.base.dir = /public/retail_db/\n",
    "prod.output.base.dir = /user/training/retail/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDemo-03-GetMonthlyProductRevenue.scala\n",
    "\n",
    "----------------------------\n",
    "\n",
    "package retail\n",
    "\n",
    "import com.typesafe.config.ConfigFactory\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "import scala.io.Source\n",
    "\n",
    "/**\n",
    "  * Created by itversity on 05/06/17.\n",
    "  */\n",
    "\n",
    "/* Get monthly product revenue for given month\n",
    " * orders and order_items will be read from HDFS\n",
    " * products will be read from local file system\n",
    " * Program takes 5 arguments -\n",
    " *  environment\n",
    " *  base directory for HDFS for orders and order_items\n",
    " *  output directory to which data should be saved\n",
    " *  local directory path to read products\n",
    " *  month for which we need to get product revenue\n",
    " * Filter for orders which fall in the month passed as argument\n",
    " * Join filtered orders and order_items to get order_item details for a given month\n",
    " * Get revenue for each product_id\n",
    " * We need to read products from local file system\n",
    " * Convert into RDD and extract product_id and product_name\n",
    " * Join it with aggregated order_items (product_id, revenue)\n",
    " * Get product_name and revenue for each product\n",
    " */\n",
    "object GetMonthlyProductRevenue {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val props = ConfigFactory.load()\n",
    "    val envProps = props.getConfig(args(0))\n",
    "    val inputPath = envProps.getString(\"input.base.dir\")\n",
    "    val outputPath = envProps.getString(\"output.base.dir\") + \"monthly_product_revenue\"\n",
    "    val month = args(1)\n",
    "    val conf = new SparkConf().\n",
    "      setAppName(\"Revenue Per Product for \" + month).\n",
    "      setMaster(envProps.getString(\"execution.mode\"))\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    val fs = FileSystem.get(sc.hadoopConfiguration)\n",
    "\n",
    "    if (!fs.exists(new Path(inputPath))) {\n",
    "      println(\"Input path does not exist\")\n",
    "    } else {\n",
    "      if (fs.exists(new Path(outputPath)))\n",
    "        fs.delete(new Path(outputPath), true)\n",
    "\n",
    "      // Filter for orders which fall in the month passed as argument\n",
    "      val orders = inputPath + \"orders\"\n",
    "      val ordersFiltered = sc.textFile(orders).\n",
    "        filter(order => {\n",
    "          order.split(\",\")(1).contains(month) &&\n",
    "            List(\"COMPLETE\", \"CLOSED\").contains(order.split(\",\")(3))\n",
    "        }).\n",
    "        map(order => (order.split(\",\")(0).toInt, 1))\n",
    "\n",
    "      // Join filtered orders and order_items to get order_item details for a given month\n",
    "      // Get revenue for each product_id\n",
    "      val orderItems = inputPath + \"order_items\"\n",
    "      val revenueByProductId = sc.textFile(orderItems).\n",
    "        map(orderItem => {\n",
    "          val oi = orderItem.split(\",\")\n",
    "          (oi(1).toInt, (oi(2).toInt, oi(4).toFloat))\n",
    "        }).\n",
    "        join(ordersFiltered).\n",
    "        map(rec => rec._2._1).\n",
    "        reduceByKey(_ + _)\n",
    "\n",
    "      // We need to read products from local file system\n",
    "      val localPath = args(2)\n",
    "      val products = Source.\n",
    "        fromFile(localPath).\n",
    "        getLines()\n",
    "\n",
    "      // Convert into RDD and extract product_id and product_name\n",
    "      // Join it with aggregated order_items (product_id, revenue)\n",
    "      // Get product_name and revenue for each product\n",
    "      sc.parallelize(products.toList).\n",
    "        map(product => (product.split(\",\")(0).toInt, product.split(\",\")(2))).\n",
    "        join(revenueByProductId).\n",
    "        map(rec => (rec._2._2, rec._2._1)).\n",
    "        sortByKey().\n",
    "        map(rec => rec.productIterator.mkString(\"\\t\")).\n",
    "        saveAsTextFile(outputPath)\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Deploy\n",
    "Once the development is done, we can build the jar file and run it on cluster.\n",
    "\n",
    "* Go to the project working directory and run **sbt package** to build jar file.\n",
    "* We can validate jar file in the local environment using **spark-submit**\n",
    "* As we have used 3rd party plugin to externalize properties we need to pass it using **–jars** or **–packages**.\n",
    "* **–jars** can be used to point to the local jar file using fully qualified path, we can pass multiple jars using comma as separator.\n",
    "* **–packages** can be used to download the jar file from repositorites over internet. We need to pass group id, artifact id and version using colon as delimiter.\n",
    "* We can ship the jar to the gateway node on the cluster and run using **spark-submit** command.\n",
    "* In typical environments these commands are used to schedule using enterprise scheduling tools such as Azkaban, Air Flow, Control M, Cron Job etc.\n",
    "\n",
    "**Validate Locally**\n",
    "\n",
    "**Run on Cluster**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkDemo-spark-submit-01-local.sh\n",
    "\n",
    "\n",
    "spark-submit --master local \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --class GetMonthlyProductRevenue \\\n",
    "  --packages com.typesafe:config:1.3.2 \\\n",
    "  target/scala-2.11/sparkdemo_2.11-0.1.jar dev 2014-01 /mnt/c/data/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "SparkDemo-spark-submit-02-yarn.sh \n",
    "\n",
    "\n",
    "spark-submit --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --class GetMonthlyProductRevenue \\\n",
    "  --packages com.typesafe:config:1.3.2 \\\n",
    "  sparkdemo_2.11-0.1.jar prod 2014-01 /data/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Mode vs. YARN Mode\n",
    "\n",
    "Let us understand different modes using which we deploy applications.\n",
    "\n",
    "* Local Mode\n",
    "* Stand Alone Mode (Spark Native)\n",
    "* Mesos Mode\n",
    "* YARN Mode\n",
    "\n",
    "Local and Stand Alone is used for development and exploratory purposes. In Production we either use Mesos or YARN.\n",
    "\n",
    "* Most of the Big Data distributions such as Cloudera, Hortonworks etc have eco system of tools such as Kafka, Hadoop etc along with Spark.\n",
    "* They are typically deployed using YARN, while Spark exclusive clusters typically use Mesos.\n",
    "* As our clusters are either built using Hortonworks or Cloudera Distribution, we tend to run our Spark Jobs using YARN mode.\n",
    "* We can make YARN default while setting up clusters. However, if the cluster is configured to use local mode, then we can overwrite it by using **–master** as part of **spark-submit** or **spark-shell**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick walkthrough of Spark UI\n",
    "\n",
    "Let us walk through the details about Spark UI.\n",
    "\n",
    "* While the job is running it is served by Application Master using Resource Manager Web Interface as Proxy.\n",
    "* There are several important tabs which we should be familiar with\n",
    "    * Executors\n",
    "    * Jobs\n",
    "    * Stages\n",
    "    * Environment\n",
    "    * Storage\n",
    "* It is very important to spend some time and understand all these different components of Spark UI.\n",
    "\n",
    "### YARN Deployment Modes\n",
    "\n",
    "We can submit Spark application in 2 deployment modes.\n",
    "\n",
    "* Client Mode – Driver Program will be running in the node on which job is submitted – i.e., Gateway Node.\n",
    "* Cluster Mode – Driver Program will be running as part of the application master – i.e., one of the worker nodes in the cluster.\n",
    "* We can control the mode by using –deploy-mode control argument as part of spark-submit or spark-shell.\n",
    "\n",
    "Spark Execution Cycle\n",
    "Let us understand details about Spark Execution Cycle. You can review this using [Spark Official Documentation](https://spark.apache.org/docs/latest/cluster-overview.html).\n",
    "\n",
    "* We submit the job for the client. The JVM typically acts as the Driver Program.\n",
    "* It will talk to the Resource Manager and create the Application Master.\n",
    "* Application Master will talk to Worker Nodes on which Node Managers are running and provision resources based on Allocation Settings. Allocation can be either static or dynamic.\n",
    "* These resources are nothing but Executors. From YARN perspective they are Containers.\n",
    "* The Executor is nothing but JVM which can run multiple concurrent threads until the Job is complete\n",
    "* Here are the different components that are used as part of execution.\n",
    "    * Driver Program\n",
    "    * Spark Context\n",
    "    * Executors\n",
    "    * Executor Tasks\n",
    "    * Job\n",
    "    * Stages\n",
    "    * Tasks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
