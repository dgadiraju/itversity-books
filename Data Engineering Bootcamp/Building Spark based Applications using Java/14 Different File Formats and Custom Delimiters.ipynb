{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different File Formats and Custom Delimiters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of this session we will talk about dealing with different file formats and also custom delimiters in text data. We will see how to read and how to write the data. Also we will understand APIs such as persist/cache on Data Frames.\n",
    "\n",
    "* Overview of write APIs – dataframe.write\n",
    "* Overview of read APIs – spark.read\n",
    "* Supported file formats\n",
    "    * csv, text (for text file formats)\n",
    "    * json (using complex schema)\n",
    "    * orc\n",
    "    * parquet\n",
    "    * avrò (3rd party)\n",
    "* Text Data – Custom Delimiters\n",
    "* Data Frames Persistence\n",
    "\n",
    "### Overview of write APIs – dataframe.write\n",
    "\n",
    "Let us see how we can write data to different targets using APIs under write on top of data frame.\n",
    "\n",
    "* Supported file formats – csv, text json, orc, parquet etc.\n",
    "* We can also write data to 3rd party supported file formats such as avro\n",
    "* Data can be written to Hive tables as well\n",
    "* We can also connect to relational databases over JDBC and save our output into remote relational databases.\n",
    "* We can also connect to any 3rd party database using relevant plugin and preserve data over there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "schemaString = order_id:int order_date:string order_customer_id:int order_status:string\n",
       "a = Array(order_id:int, order_date:string, order_customer_id:int, order_status:string)\n",
       "fields = Array(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "schema = StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "inputBaseDir = /public/retail_db\n",
       "ordersDF = [order_id: i...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: i..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "ordersDF.printSchema\n",
    "ordersDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.write.\n",
    "  format(\"json\").\n",
    "  save(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersDF.write.json(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// dataframe-write-examples-02-jdbc.scala\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "ordersDF.write.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  mode(\"append\").\n",
    "  save\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "ordersDF.\n",
    "  write.\n",
    "  mode(\"append\").\n",
    "  jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// dataframe-write-examples-03-hive.scala\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "ordersDF.write.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  mode(\"append\").\n",
    "  save\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "ordersDF.\n",
    "  write.\n",
    "  mode(\"append\").\n",
    "  jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/public/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "// To create new table and insert into it\n",
    "ordersDF.write.\n",
    "  format(\"hive\").\n",
    "  mode(\"overwrite\").\n",
    "  saveAsTable(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "ordersDF.write.\n",
    "  mode(\"overwrite\").\n",
    "  saveAsTable(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "// To insert data into existing table\n",
    "ordersDF.write.\n",
    "  format(\"hive\").\n",
    "  mode(\"overwrite\").\n",
    "  insertInto(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "ordersDF.write.\n",
    "  mode(\"overwrite\").\n",
    "  insertInto(\"bootcampdemo.orders_hive\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of read APIs – spark.read\n",
    "\n",
    "spark.read have bunch of APIs to read data from different source types.\n",
    "\n",
    "* Supported file formats- csv, text, json, orc, parquet etc\n",
    "* We can also read data from 3rd party supported file formats such as avro\n",
    "* We can read data directly from hive tables\n",
    "* JDBC – to read data from relational databases\n",
    "* There is generic API called format which can be used in conjunction with option to pass relevant arguments and then load data from either files or over JDBC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// dataframe-read-examples-01-files.scala\n",
    "\n",
    "// val orders = spark.read.\n",
    "//   format(\"json\").\n",
    "//   load(\"/public/retail_db_json/orders\")\n",
    "\n",
    "val orders = spark.read.json(\"/public/retail_db_json/orders\")\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// dataframe-read-examples-02-jdbc.scala\n",
    "\n",
    "val table = \"retail_export.orders_export\"\n",
    "\n",
    "val orders = spark.read.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", \"retail_export.orders_export\").\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  load()\n",
    "\n",
    "import java.util.Properties\n",
    "val props = new Properties\n",
    "\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "val orders = spark.read.\n",
    "    jdbc(\"jdbc:mysql://ms.itversity.com\", table, props)\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// dataframe-read-examples-03-hive.scala\n",
    "\n",
    "val orders = spark.read.\n",
    "  format(\"hive\").\n",
    "  table(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "val orders = spark.read.table(\"bootcampdemo.orders_hive\")\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported file formats\n",
    "\n",
    "Let us see details about all the supported formats in Spark to create data frames and save them.\n",
    "\n",
    "* Following file formats are supported out of the box with Spark\n",
    "    * text – using text (fixed length) or csv (delimited)\n",
    "    * json\n",
    "    * orc\n",
    "    * parquet\n",
    "* Avro is available with 3rd party plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.hadoop.security.AccessControlException\n",
       "Message: Permission denied: user=nagasreed, access=EXECUTE, inode=\"/user/ramchander_chikkala/sparkscala/orders_text\":ramchander_chikkala:hdfs:drwx------\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:292)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:238)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4146)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "\n",
       "StackTrace: \tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:292)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:238)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4146)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
       "  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
       "  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
       "  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
       "  at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)\n",
       "  at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)\n",
       "  at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2179)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1442)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem$26.doCall(DistributedFileSystem.java:1438)\n",
       "  at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
       "  at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1454)\n",
       "  at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1448)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:714)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.flatMap(List.scala:344)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:174)\n",
       "  ... 42 elided\n",
       "Caused by: org.apache.hadoop.ipc.RemoteException: Permission denied: user=nagasreed, access=EXECUTE, inode=\"/user/ramchander_chikkala/sparkscala/orders_text\":ramchander_chikkala:hdfs:drwx------\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:353)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkTraverse(FSPermissionChecker.java:292)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:238)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:190)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1950)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getFileInfo(FSDirStatAndListingOp.java:108)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getFileInfo(FSNamesystem.java:4146)\n",
       "\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getFileInfo(NameNodeRpcServer.java:1137)\n",
       "\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getFileInfo(ClientNamenodeProtocolServerSideTranslatorPB.java:866)\n",
       "\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n",
       "\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:640)\n",
       "\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2351)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2347)\n",
       "\tat java.security.AccessController.doPrivileged(Native Method)\n",
       "\tat javax.security.auth.Subject.doAs(Subject.java:422)\n",
       "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)\n",
       "\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2347)\n",
       "  at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1554)\n",
       "  at org.apache.hadoop.ipc.Client.call(Client.java:1498)\n",
       "  at org.apache.hadoop.ipc.Client.call(Client.java:1398)\n",
       "  at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)\n",
       "  at com.sun.proxy.$Proxy19.getFileInfo(Unknown Source)\n",
       "  at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:823)\n",
       "  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "  at java.lang.reflect.Method.invoke(Method.java:498)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:290)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:202)\n",
       "  at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:184)\n",
       "  at com.sun.proxy.$Proxy20.getFileInfo(Unknown Source)\n",
       "  at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2177)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n",
    "\n",
    "\n",
    "val orders_read = spark.read.format(\"text\").\n",
    "  load(\"/user/training/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: lastException: Throwable = null\n",
       "<console>:26: error: not found: value orders\n",
       "       orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\").\n",
       "       ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\").\n",
    "  write.\n",
    "  format(\"text\").\n",
    "  save(\"/user/training/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "orders.selectExpr(\"concat(order_id, ',', order_date, ',', order_customer_id, ',', order_status)\").\n",
    "  write.\n",
    "  mode(\"overwrite\").\n",
    "  text(\"/user/training/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [value: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[value: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.text(\"/user/training/sparkscala/orders_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|1,2013-07-25 00:0...|\n",
      "|2,2013-07-25 00:0...|\n",
      "|3,2013-07-25 00:0...|\n",
      "|4,2013-07-25 00:0...|\n",
      "|5,2013-07-25 00:0...|\n",
      "|6,2013-07-25 00:0...|\n",
      "|7,2013-07-25 00:0...|\n",
      "|8,2013-07-25 00:0...|\n",
      "|9,2013-07-25 00:0...|\n",
      "|10,2013-07-25 00:...|\n",
      "|11,2013-07-25 00:...|\n",
      "|12,2013-07-25 00:...|\n",
      "|13,2013-07-25 00:...|\n",
      "|14,2013-07-25 00:...|\n",
      "|15,2013-07-25 00:...|\n",
      "|16,2013-07-25 00:...|\n",
      "|17,2013-07-25 00:...|\n",
      "|18,2013-07-25 00:...|\n",
      "|19,2013-07-25 00:...|\n",
      "|20,2013-07-25 00:...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: path hdfs://nn01.itversity.com:8020/user/ramchander_chikkala/bootcampdemo/pyspark/orders_csv already exists.;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
       "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
       "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
       "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n",
       "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n",
    "\n",
    "orders.write.\n",
    "  format(\"csv\").\n",
    "  save(\"/user/training/bootcampdemo/pyspark/orders_csv\")\n",
    "\n",
    "orders.write.csv(\"/user/training/bootcampdemo/pyspark/orders_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders_read1 = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read1 = spark.read.\n",
    "  format(\"csv\").\n",
    "  load(\"/user/training/bootcampdemo/pyspark/orders_csv\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "orders_read1.show()\n",
    "orders_read1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders_read2 = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read2 = spark.read.\n",
    "  csv(\"/user/training/bootcampdemo/pyspark/orders_csv\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "orders_read2.show()\n",
    "orders_read2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"json\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  json(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"json\").\n",
    "  load(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  json(\"/user/training/sparkscala/orders_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|            11599|2013-07-25 00:00:...|       1|         CLOSED|\n",
      "|              256|2013-07-25 00:00:...|       2|PENDING_PAYMENT|\n",
      "|            12111|2013-07-25 00:00:...|       3|       COMPLETE|\n",
      "|             8827|2013-07-25 00:00:...|       4|         CLOSED|\n",
      "|            11318|2013-07-25 00:00:...|       5|       COMPLETE|\n",
      "|             7130|2013-07-25 00:00:...|       6|       COMPLETE|\n",
      "|             4530|2013-07-25 00:00:...|       7|       COMPLETE|\n",
      "|             2911|2013-07-25 00:00:...|       8|     PROCESSING|\n",
      "|             5657|2013-07-25 00:00:...|       9|PENDING_PAYMENT|\n",
      "|             5648|2013-07-25 00:00:...|      10|PENDING_PAYMENT|\n",
      "|              918|2013-07-25 00:00:...|      11| PAYMENT_REVIEW|\n",
      "|             1837|2013-07-25 00:00:...|      12|         CLOSED|\n",
      "|             9149|2013-07-25 00:00:...|      13|PENDING_PAYMENT|\n",
      "|             9842|2013-07-25 00:00:...|      14|     PROCESSING|\n",
      "|             2568|2013-07-25 00:00:...|      15|       COMPLETE|\n",
      "|             7276|2013-07-25 00:00:...|      16|PENDING_PAYMENT|\n",
      "|             2667|2013-07-25 00:00:...|      17|       COMPLETE|\n",
      "|             1205|2013-07-25 00:00:...|      18|         CLOSED|\n",
      "|             9488|2013-07-25 00:00:...|      19|PENDING_PAYMENT|\n",
      "|             9198|2013-07-25 00:00:...|      20|     PROCESSING|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_customer_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"orc\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/training/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  orc(\"/user/training/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"orc\").\n",
    "  load(\"/user/training/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:2: error: unclosed string literal\n",
       "  orc(\"/user/ramchander_chikkala\n",
       "      ^\n",
       "<console>:3: error: unclosed string literal\n",
       "  /sparkscala/orders_orc\")\n",
       "                         ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  orc(\"/user/training/sparkscala/orders_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n",
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")\n",
    "\n",
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  format(\"parquet\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/training/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  parquet(\"/user/training/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"parquet\").\n",
    "  load(\"/user/training/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders_read = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders_read = spark.read.\n",
    "  parquet(\"/user/training/sparkscala/orders_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Error parsing magics!\n",
       "Message: Magic ssh does not exist!\n",
       "StackTrace: "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%ssh\n",
    "spark-shell --master yarn --conf spark.ui.port=12901 --packages com.databricks:spark-avro_2.11:4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ordersCSV = [order_id: string, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: string, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// spark-shell --master yarn --conf spark.ui.port=12901 --packages com.databricks:spark-avro_2.11:4.0.0\n",
    "\n",
    "val ordersCSV = spark.read.csv(\"/public/retail_db/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(\"int\")).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Failed to find data source: com.databricks.spark.avro. Please find an Avro package at http://spark.apache.org/third-party-projects.html;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:630)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:241)\n",
       "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.write.\n",
    "  format(\"com.databricks.spark.avro\").\n",
    "  mode(\"overwrite\").\n",
    "  save(\"/user/training/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  mode(\"overwrite\").\n",
    "  avro(\"/user/training/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = spark.read.\n",
    "  format(\"com.databricks.spark.avro\").\n",
    "  load(\"/user/training/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = spark.read.\n",
    "  avro(\"/user/training/sparkscala/orders_avro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data – Custom Delimiters\n",
    "\n",
    "Now let us understand how to process text data with the different line as well as field delimiters.\n",
    "\n",
    "* We can read text data into RDD using SparkContext’s textFile. It will treat newline character as a record delimiter.\n",
    "* We have to parse each record in RDD and derive data to process further\n",
    "* With Spark Data Frames we have csv and text APIs to read text data int Data Frame\n",
    "* Both of them use newline character as a record delimiter. When we use csv API to create data frame we can also specify field separator/delimiter using sep as one of the options.\n",
    "* We can also specify sep while writing data into text files with any field separator or delimiter using csv API. Also, we can concatenate data as part of selectExpr with a delimiter of our choice and use text API.\n",
    "* Here is the example to read and write data with ascii null character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val inputBaseDir = \"/mnt/c/data/retail_db\"\n",
    "val outputBaseDir = \"/mnt/c/data/sparkscala\"\n",
    "\n",
    "val ordersCSV = spark.\n",
    "  read.\n",
    "  csv(inputBaseDir +\"/orders\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types.{IntegerType, FloatType}\n",
    "val orders = ordersCSV.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(IntegerType)).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.\n",
    "  selectExpr(\"concat(order_id, '\\00', order_date, '\\00', order_customer_id, '\\00', order_status)\").\n",
    "  write.\n",
    "  text(outputBaseDir + \"/orders_null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write.\n",
    "  options(\"sep\", \"\\00\").\n",
    "  csv(outputBaseDir + \"/orders_null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read_csv = spark.\n",
    "  read.\n",
    "  option(\"sep\", \"\\00\").\n",
    "  csv(outputBaseDir + \"/orders_null\").\n",
    "  toDF(\"order_id\", \"order_date\", \"order_customer_id\", \"order_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders_read = orders_read_csv.\n",
    "  withColumn(\"order_id\", $\"order_id\".cast(IntegerType)).\n",
    "  withColumn(\"order_customer_id\", $\"order_customer_id\".cast(IntegerType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_read.show()\n",
    "orders_read.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At times, we might have to deal with text data where line delimiter is different than newline character.\n",
    "* In this case, we need to use HDFS APIs to read data from files with custom line delimiter into RDD and process further (either using transformations/actions or data frame operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val path = \"/public/yelp-dataset/yelp_review.csv\"\n",
    "\n",
    "spark.read.text(path).\n",
    "  select(size(split($\"value\", \",\")).alias(\"value\")).\n",
    "  groupBy(\"value\").\n",
    "  count.\n",
    "  show(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = sc.hadoopConfiguration\n",
    "conf.set(\"textinputformat.record.delimiter\", \"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val yelpReview = sc.newAPIHadoopFile(path, \n",
    "  classOf[org.apache.hadoop.mapreduce.lib.input.TextInputFormat], \n",
    "  classOf[org.apache.hadoop.io.LongWritable], \n",
    "  classOf[org.apache.hadoop.io.Text], \n",
    "  conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.map(rec => rec._2.toString).\n",
    "  take(10).\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelpReview.\n",
    "  map(rec => (rec._2.toString.split(\"\\\",\\\"\").size, 1)).\n",
    "  reduceByKey((x, y) => x + y).\n",
    "  collect.\n",
    "  foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frames Persistence\n",
    "\n",
    "Now let us see how we can persist data frames.\n",
    "\n",
    "* By default, data will be streamed as data frames to executor tasks as data being processed.\n",
    "* Here is what will happen when data is read into executor task while it is being processed\n",
    "    * Deserialize into object\n",
    "    * Stream into memory\n",
    "    * Process data by executor task by applying logic\n",
    "    * Flush deserialized objects from memory as executor tasks are terminated\n",
    "* Some times we might have to read same data multiple times for processing with in the same job. By default every time data need to be deserialized and submitted to executor tasks for processing\n",
    "* To avoid deserializing into java objects when same data have to be read multiple times we can leverage caching.\n",
    "* There are 2 methods persist and cache. By default with data frames caching will be done as MEMORY_AND_DISK from Spark 2.\n",
    "* cache is shorthand method for persist at MEMORY_AND_DISK\n",
    "* This is what happens when we cache Data Frame\n",
    "    * Caching will be done only when data is read at least once for processing\n",
    "    * Each record will be deserialized into object\n",
    "    * These deserialized objects will be cached in memory as long as they fit\n",
    "    * If not, deserialized objects will be spilled out to disk\n",
    "* You can get details about different persistence levels from [here](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.StorageLevel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
