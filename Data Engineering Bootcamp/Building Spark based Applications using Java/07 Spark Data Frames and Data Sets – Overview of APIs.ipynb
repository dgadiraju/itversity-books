{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Frames and Data Sets – Overview of APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have basic knowledge of how to create Data Frames or Data Sets, now let us explore some key APIs to Create Data Frames Dynamically as well as to process the Data.\n",
    "\n",
    "* List of Important APIs\n",
    "* Creating Data Frame Dynamically\n",
    "* Data Frame Native Operations – Overview\n",
    "* Spark SQL – Overview\n",
    "* Saving Data Frames into Files – Overview\n",
    "\n",
    "### List of Important APIs\n",
    "\n",
    "Let us explore the important packages which provide APIs to create, process as well as write Data Frames or Data Sets.\n",
    "\n",
    "* We have already seen spark.read to read the data\n",
    "* We also have write APIs on top of Data Frames which can be used to save Data Frame to underlying File System.\n",
    "* **org.apache.spark.sql** have several other APIs for different purposes.\n",
    "    * **org.apache.spark.sql.types** for pre-defined types or to create schemas dynamically.\n",
    "    * **org.apache.spark.sql.functions** for pre-defined functions\n",
    "    * **createTempView** or **createOrReplaceTempView** on top of Data Frame to register it as in-memory view and process data using SQL based Queries.\n",
    "    * **spark.sql** to run queries from Hive Tables or temporary views or even Hive commands\n",
    "    * **org.apache.spark.sql.functions.udf** to create User Defined Functions for Data Frame Operations.\n",
    "    * **spark.register.udf** to register standard Scala Functions as SQL functions.\n",
    "\n",
    "### Creating Data Frame Dynamically\n",
    "\n",
    "Let us see how we can use StructTypes to create Data Frame dynamically based upon control files.\n",
    "\n",
    "* Many times we will get metadata about data in the form of control files.\n",
    "* Control files will have information such as column names, Data Types etc.\n",
    "* We need to create fields in Data Frame dynamically using column names and Data Types provided as part of control files.\n",
    "* The process is divided into two steps\n",
    "    * Create Schema\n",
    "    * Create Data Frame using Schema\n",
    "    \n",
    " **Creating a Schema**\n",
    " \n",
    "Here are the steps involved in creating Schema by using metadata from control files.\n",
    "\n",
    "* Load data from a file into Scala collection\n",
    "* Build an array of fields using StructField with column name and Data Type\n",
    "* Using the array we can build StructType\n",
    "* Also, we need to read data from files and then apply the map to build RDD of Row type for each record with attributes.\n",
    "* Then we can use spark.createDataFrame to create Data Frame programmatically by passing RDD of records of type Row and StructType   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaString = order_id:int order_date:string order_customer_id:int order_status:string\n",
       "a = Array(order_id:int, order_date:string, order_customer_id:int, order_status:string)\n",
       "fields = Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CreateSchemaUsingMetadata.scala\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "val fields = a.map(f => {\n",
    "  if(f.split(\":\")(1) == \"int\") \n",
    "    StructField(f.split(\":\")(0), IntegerType)\n",
    "  else\n",
    "    StructField(f.split(\":\")(1), StringType)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fields = Array(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "schema = StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Data Frame Dynamically**\n",
    "\n",
    "Let us see how we can create Data Frame after defining the Schema using metadata.\n",
    "\n",
    "* Read the data from the file – orders\n",
    "* Apply the necessary transformation to create RDD of type Row with four fields using map.\n",
    "* Convert into dataframe using **spark.createDataFrame**. It take RDD and schema as arguments.\n",
    "* RDD will be converted to Data Frame using Schema defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaString = order_id:int order_date:string order_customer_id:int order_status:string\n",
       "a = Array(order_id:int, order_date:string, order_customer_id:int, order_status:string)\n",
       "fields = Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "val fields = a.map(f => {\n",
    "  if(f.split(\":\")(1) == \"int\") \n",
    "    StructField(f.split(\":\")(0), IntegerType)\n",
    "  else\n",
    "    StructField(f.split(\":\")(1), StringType)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fields = Array(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n",
       "schema = StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(order_id,IntegerType,true), StructField(order_date,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(order_status,StringType,true))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|order_id|          order_date|order_customer_id|   order_status|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "|       1|2013-07-25 00:00:...|            11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|            12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|             8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|            11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|             7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|             4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|             2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|             1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|             9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|             2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|             2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|             1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|             9198|     PROCESSING|\n",
      "+--------+--------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputBaseDir = /public/retail_db\n",
       "orders = /public/retail_db/orders MapPartitionsRDD[1] at textFile at <console>:42\n",
       "ordersRDD = MapPartitionsRDD[2] at map at <console>:46\n",
       "ordersDF = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputBaseDir = \"/public/retail_db\"\n",
    "val orders = sc.textFile(inputBaseDir + \"/orders\")\n",
    "\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val ordersRDD = orders.map(o => Row(o.split(\",\")(0).toInt, o.split(\",\")(1), o.split(\",\")(2).toInt, o.split(\",\")(3)))\n",
    "val ordersDF = spark.createDataFrame(ordersRDD, schema)\n",
    "\n",
    "ordersDF.printSchema\n",
    "ordersDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schemaString = order_id:int order_date:string order_customer_id:int order_status:string\n",
       "a = Array(order_id:int, order_date:string, order_customer_id:int, order_status:string)\n",
       "fields = Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Array(StructField(order_id,IntegerType,true), StructField(string,StringType,true), StructField(order_customer_id,IntegerType,true), StructField(string,StringType,true))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val schemaString = \"order_id:int order_date:string order_customer_id:int order_status:string\"\n",
    "\n",
    "val a = schemaString.split(\" \")\n",
    "\n",
    "val fields = a.map(f => {\n",
    "  if(f.split(\":\")(1) == \"int\") \n",
    "    StructField(f.split(\":\")(0), IntegerType)\n",
    "  else\n",
    "    StructField(f.split(\":\")(1), StringType)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.sql.AnalysisException\n",
       "Message: Path does not exist: hdfs://nn01.itversity.com:8020/Users/itversity/Research/data/retail_db/orders;\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.flatMap(List.scala:344)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:473)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Using pattern matching\n",
    "val fields = a.map(f => f.split(\":\")(1) match {\n",
    "  case \"int\" => StructField(f.split(\":\")(0), IntegerType)\n",
    "  case _ => StructField(f.split(\":\")(0), StringType)\n",
    "})\n",
    "\n",
    "val schema = StructType(fields)\n",
    "val inputBaseDir = \"/Users/itversity/Research/data/retail_db\"\n",
    "\n",
    "val ordersDF = spark.\n",
    "  read.\n",
    "  schema(schema).\n",
    "  csv(inputBaseDir + \"/orders\")\n",
    "\n",
    "ordersDF.printSchema\n",
    "ordersDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Frame Native Operations – Overview\n",
    "\n",
    "As we have seen how to create Data Frames from files, RDD etc., let us get into the high-level details of Data Frame Native Operations. We can perform all standard transformations using Data Frame Operations.\n",
    "\n",
    "* Previewing Schema and Data – printSchema and show\n",
    "* Row-level transformations – using select, withColumn\n",
    "* Filtering the Data – filter or where. We can pass filtering either by using SQL style syntax or Data Frame Native Syntax.\n",
    "* Aggregations – count, sum, avg, min, max etc\n",
    "* Sorting\n",
    "* Ranking using Windowing or Analytical Functions\n",
    "\n",
    "Let us see a few simple examples.\n",
    "\n",
    "* Get orders for the month of 2014 January.\n",
    "* Get count by status from filtered orders\n",
    "* Get revenue for each order_id from order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputBaseDir = /public/retail_db_json\n",
       "ordersDF = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.count\n",
    "\n",
    "val inputBaseDir = \"/public/retail_db_json\"\n",
    "val ordersDF = spark.read.json(inputBaseDir + \"/orders\")\n",
    "\n",
    "// We can use either filter or where\n",
    "ordersDF.where(\"order_date like '2014-01%'\").show\n",
    "ordersDF.where($\"order_date\".like(\"2014-01%\")).show\n",
    "\n",
    "ordersDF.filter(\"order_date like '2014-01%'\").show\n",
    "ordersDF.filter($\"order_date\".like(\"2014-01%\")).show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+\n",
      "|   order_status|order_count|\n",
      "+---------------+-----------+\n",
      "|PENDING_PAYMENT|       1334|\n",
      "|       COMPLETE|       1911|\n",
      "|        ON_HOLD|        365|\n",
      "| PAYMENT_REVIEW|         77|\n",
      "|     PROCESSING|        712|\n",
      "|         CLOSED|        633|\n",
      "|SUSPECTED_FRAUD|        131|\n",
      "|        PENDING|        635|\n",
      "|       CANCELED|        110|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ordersDF.\n",
    "  where(\"order_date like '2014-01%'\").\n",
    "  groupBy(\"order_status\").\n",
    "  agg(count(\"order_status\").alias(\"order_count\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Unknown Error\n",
       "Message: <console>:45: error: not found: value round\n",
       "         agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\")).\n",
       "             ^\n",
       "<console>:45: error: not found: value sum\n",
       "         agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\")).\n",
       "                   ^\n",
       "\n",
       "StackTrace: "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItemsDF = spark.read.json(inputBaseDir + \"/order_items\")\n",
    "orderItemsDF.\n",
    "  groupBy(\"order_item_order_id\").\n",
    "  agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL – Overview\n",
    "\n",
    "Let us get into the details related to Spark SQL. We can submit queries on Hive tables or in memory temp tables using spark.sql API.\n",
    "\n",
    "* We can directly run queries on Hive tables or tables from remote databases using JDBC and create Data Frame for the results.\n",
    "* We can also register a temporary table for a Data Frame and run queries against it.\n",
    "* We can perform all the standard transformations using SQL syntax\n",
    "* We can also run standard Hive commands using spark.sql, such as **show tables, describe table** etc.\n",
    "* As part of Data Processing, we typically perform these operations.\n",
    "    * Row Level Transformations (Data Standardization, Cleansing etc)\n",
    "    * Filtering the data\n",
    "    * Joining the Data Sets\n",
    "    * Aggregations such as sum, min, max\n",
    "    * Sorting and Ranking\n",
    "    * and more\n",
    "    \n",
    "Let us see a few simple examples.\n",
    "\n",
    "* Get orders for the month of 2014 January.\n",
    "* Get count by status from filtered orders\n",
    "* Get revenue for each order_id from order_items    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------+---------------+\n",
      "|order_customer_id|          order_date|order_id|   order_status|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "|             3414|2014-01-01 00:00:...|   25876|PENDING_PAYMENT|\n",
      "|             5549|2014-01-01 00:00:...|   25877|PENDING_PAYMENT|\n",
      "|             9084|2014-01-01 00:00:...|   25878|        PENDING|\n",
      "|             5118|2014-01-01 00:00:...|   25879|        PENDING|\n",
      "|            10146|2014-01-01 00:00:...|   25880|       CANCELED|\n",
      "|             3205|2014-01-01 00:00:...|   25881|PENDING_PAYMENT|\n",
      "|             4598|2014-01-01 00:00:...|   25882|       COMPLETE|\n",
      "|            11764|2014-01-01 00:00:...|   25883|        PENDING|\n",
      "|             7904|2014-01-01 00:00:...|   25884|PENDING_PAYMENT|\n",
      "|             7253|2014-01-01 00:00:...|   25885|        PENDING|\n",
      "|             8195|2014-01-01 00:00:...|   25886|     PROCESSING|\n",
      "|            10062|2014-01-01 00:00:...|   25887|        PENDING|\n",
      "|             6735|2014-01-01 00:00:...|   25888|       COMPLETE|\n",
      "|            10045|2014-01-01 00:00:...|   25889|       COMPLETE|\n",
      "|             2581|2014-01-01 00:00:...|   25890|        PENDING|\n",
      "|             3037|2014-01-01 00:00:...|   25891|         CLOSED|\n",
      "|             3853|2014-01-01 00:00:...|   25892|        ON_HOLD|\n",
      "|             8679|2014-01-01 00:00:...|   25893|PENDING_PAYMENT|\n",
      "|             7839|2014-01-01 00:00:...|   25894|     PROCESSING|\n",
      "|             1044|2014-01-01 00:00:...|   25895|       COMPLETE|\n",
      "+-----------------+--------------------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+-----------+\n",
      "|   order_status|order_count|\n",
      "+---------------+-----------+\n",
      "|PENDING_PAYMENT|       1334|\n",
      "|       COMPLETE|       1911|\n",
      "|        ON_HOLD|        365|\n",
      "| PAYMENT_REVIEW|         77|\n",
      "|     PROCESSING|        712|\n",
      "|         CLOSED|        633|\n",
      "|SUSPECTED_FRAUD|        131|\n",
      "|        PENDING|        635|\n",
      "|       CANCELED|        110|\n",
      "+---------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inputBaseDir = /public/retail_db_json\n",
       "ordersDF = [order_customer_id: bigint, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_customer_id: bigint, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val inputBaseDir = \"/public/retail_db_json\"\n",
    "val ordersDF = spark.read.json(inputBaseDir + \"/orders\")\n",
    "\n",
    "ordersDF.createTempView(\"orders\")\n",
    "\n",
    "spark.\n",
    "  sql(s\"\"\"SELECT * FROM orders \n",
    "          WHERE order_date LIKE '2014-01%'\"\"\").\n",
    "  show\n",
    "\n",
    "spark.\n",
    "  sql(s\"\"\"SELECT order_status, COUNT(1) order_count \n",
    "          FROM orders\n",
    "          WHERE order_date LIKE '2014-01%'\n",
    "          GROUP BY order_status\"\"\").\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|order_item_order_id|order_revenue|\n",
      "+-------------------+-------------+\n",
      "|                  1|       299.98|\n",
      "|                  2|       579.98|\n",
      "|                  4|       699.85|\n",
      "|                  5|      1129.86|\n",
      "|                  7|       579.92|\n",
      "|                  8|       729.84|\n",
      "|                  9|       599.96|\n",
      "|                 10|       651.92|\n",
      "|                 11|       919.79|\n",
      "|                 12|      1299.87|\n",
      "|                 13|       127.96|\n",
      "|                 14|       549.94|\n",
      "|                 15|       925.91|\n",
      "|                 16|       419.93|\n",
      "|                 17|       694.84|\n",
      "|                 18|       449.96|\n",
      "|                 19|       699.96|\n",
      "|                 20|       879.86|\n",
      "|                 21|       372.91|\n",
      "|                 23|       299.98|\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderItemsDF = [order_item_id: bigint, order_item_order_id: bigint ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_item_id: bigint, order_item_order_id: bigint ... 4 more fields]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItemsDF = spark.read.json(inputBaseDir + \"/order_items\")\n",
    "\n",
    "orderItemsDF.createTempView(\"order_items\")\n",
    "\n",
    "spark.\n",
    "  sql(s\"\"\"SELECT order_item_order_id, \n",
    "            ROUND(SUM(order_item_subtotal), 2) order_revenue \n",
    "          FROM order_items\n",
    "          GROUP BY order_item_order_id\n",
    "          ORDER BY order_item_order_id\"\"\").\n",
    "  show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data Frames into Files – Overview\n",
    "\n",
    "Once data in Data Frame is processed using either Data Frame Operations or Spark SQL, we can write Data Frame into target systems.\n",
    "\n",
    "* There are APIs to write data into files, Hive tables as well as remote RDBMS table over JDBC.\n",
    "* spark.write or spark.save are the main packages to write data into files in supported file systems.\n",
    "* We have APIs for these different file formats.\n",
    "    * Text File Format – csv and text\n",
    "    * parquet\n",
    "    * orc\n",
    "    * json\n",
    "    * avro (require plugin)\n",
    "    \n",
    "* We will see the details at a later point in time. For now, we will just validate on a Data Frame by writing into JSON format.\n",
    "\n",
    "Let us see a demo.\n",
    "\n",
    "* Read JSON data from order_items\n",
    "* Compute revenue for each order. We can either use Data Frame Native Operations or Spark SQL for this purpose.\n",
    "* Let us spark.sql.shuffle.partitions to 2, so that data can be aggregated using 2 tasks. By default Spark SQL or Data Frame Operations use 200.\n",
    "* As our data set size is very small, it does not make sense to use 200 threads to perform aggregation.\n",
    "* Save data back to File System in the form of JSON"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// DataFrameWriteOperations.scala \n",
    "\n",
    "import org.apache.spark.sql.functions\n",
    "\n",
    "val inputBaseDir = \"/public/retail_db_json\"\n",
    "val orderItemsDF = spark.read.json(inputBaseDir + \"/order_items\")\n",
    "\n",
    "orderItemsDF.createTempView(\"order_items\")\n",
    "\n",
    "val revenuePerOrder1 = orderItemsDF.\n",
    "  groupBy(\"order_item_order_id\").\n",
    "  agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\"))\n",
    "\n",
    "val revenuePerOrder = spark.\n",
    "  sql(s\"\"\"SELECT order_item_order_id, \n",
    "            ROUND(SUM(order_item_subtotal), 2) order_revenue \n",
    "          FROM order_items\n",
    "          GROUP BY order_item_order_id\n",
    "          ORDER BY order_item_order_id\"\"\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")\n",
    "\n",
    "val outputDir = \"/test\"\n",
    "revenuePerOrder.\n",
    "  write.\n",
    "  json(outputDir)\n",
    "\n",
    "revenuePerOrder.\n",
    "  write.\n",
    "  format(\"json\").\n",
    "  save(outputDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
