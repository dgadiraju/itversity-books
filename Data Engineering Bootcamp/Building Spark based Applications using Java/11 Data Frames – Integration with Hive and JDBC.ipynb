{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Frames – Integration with Hive and JDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how we can use Data Frames to get data from Hive and Relational databases using JDBC and then process data using operations available on Data Frames.\n",
    "\n",
    "* Recap of Hive\n",
    "* Reading and Writing Data – Hive tables\n",
    "* Reading and Writing Data – JDBC\n",
    "* Querying data from Hive tables\n",
    "* Hive DDL and DML – Data Frame Operations\n",
    "* Hive DDL and DML – Spark SQL\n",
    "* Hive Functions – Overview\n",
    "\n",
    "We will be using labs for the demonstration of interacting with Hive. It is straightforward to connect to Hive from Spark when both of them are integrated. If you want to explore Hive using Spark on your PC, I would recommend to use Cloudera’s QuickStart VM or Hortonworks Sandbox. Setting up Hive and integrating with Spark is a bit tedious (especially on Windows).\n",
    "\n",
    "Data Frames – Integration with Hive and JDBC\n",
    "JUNE 26, 2018 By Durga Gadiraju Leave a Comment\n",
    "\n",
    "Topic Progress:               \n",
    "← Back to Lesson\n",
    "Let us see how we can use Data Frames to get data from Hive and Relational databases using JDBC and then process data using operations available on Data Frames.\n",
    "\n",
    "Recap of Hive\n",
    "Reading and Writing Data – Hive tables\n",
    "Reading and Writing Data – JDBC\n",
    "Querying data from Hive tables\n",
    "Hive DDL and DML – Data Frame Operations\n",
    "Hive DDL and DML – Spark SQL\n",
    "Hive Functions – Overview\n",
    "We will be using labs for the demonstration of interacting with Hive. It is straightforward to connect to Hive from Spark when both of them are integrated. If you want to explore Hive using Spark on your PC, I would recommend to use Cloudera’s QuickStart VM or Hortonworks Sandbox. Setting up Hive and integrating with Spark is a bit tedious (especially on Windows).\n",
    "\n",
    "\n",
    "### Recap of Hive\n",
    "\n",
    "Let us see a quick recap of Hive.\n",
    "\n",
    "* Hive is used to create databases, tables and process data in Big Data Clusters.\n",
    "* It uses HDFS for the file system. A Hive database or a table or a partition is nothing but a directory in HDFS.\n",
    "* Once we create table metadata will be stored in an RDBMS database configured. We have configured our Hive with MySQL.\n",
    "* Hive also have query engine. A query will be typically compiled into Map Reduce Job.\n",
    "* Once we have tables created, we can run queries using different query engines.\n",
    "    * Hive\n",
    "    * Tez\n",
    "    * Impala\n",
    "    * Spark SQL\n",
    "    * Presto\n",
    "    * and more\n",
    "* Hive support DDL, batch data load as well as inserting query results on top of running SQL type queries.\n",
    "* Let us create Hive Database, create few tables and process using standard Hive Queries before we jump into Spark SQL."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hive-getting-started.sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS trainingdemo;\n",
    "\n",
    "USE trainingdemo;\n",
    "\n",
    "CREATE TABLE orders (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/orders' INTO TABLE orders;\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/order_items' INTO TABLE order_items;\n",
    "\n",
    "SELECT * FROM orders LIMIT 10;\n",
    "SELECT * FROM order_items LIMIT 10;\n",
    "\n",
    "CREATE TABLE daily_revenue (\n",
    "  order_date STRING,\n",
    "  revenue FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "INSERT INTO daily_revenue\n",
    "SELECT order_date, sum(order_item_subtotal) revenue \n",
    "FROM orders JOIN order_items\n",
    "ON order_id = order_item_order_id\n",
    "GROUP BY order_date;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data – Hive tables\n",
    "\n",
    "Let us see how we can read and write data using Hive Tables leveraging Data Frame Operations.\n",
    "\n",
    "* SparkSession have an API under read to get data directly from Hive Tables.\n",
    "* We can use SparkSession’s (spark) read.table to read data from Hive table.\n",
    "* Data from Hive Table will be loaded into Data Frame. We can now leverage Data Frame Operations to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"use trainingdemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|    database|           tableName|isTemporary|\n",
      "+------------+--------------------+-----------+\n",
      "|trainingdemo|       daily_revenue|      false|\n",
      "|trainingdemo|order_count_by_st...|      false|\n",
      "|trainingdemo|         order_items|      false|\n",
      "|trainingdemo|       order_revenue|      false|\n",
      "|trainingdemo|              orders|      false|\n",
      "+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130             |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530             |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911             |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657             |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648             |PENDING_PAYMENT|\n",
      "|11      |2013-07-25 00:00:00.0|918              |PAYMENT_REVIEW |\n",
      "|12      |2013-07-25 00:00:00.0|1837             |CLOSED         |\n",
      "|13      |2013-07-25 00:00:00.0|9149             |PENDING_PAYMENT|\n",
      "|14      |2013-07-25 00:00:00.0|9842             |PROCESSING     |\n",
      "|15      |2013-07-25 00:00:00.0|2568             |COMPLETE       |\n",
      "|16      |2013-07-25 00:00:00.0|7276             |PENDING_PAYMENT|\n",
      "|17      |2013-07-25 00:00:00.0|2667             |COMPLETE       |\n",
      "|18      |2013-07-25 00:00:00.0|1205             |CLOSED         |\n",
      "|19      |2013-07-25 00:00:00.0|9488             |PENDING_PAYMENT|\n",
      "|20      |2013-07-25 00:00:00.0|9198             |PROCESSING     |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"trainingdemo.orders\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          order_date|count|\n",
      "+--------------------+-----+\n",
      "|2013-08-13 00:00:...|  146|\n",
      "|2013-10-12 00:00:...|  324|\n",
      "|2013-11-15 00:00:...|  270|\n",
      "|2014-03-19 00:00:...|  260|\n",
      "|2014-04-26 00:00:...|  502|\n",
      "|2013-09-16 00:00:...|  242|\n",
      "|2013-09-20 00:00:...|  278|\n",
      "|2013-12-31 00:00:...|  532|\n",
      "|2013-09-06 00:00:...|  552|\n",
      "|2014-06-15 00:00:...|  256|\n",
      "|2013-12-24 00:00:...|  340|\n",
      "|2014-01-07 00:00:...|  326|\n",
      "|2014-06-07 00:00:...|  382|\n",
      "|2013-10-14 00:00:...|  278|\n",
      "|2013-11-11 00:00:...|  492|\n",
      "|2014-01-27 00:00:...|  326|\n",
      "|2014-01-29 00:00:...|  316|\n",
      "|2014-02-14 00:00:...|  348|\n",
      "|2014-04-15 00:00:...|  360|\n",
      "|2014-04-22 00:00:...|  288|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_id: int, order_date: string ... 2 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.read.table(\"orders\")\n",
    "orders.\n",
    "  groupBy(\"order_date\").\n",
    "  count.\n",
    "  show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|order_item_order_id|order_revenue|\n",
      "+-------------------+-------------+\n",
      "|                148|       959.98|\n",
      "|                463|      1659.84|\n",
      "|                471|       339.96|\n",
      "|                496|        883.9|\n",
      "|               1088|       499.94|\n",
      "|               1580|        599.9|\n",
      "|               1591|       879.72|\n",
      "|               1645|      3019.58|\n",
      "|               2366|       599.94|\n",
      "|               2659|      1449.82|\n",
      "|               2866|      1139.92|\n",
      "|               3175|       419.94|\n",
      "|               3749|       287.94|\n",
      "|               3794|        599.9|\n",
      "|               3918|      1659.86|\n",
      "|               3997|       1159.9|\n",
      "|               4101|       259.98|\n",
      "|               4519|       159.96|\n",
      "|               4818|       799.96|\n",
      "|               4900|       359.94|\n",
      "+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "orderItems = [order_item_id: int, order_item_order_id: int ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_item_id: int, order_item_order_id: int ... 4 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItems = spark.read.table(\"order_items\")\n",
    "orderItems.\n",
    "  groupBy(\"order_item_order_id\").\n",
    "  agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\")).\n",
    "  show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Processed Data Frame can be saved into a Hive table using multiple APIs under **spark.write**\n",
    "    * **saveAsTable** – creates a new table in Hive with Parquet file format\n",
    "    * **insertInto** – insert data frame into existing tables\n",
    "We can use mode to append or overwrite into the table. Modes can be passed as string or org.apache.spark.sql.SaveMode object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+-----------+\n",
      "|    database|           tableName|isTemporary|\n",
      "+------------+--------------------+-----------+\n",
      "|trainingdemo|       daily_revenue|      false|\n",
      "|trainingdemo|order_count_by_st...|      false|\n",
      "|trainingdemo|         order_items|      false|\n",
      "|trainingdemo|       order_revenue|      false|\n",
      "|trainingdemo|              orders|      false|\n",
      "+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"use trainingdemo\")\n",
    "spark.sql(\"show tables\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130             |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530             |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911             |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657             |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648             |PENDING_PAYMENT|\n",
      "|11      |2013-07-25 00:00:00.0|918              |PAYMENT_REVIEW |\n",
      "|12      |2013-07-25 00:00:00.0|1837             |CLOSED         |\n",
      "|13      |2013-07-25 00:00:00.0|9149             |PENDING_PAYMENT|\n",
      "|14      |2013-07-25 00:00:00.0|9842             |PROCESSING     |\n",
      "|15      |2013-07-25 00:00:00.0|2568             |COMPLETE       |\n",
      "|16      |2013-07-25 00:00:00.0|7276             |PENDING_PAYMENT|\n",
      "|17      |2013-07-25 00:00:00.0|2667             |COMPLETE       |\n",
      "|18      |2013-07-25 00:00:00.0|1205             |CLOSED         |\n",
      "|19      |2013-07-25 00:00:00.0|9488             |PENDING_PAYMENT|\n",
      "|20      |2013-07-25 00:00:00.0|9198             |PROCESSING     |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|order_id|order_date           |order_customer_id|order_status   |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "|1       |2013-07-25 00:00:00.0|11599            |CLOSED         |\n",
      "|2       |2013-07-25 00:00:00.0|256              |PENDING_PAYMENT|\n",
      "|3       |2013-07-25 00:00:00.0|12111            |COMPLETE       |\n",
      "|4       |2013-07-25 00:00:00.0|8827             |CLOSED         |\n",
      "|5       |2013-07-25 00:00:00.0|11318            |COMPLETE       |\n",
      "|6       |2013-07-25 00:00:00.0|7130             |COMPLETE       |\n",
      "|7       |2013-07-25 00:00:00.0|4530             |COMPLETE       |\n",
      "|8       |2013-07-25 00:00:00.0|2911             |PROCESSING     |\n",
      "|9       |2013-07-25 00:00:00.0|5657             |PENDING_PAYMENT|\n",
      "|10      |2013-07-25 00:00:00.0|5648             |PENDING_PAYMENT|\n",
      "|11      |2013-07-25 00:00:00.0|918              |PAYMENT_REVIEW |\n",
      "|12      |2013-07-25 00:00:00.0|1837             |CLOSED         |\n",
      "|13      |2013-07-25 00:00:00.0|9149             |PENDING_PAYMENT|\n",
      "|14      |2013-07-25 00:00:00.0|9842             |PROCESSING     |\n",
      "|15      |2013-07-25 00:00:00.0|2568             |COMPLETE       |\n",
      "|16      |2013-07-25 00:00:00.0|7276             |PENDING_PAYMENT|\n",
      "|17      |2013-07-25 00:00:00.0|2667             |COMPLETE       |\n",
      "|18      |2013-07-25 00:00:00.0|1205             |CLOSED         |\n",
      "|19      |2013-07-25 00:00:00.0|9488             |PENDING_PAYMENT|\n",
      "|20      |2013-07-25 00:00:00.0|9198             |PROCESSING     |\n",
      "+--------+---------------------+-----------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"orders\").show(false)\n",
    "spark.read.table(\"trainingdemo.orders\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orders = [order_id: int, order_date: string ... 2 more fields]\n",
       "orderCountByStatus = [order_date: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_date: string, count: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orders = spark.read.table(\"orders\")\n",
    "val orderCountByStatus = orders.\n",
    "  groupBy(\"order_date\").\n",
    "  count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Dosent have access.\n",
    "\n",
    "// orderCountByStatus.\n",
    "//   write.\n",
    "//   mode(\"overwrite\").\n",
    "//   saveAsTable(\"order_count_by_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "// orderCountByStatus.\n",
    "//   write.\n",
    "//   mode(SaveMode.Append).\n",
    "//   saveAsTable(\"order_count_by_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "orderItems = [order_item_id: int, order_item_order_id: int ... 4 more fields]\n",
       "orderRevenue = [order_item_order_id: int, order_revenue: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[order_item_order_id: int, order_revenue: double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val orderItems = spark.read.table(\"order_items\")\n",
    "val orderRevenue = orderItems.\n",
    "  groupBy(\"order_item_order_id\").\n",
    "  agg(round(sum(\"order_item_subtotal\"), 2).alias(\"order_revenue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(s\"\"\"CREATE TABLE IF NOT EXISTS trainingdemo.order_revenue (\n",
    "              order_id INT, \n",
    "              revenue FLOAT)\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderRevenue.\n",
    "  write.\n",
    "  mode(SaveMode.Overwrite).\n",
    "  insertInto(\"order_revenue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing Data – JDBC\n",
    "\n",
    "Let us see how we can read data from a remote database using Spark JDBC.\n",
    "\n",
    "* To connect to a Database, we need\n",
    "    * JDBC Jar File, which can be passed using –packages or –jars or as part of the fat jar file\n",
    "    * Server ip or DNS alias on which Database is running\n",
    "    * Credentials for authentication and authorization to the Database\n",
    "    * Database and Table Names we want to read the data from.\n",
    "* We can read data from the remote database into Data Frame using **spark.read.jdbc**\n",
    "* Database Connectivity information can be passed as options.\n",
    "* We can also use **spark.read.format(“jdbc”)** along with options.\n",
    "* Once data is processed we can write data back to the remote database using **write.jdbc** or **write.format**\n",
    "* We can either append to an existing table or overwrite it, but we will not be able to update or merge table over JDBC.\n",
    "* We can see the list of options in official documentation. Spark JDBC can replace Sqoop to get data from Relational Databases into HDFS and vice-versa."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# spark-jdbc-01-launch.sh\n",
    "\n",
    "spark-shell \\\n",
    "  --master yarn \\\n",
    "  --conf spark.ui.port=12901 \\\n",
    "  --jars /usr/share/java/mysql-connector-java.jar \\\n",
    "  --driver-class-path /usr/share/java/mysql-connector-java.jar\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-jdbc-02-read-data.scala\n",
    "\n",
    "val table = \"retail_db.orders\"\n",
    "\n",
    "val orders = spark.read.\n",
    "  format(\"jdbc\").\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", table).\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  load()\n",
    "\n",
    "import java.util.Properties\n",
    "\n",
    "val props = new Properties\n",
    "props.put(\"user\", \"retail_user\")\n",
    "props.put(\"password\", \"itversity\")\n",
    "\n",
    "val orders = spark.read.\n",
    "    jdbc(\"jdbc:mysql://ms.itversity.com\", table,\n",
    "         props)\n",
    "\n",
    "orders.show()\n",
    "orders.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// spark-jdbc-03-write-data.scala\n",
    "\n",
    "val table = \"retail_export.orders\"\n",
    "\n",
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "orders.write.\n",
    "  format(\"jdbc\").\n",
    "  mode(SaveMode.Overwrite).\n",
    "  option(\"url\", \"jdbc:mysql://ms.itversity.com\").\n",
    "  option(\"dbtable\", table).\n",
    "  option(\"user\", \"retail_user\").\n",
    "  option(\"password\", \"itversity\").\n",
    "  save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying Data from Hive Tables\n",
    "\n",
    "Apart from reading data from Hive Tables using Data Frame APIs, we can also use spark.sql to read data from Hive Tables as well as to write data to Hive Tables.\n",
    "\n",
    "* spark.sql can be used to issue any valid Hive Command or Query\n",
    "* It will always return a Data Frame\n",
    "* We need to use **show** to preview the data or collect to print the results\n",
    "* If we issue a complex query, that query will be executed and results will be returned to Data Frame.\n",
    "* Once Data Frame is created we can use all Data Fram Operations.\n",
    "* Let us run **select * from orders** using spark.sql and preview the results.\n",
    "* We can run any valid query using spark.sql. We will see more examples on Spark SQL in upcoming sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive DDL and DML – Spark\n",
    "\n",
    "Let us see how we can perform DDL and DML on top of Hive tables using Spark.\n",
    "\n",
    "* We can use Data Frame Operations or Spark SQL to perform Hive DDL as well as DML.\n",
    "* For Data Frame Operations we can use **write.saveAsTable** or **write.insertInto**. We can also use mode to overwrite an existing table or append to it. Options can be used to define additional properties while saving into Hive tables.\n",
    "* We can pick the queries which are executed to create the tables and run them using **spark.sql**. We need to format them as strings."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# hive-getting-started.sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS trainingdemo;\n",
    "\n",
    "USE trainingdemo;\n",
    "\n",
    "CREATE TABLE orders (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/orders' INTO TABLE orders;\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/order_items' INTO TABLE order_items;\n",
    "\n",
    "SELECT * FROM orders LIMIT 10;\n",
    "SELECT * FROM order_items LIMIT 10;\n",
    "\n",
    "CREATE TABLE daily_revenue (\n",
    "  order_date STRING,\n",
    "  revenue FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "INSERT INTO daily_revenue\n",
    "SELECT order_date, sum(order_item_subtotal) revenue \n",
    "FROM orders JOIN order_items\n",
    "ON order_id = order_item_order_id\n",
    "GROUP BY order_date;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive Functions – Overview\n",
    "\n",
    "Let us go through the details about Hive Functions. For most of the functions we have seen as part of org.apache.spark.sql.functions, we have counterparts in Hive.\n",
    "\n",
    "* We can get the list of functions by using show functions command.\n",
    "* We can get basic usage and syntax of a given function by using **describe function** command.\n",
    "* As mentioned earlier we can run these commands as part of **spark.sql**\n",
    "* Make sure to use **show(false)** to get the complete details.\n",
    "* We can categorize the functions into below groups.\n",
    "    * String Manipulation Functions\n",
    "        * trim, rtrim, ltrim\n",
    "        * lpad, rpad\n",
    "        * length\n",
    "        * substring, split\n",
    "    * Date Manipulation Functions\n",
    "        * date_add, date_sub, datediff\n",
    "        * date_format\n",
    "        * trunc\n",
    "     * Type Conversion Functions\n",
    "        * cast as\n",
    "     * CASE WHEN\n",
    "     * and more\n",
    "* Syntax can be different between functions in the package **org.apache.spark.sql.functions** and functions made available via Hive.     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
